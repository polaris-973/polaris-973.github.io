<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Pytorch_DeepLizard教程10-13：PyTorch中的tensor操作 - Paddey&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Paddey&#039;s Blog"><meta name="msapplication-TileImage" content="https://polaris-973.oss-cn-shenzhen.aliyuncs.com/img/b9c9db0f589cd855eb655fce4054665.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Paddey&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="此笔记引用自文章知乎链接并修改 学习笔记教程10-13集讲了在PyTorch中对tensor的几种操作。 在Pytorch中对tensor的操作主要有以下四种类型：  Reshaping operations Element-wise operations Reduction operations Access operations"><meta property="og:type" content="blog"><meta property="og:title" content="Pytorch_DeepLizard教程10-13：PyTorch中的tensor操作"><meta property="og:url" content="https://paddeyzhang.tech/2021/11/01/Pytorch_DeepLizard%E6%95%99%E7%A8%8B10-13%EF%BC%9APyTorch%E4%B8%AD%E7%9A%84tensor%E6%93%8D%E4%BD%9C/"><meta property="og:site_name" content="Paddey&#039;s Blog"><meta property="og:description" content="此笔记引用自文章知乎链接并修改 学习笔记教程10-13集讲了在PyTorch中对tensor的几种操作。 在Pytorch中对tensor的操作主要有以下四种类型：  Reshaping operations Element-wise operations Reduction operations Access operations"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://paddeyzhang.tech/img/og_image.png"><meta property="article:published_time" content="2021-11-01T06:46:13.000Z"><meta property="article:modified_time" content="2021-11-01T13:50:08.399Z"><meta property="article:author" content="paddeyzhang"><meta property="article:tag" content="机器学习 深度学习 Pytorch"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://paddeyzhang.tech/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://paddeyzhang.tech/2021/11/01/Pytorch_DeepLizard%E6%95%99%E7%A8%8B10-13%EF%BC%9APyTorch%E4%B8%AD%E7%9A%84tensor%E6%93%8D%E4%BD%9C/"},"headline":"Pytorch_DeepLizard教程10-13：PyTorch中的tensor操作","image":["https://paddeyzhang.tech/img/og_image.png"],"datePublished":"2021-11-01T06:46:13.000Z","dateModified":"2021-11-01T13:50:08.399Z","author":{"@type":"Person","name":"paddeyzhang"},"publisher":{"@type":"Organization","name":"Paddey's Blog","logo":{"@type":"ImageObject","url":"https://polaris-973.oss-cn-shenzhen.aliyuncs.com/img/b9c9db0f589cd855eb655fce4054665.jpg"}},"description":"此笔记引用自文章知乎链接并修改 学习笔记教程10-13集讲了在PyTorch中对tensor的几种操作。 在Pytorch中对tensor的操作主要有以下四种类型：  Reshaping operations Element-wise operations Reduction operations Access operations"}</script><link rel="canonical" href="https://paddeyzhang.tech/2021/11/01/Pytorch_DeepLizard%E6%95%99%E7%A8%8B10-13%EF%BC%9APyTorch%E4%B8%AD%E7%9A%84tensor%E6%93%8D%E4%BD%9C/"><link rel="icon" href="https://polaris-973.oss-cn-shenzhen.aliyuncs.com/img/b9c9db0f589cd855eb655fce4054665.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://polaris-973.oss-cn-shenzhen.aliyuncs.com/img/b9c9db0f589cd855eb655fce4054665.jpg" alt="Paddey&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-11-01T06:46:13.000Z" title="11/1/2021, 14:46:13">2021-11-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-11-01T13:50:08.399Z" title="11/1/2021, 21:50:08">2021-11-01</time></span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Pytorch/">机器学习 深度学习 Pytorch</a></span><span class="level-item">27 minutes read (About 4059 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Pytorch_DeepLizard教程10-13：PyTorch中的tensor操作</h1><div class="content"><p>此笔记引用自文章<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/108842363">知乎链接</a>并修改</p>
<h2 id="学习笔记"><a href="#学习笔记" class="headerlink" title="学习笔记"></a>学习笔记</h2><p>教程10-13集讲了在PyTorch中对tensor的几种操作。</p>
<p>在Pytorch中对tensor的操作主要有以下四种类型：</p>
<ol>
<li>Reshaping operations</li>
<li>Element-wise operations</li>
<li>Reduction operations</li>
<li>Access operations</li>
</ol>
<span id="more"></span>
<h3 id="Episode-10-of-33"><a href="#Episode-10-of-33" class="headerlink" title="- Episode 10 of 33"></a>- Episode 10 of 33</h3><p>这一集主要讲的是<strong>reshaping</strong>。</p>
<h3 id="（1）Reshape-操作"><a href="#（1）Reshape-操作" class="headerlink" title="（1）Reshape 操作"></a>（1）Reshape 操作</h3><p>先创建一个具有浮点数数据类型的3x4的二阶张量方便举例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; import torch</span><br><span class="line">&gt; t = torch.tensor([</span><br><span class="line">    [1,1,1,1],</span><br><span class="line">    [2,2,2,2],</span><br><span class="line">    [3,3,3,3]</span><br><span class="line">], dtype=torch.float32)</span><br></pre></td></tr></table></figure>
<p>张量 t 的 shape 或者 size 为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.size()</span><br><span class="line">torch.Size([3, 4])</span><br><span class="line"></span><br><span class="line">&gt; t.shape</span><br><span class="line">torch.Size([3, 4])</span><br></pre></td></tr></table></figure>
<p>前面在《教程5-7：tensor基本概念》中介绍过要注意reshape前后元素总量不变，而查看一个张量中的元素总量可以用以下两种方法：</p>
<p>① 计算各个 tensor 各个 axis 上的长度乘积：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; torch.tensor(t.shape).prod()</span><br><span class="line">tensor(12)</span><br></pre></td></tr></table></figure>
<p>② 或者直接用 <strong>numel() 函数</strong>（number of elements的缩写）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.numel()</span><br><span class="line">12</span><br></pre></td></tr></table></figure>
<p>二阶张量 t 中一共有12个元素，那么对t进行 reshape 可能的结果是将 3x4 改成 1x12、2x6、3x4、4x3、6x2、12x1，当然也可以用 t.reshape(2,2,3) 这样的方式 reshape 为 2x2x3 的三阶张量。</p>
<p>另外PyTorch中还有一个 view() 函数和 reshape() 具有相同的作用。</p>
<h3 id="（2）Flatten-操作"><a href="#（2）Flatten-操作" class="headerlink" title="（2）Flatten 操作"></a>（2）Flatten 操作</h3><p>在 reshape 的各种操作中，最常用的一种操作叫做 <strong>flatten 操作</strong>，就是把一个n阶的张量变形为只有一行的一阶张量，即变为 array 类型。<strong>这个操作通常在CNN中由卷积层传递到全连接层的时候发生</strong>。上一篇说过，卷积层的输出是很多的 feature map，每一个 feature map 都是一个二阶张量，但是全连接层的输入只能是数组，因此需要把卷积层输出的高阶张量压平变为一维数组，才能输入全连接层。</p>
<p>这里要说明的是，对二阶张量进行 flatten 的操作，是第一行数据不动，将第二行数据拼接至第一行数据末尾，再将第三行数据拼接到第二行数据的末尾，以此类推。</p>
<p>比如采用 reshape() 函数对张量 t 进行变换：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; print(t.reshape([1,12]))</span><br><span class="line">&gt; print(t.reshape([1,12]).shape)</span><br><span class="line">tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])</span><br><span class="line">torch.Size([1, 12])</span><br></pre></td></tr></table></figure>
<p>这里要注意的是，输出的张量有两层中括号，而且 shape 为“[1, 12]”，表明其在计算机中存储的形式仍然为二阶张量，只不过有一个 axis 的长度为 1 而已。<strong>要进一步将其转为一阶张量，需要用到 squeeze() 函数：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; print(t.reshape([1,12]).squeeze())</span><br><span class="line">&gt; print(t.reshape([1,12]).squeeze().shape)</span><br><span class="line">tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])</span><br><span class="line">torch.Size([12])</span><br></pre></td></tr></table></figure>
<p>这时注意到输出结果变为了一阶张量，因此 <strong>squeeze() 函数的作用就是去掉高阶张量中所有长度为 1 的 axes，使高阶张量降低阶数为低阶张量</strong>。</p>
<p>类似的还有 <strong>unsqueeze() 函数，为张量添加一个长度为1的 axis</strong>，相当于 squeeze() 的逆操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; print(t.reshape([1,12]).squeeze().unsqueeze(dim=0))</span><br><span class="line">&gt; print(t.reshape([1,12]).squeeze().unsqueeze(dim=0).shape)</span><br><span class="line">tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])</span><br><span class="line">torch.Size([1, 12])</span><br></pre></td></tr></table></figure>
<p>可以定义一个 flatten() 函数包含上述 reshape 和 squeeze 两步：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def flatten(t):</span><br><span class="line">    t = t.reshape(1, -1)</span><br><span class="line">    t = t.squeeze()</span><br><span class="line">    return t</span><br></pre></td></tr></table></figure>
<p><strong>这里在 reshape() 函数中的第二个参数填写了“-1”，是让计算机自己根据张量中的元素数量决定一维数组的长度。</strong>可以验证：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; flatten(t)</span><br><span class="line">tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])</span><br></pre></td></tr></table></figure>
<h3 id="（3）Concatenating-操作（级联）"><a href="#（3）Concatenating-操作（级联）" class="headerlink" title="（3）Concatenating 操作（级联）"></a>（3）Concatenating 操作（级联）</h3><p>视频最后讲了一下张量的拼接，称为 concatenating，对应的函数是 cat()。</p>
<p>例如现在有两个二阶张量：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; t1 = torch.tensor([</span><br><span class="line">    [1,2],</span><br><span class="line">    [3,4]</span><br><span class="line">])</span><br><span class="line">&gt; t2 = torch.tensor([</span><br><span class="line">    [5,6],</span><br><span class="line">    [7,8]</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>使用 cat() 函数同时指定拼接的方向（维数），可以得到一个拼接后的新张量。</p>
<p>沿第一个维度（行）拼接为4行2列：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; torch.cat((t1, t2), dim=0)</span><br><span class="line">tensor([[1, 2],</span><br><span class="line">        [3, 4],</span><br><span class="line">        [5, 6],</span><br><span class="line">        [7, 8]])</span><br></pre></td></tr></table></figure>
<p>沿第二个维度（列）拼接为2行4列：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; torch.cat((t1, t2), dim=1)</span><br><span class="line">tensor([[1, 2, 5, 6],</span><br><span class="line">        [3, 4, 7, 8]])</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Episode-11-of-33"><a href="#Episode-11-of-33" class="headerlink" title="- Episode 11 of 33"></a>- Episode 11 of 33</h3><p>这一集就讲了一个事情——如何只对一个高阶张量的部分维度进行 flatten 操作。</p>
<p>上一集只讲了对一张图片的二阶张量进行展平，但前面的章节说过，一次输入CNN的是不是一张而是一批图像，输入的是一个具有 [B, C, H, W] 形状的四阶张量。如果直接对这个张量进行展平，会把所有图像的所有颜色通道（或者 feature map）全都混在一起。而我们希望的是仅仅对一个 feature map 进行展平，而不把不同的图片、不同的颜色通道和不同的 feature map 都混合在一起。</p>
<p>所以我们要做的其实只是把 [B, C, H, W] 这个四阶张量的 H 和 W 两个维度展平。</p>
<p>为了方便讲解，举了一个 3x1x4x4 的四阶张量作为例子（灰度图片只有一个颜色通道）。</p>
<p>首先创建3个 4x4 的二阶张量代表3张 4x4 的图片，数字 i 只属于第 i 张图片：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">t1 = torch.tensor([</span><br><span class="line">    [1,1,1,1],</span><br><span class="line">    [1,1,1,1],</span><br><span class="line">    [1,1,1,1],</span><br><span class="line">    [1,1,1,1]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">t2 = torch.tensor([</span><br><span class="line">    [2,2,2,2],</span><br><span class="line">    [2,2,2,2],</span><br><span class="line">    [2,2,2,2],</span><br><span class="line">    [2,2,2,2]</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">t3 = torch.tensor([</span><br><span class="line">    [3,3,3,3],</span><br><span class="line">    [3,3,3,3],</span><br><span class="line">    [3,3,3,3],</span><br><span class="line">    [3,3,3,3]</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>为了将这三个二阶张量堆栈起来组成一个 batch，可以用 <strong>torch.stack() 函数</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; t = torch.stack((t1, t2, t3))</span><br><span class="line">&gt; t.shape</span><br><span class="line"></span><br><span class="line">torch.Size([3, 4, 4])</span><br></pre></td></tr></table></figure>
<p>此时还缺一个中间的颜色通道维度，采用 reshape() 函数来添加：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; t = t.reshape(3,1,4,4)</span><br><span class="line">&gt; t</span><br></pre></td></tr></table></figure>
<p>最后创建的四阶张量 t 为（注释标明了每一个维度的含义）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">tensor(</span><br><span class="line"># Batch</span><br><span class="line">[   # Channel</span><br><span class="line">    [   # Height</span><br><span class="line">        [   # Width</span><br><span class="line">            [1, 1, 1, 1],</span><br><span class="line">            [1, 1, 1, 1],</span><br><span class="line">            [1, 1, 1, 1],</span><br><span class="line">            [1, 1, 1, 1]</span><br><span class="line">        ]</span><br><span class="line">    ],</span><br><span class="line">    [</span><br><span class="line">        [</span><br><span class="line">            [2, 2, 2, 2],</span><br><span class="line">            [2, 2, 2, 2],</span><br><span class="line">            [2, 2, 2, 2],</span><br><span class="line">            [2, 2, 2, 2]</span><br><span class="line">        ]</span><br><span class="line">    ],</span><br><span class="line">    [</span><br><span class="line">        [</span><br><span class="line">            [3, 3, 3, 3],</span><br><span class="line">            [3, 3, 3, 3],</span><br><span class="line">            [3, 3, 3, 3],</span><br><span class="line">            [3, 3, 3, 3]</span><br><span class="line">        ]</span><br><span class="line">    ]</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<p>因为<strong>灰度图片只有一个颜色通道</strong>，所以要把 [B, C, H, W] 中从第二个维度起的后三个维度 C、H、W 都展平，这里<strong>采用PyTorch自带的 torch.flatten() 添加一个 start_dim=1 的参数即可</strong>（上一集中为了讲解方便自己定义了 flatten() 函数，但PyTorch中其实自带了功能更强的 torch.flatten() 函数）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.flatten(start_dim=1).shape</span><br><span class="line">torch.Size([3, 16])</span><br><span class="line"></span><br><span class="line">&gt; t.flatten(start_dim=1)</span><br><span class="line">tensor(</span><br><span class="line">[</span><br><span class="line">    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],</span><br><span class="line">    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],</span><br><span class="line">    [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]</span><br><span class="line">]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>可以看到输出结果保留了 batch 的维度，而把后面的维度都展平了，这样同一 batch 中的不同图片数据就不会混在一起了。</p>
<p>对于<strong>有三个颜色通道的 RGB 图片</strong>，不同的颜色通道（或者 feature map）也不应该混淆在一起，这时候<strong>把 torch.flatten() 函数的参数改为 start_dim=2 即可</strong>。</p>
<hr>
<h3 id="Episode-12-of-33"><a href="#Episode-12-of-33" class="headerlink" title="- Episode 12 of 33"></a>- Episode 12 of 33</h3><p>这一集讲的是张量的 <strong>element-wise operations</strong>（按元素逐项操作）。</p>
<blockquote>
<p>An <strong><em>element-wise\</em> operation</strong> operates on <strong>corresponding elements</strong> between tensors.</p>
</blockquote>
<p>而所谓的 corresponding elements 是指在张量中的位置相同，就是在各个 axis 上的索引编号相同的元素。</p>
<p>举例来说，有两个tensor：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; t1 = torch.tensor([</span><br><span class="line">    [1,2],</span><br><span class="line">    [3,4]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">&gt; t2 = torch.tensor([</span><br><span class="line">    [9,8],</span><br><span class="line">    [7,6]</span><br><span class="line">], dtype=torch.float32)</span><br></pre></td></tr></table></figure>
<p>那么 1 和 9 就是 corresponding elements，因为索引的 indexes 是一样的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt; t1[0][0]</span><br><span class="line">tensor(1.)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt; t2[0][0]</span><br><span class="line">tensor(9.)</span><br></pre></td></tr></table></figure>
<p>所以通过这一点强调的是，<strong>所有的 element-wise operations 都只能在具有相同 shape 的张量之间进行</strong>（不然元素对应不上啊）。</p>
<p>这里介绍的 element-wise operations 分为三类，一类是 arithmetic operations（算术操作），一类是 comparison operations（比较操作），还有一类是 function operations（函数操作，其实也可以归到算术操作里）。</p>
<h3 id="（1）Arithmetic-Operations"><a href="#（1）Arithmetic-Operations" class="headerlink" title="（1）Arithmetic Operations"></a>（1）Arithmetic Operations</h3><p>算数操作说的自然就是加减乘除之类的一些运算了呗，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt; t1 + t2</span><br><span class="line">tensor([[10., 10.],</span><br><span class="line">        [10., 10.]])</span><br><span class="line"></span><br><span class="line">&gt; t1 - t2</span><br><span class="line">tensor([[-8., -6.],</span><br><span class="line">        [-4., -2.]])</span><br><span class="line"></span><br><span class="line">&gt; t1 * t2</span><br><span class="line">tensor([[ 9., 16.],</span><br><span class="line">        [21., 24.]])</span><br><span class="line"></span><br><span class="line">&gt; t1 / t2</span><br><span class="line">tensor([[0.1111, 0.2500],</span><br><span class="line">        [0.4286, 0.6667]])</span><br></pre></td></tr></table></figure>
<p>所以可以看到，<strong>PyTorch中张量间的加减乘除四则运算都是 element-wise 的</strong>，相当于 matlab 中加“.”前缀的运算符。</p>
<p>不过和 matlab 一样的是，<strong>PyTorch中张量和数的运算也是 element-wise 的</strong>，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt; print(t1 + 2)</span><br><span class="line">tensor([[3., 4.],</span><br><span class="line">        [5., 6.]])</span><br><span class="line"></span><br><span class="line">&gt; print(t1 - 2)</span><br><span class="line">tensor([[-1.,  0.],</span><br><span class="line">        [ 1.,  2.]])</span><br><span class="line"></span><br><span class="line">&gt; print(t1 * 2)</span><br><span class="line">tensor([[2., 4.],</span><br><span class="line">        [6., 8.]])</span><br><span class="line"></span><br><span class="line">&gt; print(t1 / 2)</span><br><span class="line">tensor([[0.5000, 1.0000],</span><br><span class="line">        [1.5000, 2.0000]])</span><br></pre></td></tr></table></figure>
<p>也可以用函数命令来运算，结果是一样的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; print(t1.add(2))</span><br><span class="line"></span><br><span class="line">&gt; print(t1.sub(2))</span><br><span class="line"></span><br><span class="line">&gt; print(t1.mul(2))</span><br><span class="line"></span><br><span class="line">&gt; print(t1.div(2))</span><br></pre></td></tr></table></figure>
<p>但 t1 是一个 2x2 的二阶张量，而数 2 是一个零阶的张量，没有 shape，和之前说的 element-wise operations 需要在 shape 一样的张量之间进行不相符。这是因为接下来要介绍的PyTorch中的一个重要概念：</p>
<h3 id="（2）Broadcasting-Tensors"><a href="#（2）Broadcasting-Tensors" class="headerlink" title="（2）Broadcasting Tensors"></a>（2）Broadcasting Tensors</h3><p>Broadcasting 广播</p>
<p>Broadcasting tensor 就是把两个 shape 不一样的 tensor 匹配成 shape 一样的 sensor 的过程，匹配的方法是将 shape 小的 tensor 进行复制和拼接，使小的 tensor 变成和大的 tensor 具有一样的 shape。这是PyTorch在 shape 不同的张量之间进行操作之前会进行的一个步骤，即：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; np.broadcast_to(2, t1.shape)</span><br><span class="line">array([[2, 2],</span><br><span class="line">       [2, 2]])</span><br></pre></td></tr></table></figure>
<p>对于不同 shape 的 tensor 之间的操作也是如此：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt; t1 = torch.tensor([</span><br><span class="line">    [1,1],</span><br><span class="line">    [1,1]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line">&gt; t1.shape</span><br><span class="line">torch.Size([2, 2])</span><br><span class="line"></span><br><span class="line">&gt; t2 = torch.tensor([2,4], dtype=torch.float32)</span><br><span class="line">&gt; t2.shape</span><br><span class="line">torch.Size([2])</span><br></pre></td></tr></table></figure>
<p>t1 与 t2 相加时，PyTorch对 t2 进行了 broadcast 以匹配 t 的 shape：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; np.broadcast_to(t2.numpy(), t1.shape)</span><br><span class="line">array([[2., 4.],</span><br><span class="line">       [2., 4.]], dtype=float32)</span><br><span class="line"></span><br><span class="line">&gt; t1 + t2</span><br><span class="line">tensor([[3., 5.],</span><br><span class="line">        [3., 5.]])</span><br></pre></td></tr></table></figure>
<p><strong>在深度学习中，broadcasting 主要用在数据预处理（data preprocessing）和正则化（normalization routines）之中。</strong></p>
<p><strong>熟练掌握 broadcasting 的优势在于，在编程之中可以省略原本需要很多行代码进行手工张量增广和形状匹配的工作，而让PyTorch默认去完成，使代码更加简洁高效。</strong></p>
<p>这篇教程中给出了有关 broadcasting 的更详细介绍：</p>
<p><a href="https://link.zhihu.com/?target=https%3A//deeplizard.com/learn/video/6_33ulFDuCg">Broadcasting Explained - deeplizarddeeplizard.com/learn/video/6_33ulFDuCg</a></p>
<h3 id="（3）Comparison-Operations"><a href="#（3）Comparison-Operations" class="headerlink" title="（3）Comparison Operations"></a>（3）Comparison Operations</h3><p>比较操作就是两个张量之间按元素进行比较了（比如比大小），返回值的数据类型是 torch.bool（布尔运算值），即 True 和 False。</p>
<p>比如有这么一个 tensor：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; t = torch.tensor([</span><br><span class="line">    [0,5,0],</span><br><span class="line">    [6,0,7],</span><br><span class="line">    [0,8,0]</span><br><span class="line">], dtype=torch.float32)</span><br></pre></td></tr></table></figure>
<p>各项比较操作分别为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.eq(0)    # equal to 判断是否相等</span><br><span class="line">tensor([[True, False, True],</span><br><span class="line">        [False, True, False],</span><br><span class="line">        [True, False, True]])</span><br><span class="line"></span><br><span class="line">&gt; t.ge(0)    # greater than or equal to 大于等于</span><br><span class="line">tensor([[True, True, True],</span><br><span class="line">        [True, True, True],</span><br><span class="line">        [True, True, True]])</span><br><span class="line"></span><br><span class="line">&gt; t.gt(0)    # greater than 大于</span><br><span class="line">tensor([[False, True, False],</span><br><span class="line">        [True, False, True],</span><br><span class="line">        [False, True, False]])</span><br><span class="line"></span><br><span class="line">&gt; t.lt(0)    # less than 小于</span><br><span class="line">tensor([[False, False, False],</span><br><span class="line">        [False, False, False],</span><br><span class="line">        [False, False, False]])</span><br><span class="line"></span><br><span class="line">&gt; t.le(7)    # less than or equal to 小于等于</span><br><span class="line">tensor([[True, True, True],</span><br><span class="line">        [True, True, True],</span><br><span class="line">        [True, False, True]])</span><br></pre></td></tr></table></figure>
<p>可以发现PyTorch也是先对 number 类型的零维张量进行了 broadcasting 之后才进行比较操作。</p>
<h3 id="（4）Element-wise-Operations-using-Functions"><a href="#（4）Element-wise-Operations-using-Functions" class="headerlink" title="（4）Element-wise Operations using Functions"></a>（4）Element-wise Operations using Functions</h3><p>下面这些常用函数操作也都是 element-wise 的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.abs()    # absolute 取绝对值</span><br><span class="line">tensor([[0., 5., 0.],</span><br><span class="line">        [6., 0., 7.],</span><br><span class="line">        [0., 8., 0.]])</span><br><span class="line"></span><br><span class="line">&gt; t.sqrt()    # squrt root 求平方根</span><br><span class="line">tensor([[0.0000, 2.2361, 0.0000],</span><br><span class="line">        [2.4495, 0.0000, 2.6458],</span><br><span class="line">        [0.0000, 2.8284, 0.0000]])</span><br><span class="line"></span><br><span class="line">&gt; t.neg()    # negative 取负</span><br><span class="line">tensor([[-0., -5., -0.],</span><br><span class="line">        [-6., -0., -7.],</span><br><span class="line">        [-0., -8., -0.]])</span><br><span class="line"></span><br><span class="line">&gt; t.neg().abs()    # 先取负再取绝对值</span><br><span class="line">tensor([[0., 5., 0.],</span><br><span class="line">        [6., 0., 7.],</span><br><span class="line">        [0., 8., 0.]])</span><br></pre></td></tr></table></figure>
<p>最后补充一下，以下三个术语说的是同一个意思：</p>
<ul>
<li>Element-wise</li>
<li>Component-wise</li>
<li>Point-wise</li>
</ul>
<blockquote>
<p>Just keep this in mind if you encounter any of these terms <strong>in the wild</strong>.</p>
</blockquote>
<p>最后这个“in the wild”真是太逗了，你细品~</p>
<hr>
<h3 id="Episode-13-of-33"><a href="#Episode-13-of-33" class="headerlink" title="- Episode 13 of 33"></a>- Episode 13 of 33</h3><p>这一集讲PyTorch中的最后两种张量操作：</p>
<ul>
<li>Reduction operations</li>
<li>Access operations</li>
</ul>
<h3 id="（1）Reduction-Operations"><a href="#（1）Reduction-Operations" class="headerlink" title="（1）Reduction Operations"></a>（1）Reduction Operations</h3><blockquote>
<p>A <strong><em>reduction operation</em></strong> on a tensor is an operation that reduces the number of elements contained within the tensor.</p>
</blockquote>
<p>前面提过，张量是深度学习中使用的数据结构，我们使用张量不仅是用来进行数据的存储，更重要的是进行数据的访问和管理。</p>
<p>Reshaping operations 让我们能够重新排列元素；element-wise operations 让我们能够在两个张量的元素之间执行运算，<strong>reduction operations 让我们能够对单个张量内的元素执行运算</strong>。</p>
<p>以一个 3x3 二阶张量举例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; t = torch.tensor([</span><br><span class="line">    [0,1,0],</span><br><span class="line">    [2,0,2],</span><br><span class="line">    [0,3,0]</span><br><span class="line">], dtype=torch.float32)</span><br></pre></td></tr></table></figure>
<p>所谓的 reduction operations 其实特别简单：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.sum()    # 求元素之和</span><br><span class="line">tensor(8.)</span><br><span class="line"></span><br><span class="line">&gt; t.prod()    # 求元素乘积</span><br><span class="line">tensor(0.)</span><br><span class="line"></span><br><span class="line">&gt; t.mean()    # 求元素平均值</span><br><span class="line">tensor(.8889)</span><br><span class="line"></span><br><span class="line">&gt; t.std()    # 求元素标准差</span><br><span class="line">tensor(1.1667)</span><br></pre></td></tr></table></figure>
<p>之所以称之为 reduction operations 是因为他们的输出从原来输入的二阶张量变为了零阶张量，<strong>张量中总的元素数量减少了</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.numel()</span><br><span class="line">9</span><br><span class="line"></span><br><span class="line">&gt; t.sum().numel()</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">&gt; t.sum().numel() &lt; t.numel()</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<p>上述的操作在不指定 axis 时都是对张量中所有元素进行操作的，也可以<strong>指定 reduction operations 作用的 axis</strong>，得到的输出就会是一个不是只包含一个元素的张量了。</p>
<p>以一个 3x4（3行4列）的张量为例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt; t = torch.tensor([</span><br><span class="line">    [1,1,1,1],</span><br><span class="line">    [2,2,2,2],</span><br><span class="line">    [3,3,3,3]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">&gt; t.sum(dim=0)</span><br><span class="line">tensor([6., 6., 6., 6.])</span><br><span class="line"></span><br><span class="line">&gt; t.sum(dim=0).shape</span><br><span class="line">torch.Size([4])</span><br><span class="line"></span><br><span class="line">&gt; t.sum(dim=1)</span><br><span class="line">tensor([ 4.,  8., 12.])</span><br><span class="line"></span><br><span class="line">&gt; t.sum(dim=1).shape</span><br><span class="line">torch.Size([3])</span><br></pre></td></tr></table></figure>
<p>可以看到用 dim = 0 指定沿第一个 axis 操作，也就是按行数索引的方向相加，其实就是求每列的和，所以输出有张量有4个元素（4列）；与之相反 dim = 1 指定沿列索引的方向操作，就是求每行的和，所以输出有张量有3个元素（3行）。</p>
<p>这里要说明的是PyTorch中进行 reduction operations 之后会自动把输出的张量进行 squeeze() 操作，因此可以看到输出的直接就是一阶张量（都是一行，不区分行列），不再保留原来的行列信息，因此操作的时候需要自己想清楚。（可能是 matlab 惯的臭毛病吧）</p>
<h3 id="（2）Argmax-tensor-reduction-operation"><a href="#（2）Argmax-tensor-reduction-operation" class="headerlink" title="（2）Argmax tensor reduction operation"></a>（2）Argmax tensor reduction operation</h3><p>然后这里要讲一个特别的 reduction operation 叫做 <strong>Argmax</strong>（其实数学上应该加个空格吧 Arg max）。</p>
<p>数学上，我们用 max y(x) 来表示求函数 y(x) 的最大值，而 Arg max y(x) 就是求当 y 取得最大值的时候，对应的 x 的取值是多少。反映在张量之中，就是求张量中某个最大的元素的位置上（就是对应的索引序号）。</p>
<p>比如这个张量，最大值 5 在第 3 行第 4 列：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([</span><br><span class="line">    [1,0,0,2],</span><br><span class="line">    [0,3,3,0],</span><br><span class="line">    [4,0,0,5]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">&gt; t.max()</span><br><span class="line">tensor(5.)</span><br><span class="line"></span><br><span class="line">&gt; t.argmax()</span><br><span class="line">tensor(11)</span><br></pre></td></tr></table></figure>
<p>可以看到 argmax() 函数输出的只有一个零阶张量，而非刚才说的第 3 行第 4 列两个值，这是因为PyTorch先对高阶张量进行了 flatten() 操作，然后才去比较大小：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.flatten()</span><br><span class="line">tensor([1., 0., 0., 2., 0., 3., 3., 0., 4., 0., 0., 5.])</span><br><span class="line"></span><br><span class="line">&gt; t.flatten().argmax()</span><br><span class="line">tensor(11)</span><br></pre></td></tr></table></figure>
<p>同样 argmax() 函数也可以按指定的 axis 方向进行操作，比如：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.max(dim=0)</span><br><span class="line">(tensor([4., 3., 3., 5.]), tensor([2, 1, 1, 2]))</span><br><span class="line"></span><br><span class="line">&gt; t.argmax(dim=0)</span><br><span class="line">tensor([2, 1, 1, 2])</span><br><span class="line"></span><br><span class="line">&gt; t.max(dim=1)</span><br><span class="line">(tensor([2., 3., 5.]), tensor([3, 1, 3]))</span><br><span class="line"></span><br><span class="line">&gt; t.argmax(dim=1)</span><br><span class="line">tensor([3, 1, 3])</span><br></pre></td></tr></table></figure>
<p>这里 max() 函数会返回两个张量，第一个是按指定的 axis 方向找出的最大元素值，第二个是这些最大元素在指定的 axis 方向上的索引号。</p>
<p>argmax() 在神经网络中的用处在于：图像分类神经网络的输出层是一个包含 k 个元素的一阶张量（其中 k 是图片的类别数），张量中的元素是对应各个类别的 prediction values（翻译成置信概率？），所有的 prediction values 之和为 1。我们会取置信概率最大的那一种作为最终预测的类别（比如 a 类预测值 0.05，b 类 0.95，那就认为是 b 类）。这样就需要使用 argmax() 函数来找出最大的 prediction value 对应的是哪一类。</p>
<h3 id="（3）Access-Operations"><a href="#（3）Access-Operations" class="headerlink" title="（3）Access Operations"></a>（3）Access Operations</h3><p>最后就是讲一下如何访问和提取张量中的数据了。</p>
<p>比如求张量元素的平均值：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt; t = torch.tensor([</span><br><span class="line">    [1,2,3],</span><br><span class="line">    [4,5,6],</span><br><span class="line">    [7,8,9]</span><br><span class="line">], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">&gt; t.mean()</span><br><span class="line">tensor(5.)</span><br><span class="line"></span><br><span class="line">&gt; type(t.mean())</span><br><span class="line">torch.Tensor</span><br></pre></td></tr></table></figure>
<p>可以看到输出的数据类型仍然是一个张量（虽然是零阶张量），<strong>可以通过 item() 函数提取出零阶张量中的数据</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.mean().item()</span><br><span class="line">5.0</span><br><span class="line"></span><br><span class="line">&gt; type(t.mean().item())</span><br><span class="line">float</span><br></pre></td></tr></table></figure>
<p><strong>对于输出为含有多个元素的张量时，可以采用 tolist() 将其转为 Python list：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.mean(dim=0).tolist()</span><br><span class="line">[4.0, 5.0, 6.0]</span><br></pre></td></tr></table></figure>
<p><strong>或者用 numpy() 将其装换为 numpy 数组：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; t.mean(dim=0).numpy()</span><br><span class="line">array([4., 5., 6.], dtype=float32)</span><br></pre></td></tr></table></figure>
<p>关于 numpy 中高级索引的资料：</p>
<p><a href="https://link.zhihu.com/?target=https%3A//docs.scipy.org/doc/numpy/reference/arrays.indexing.html">Advanced indexing and slicingdocs.scipy.org/doc/numpy/reference/arrays.indexing.html</a></p>
<hr>
<p>那么至此deeplizard教程的 Part I，就是关于 PyTorch 和 tensor 的介绍就结束了。接下来会进入 Part II 去实际搭建一个CNN来进行图像分类。下一集将从数据集的介绍开始。</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>Pytorch_DeepLizard教程10-13：PyTorch中的tensor操作</p><p><a href="https://paddeyzhang.tech/2021/11/01/Pytorch_DeepLizard教程10-13：PyTorch中的tensor操作/">https://paddeyzhang.tech/2021/11/01/Pytorch_DeepLizard教程10-13：PyTorch中的tensor操作/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>paddeyzhang</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-11-01</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-11-01</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Pytorch/">机器学习 深度学习 Pytorch</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/11/01/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AD%A6%E4%B9%A0_FP-Growth%E7%AE%97%E6%B3%95/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">数据挖掘学习_FP-Growth算法</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/11/01/Pytorch_DeepLizard%E6%95%99%E7%A8%8B8-9%EF%BC%9A%E5%9C%A8PyTorch%E4%B8%AD%E5%88%9B%E5%BB%BAtensor/"><span class="level-item">Pytorch_DeepLizard教程8-9：在PyTorch钟创建tensor</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://polaris-973.oss-cn-shenzhen.aliyuncs.com/img/b9c9db0f589cd855eb655fce4054665.jpg" alt="PaddeyZhang"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">PaddeyZhang</p><p class="is-size-6 is-block">Master of Computer Science and Technology (GDUT)</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>GuangZhou, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">35</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">8</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/polaris-973" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:paddeyzhang@gmail.com"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/polaris-973"><i class="fab fa-github"></i></a></div></div></div><!--!--><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Latex/"><span class="level-start"><span class="level-item">Latex</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/OpenCV/"><span class="level-start"><span class="level-item">OpenCV</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2-Github-hexo/"><span class="level-start"><span class="level-item">创建博客_Github_hexo</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E5%88%B7%E9%A2%98/"><span class="level-start"><span class="level-item">刷题</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">数学理论学习</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"><span class="level-start"><span class="level-item">数据挖掘</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习 深度学习</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Pytorch/"><span class="level-start"><span class="level-item">机器学习 深度学习 Pytorch</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"><span class="level-start"><span class="level-item">统计学习方法</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-26T04:50:51.000Z">2023-02-26</time></p><p class="title"><a href="/2023/02/26/%E5%86%B3%E7%AD%96%E6%A0%91/">决策树</a></p><p class="categories"><a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">统计学习方法</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-24T08:12:30.000Z">2023-02-24</time></p><p class="title"><a href="/2023/02/24/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/">朴素贝叶斯法</a></p><p class="categories"><a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">统计学习方法</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-21T15:59:45.000Z">2023-02-21</time></p><p class="title"><a href="/2023/02/21/k%E8%BF%91%E9%82%BB%E6%B3%95/">k近邻法</a></p><p class="categories"><a href="/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">统计学习方法</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-19T14:25:29.963Z">2023-02-19</time></p><p class="title"><a href="/2023/02/19/%E5%A4%9A%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8BAccuracy,%20Precision,%20Recall%E5%92%8CF1-score%E7%9A%84%E8%B6%85%E7%BA%A7%E6%97%A0%E6%95%8C%E6%B7%B1%E5%85%A5%E6%8E%A2%E8%AE%A8/"> </a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-02-19T14:25:29.940Z">2023-02-19</time></p><p class="title"><a href="/2023/02/19/%E4%BB%80%E4%B9%88%E6%98%AF%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/"> </a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/02/"><span class="level-start"><span class="level-item">February 2023</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/12/"><span class="level-start"><span class="level-item">December 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">November 2021</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Latex/"><span class="tag">Latex</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2-%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"><span class="tag">傅里叶变换 数学理论</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%B7%E9%A2%98-leetcode-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"><span class="tag">刷题 leetcode 数据结构</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"><span class="tag">数据挖掘 数学理论</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Pytorch/"><span class="tag">机器学习 深度学习 Pytorch</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E5%BC%8F-%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"><span class="tag">泰勒展开式 数学理论</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0-%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"><span class="tag">线性代数 数学理论</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"><span class="tag">统计学习方法</span><span class="tag">4</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://polaris-973.oss-cn-shenzhen.aliyuncs.com/img/b9c9db0f589cd855eb655fce4054665.jpg" alt="Paddey&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2023 paddeyzhang</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>