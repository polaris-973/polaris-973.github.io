{"posts":[{"title":"CNN卷积神经网络","text":"CNN——卷积神经网络 简单阐述一下CNN的流程： （卷积——池化）x N次——扁平化——全连接 接下来我们具体阐述各个步骤的方法。 卷积（Convolution）图像卷积的步骤是选定滤波器大小，然后从输入图像的左上角开始，然后对两个矩阵做点积，得到输出的结果，该值就是输出矩阵a11的数值。随后依照stride（步长）来计算a12的值，以此类推。一般会有多个滤波器，因此卷积后的结果虽然矩阵的长宽变少了（输出矩阵边长为输入矩阵的边长-滤波器矩阵的边长+1，即m - n + 1），但是存在了高（高度为滤波器的个数）。 注意，如果是彩色图像，那么图像本身就存在3层（高度为3，RGB），因此滤波器也应该是三层。我们先忽律彩色图像，只考虑灰度图像（1层）。其实我们上面做的卷积操作本质上就是NN（Neural Network）从上面图可以看出来，左边矩阵被扁平化成为了一个vector，但是数值都是对应的，比如输入1、8的数值都是1, 2、3、4的数值都是0。然后滤波器卷积中有点积的这个过程就是设置权重与输入相乘的过程。第一个框完成后，按照stride为1的步长进行第二框点乘时，权重（weight）是不变的（因为滤波器的数值是没有改变的），只是连线的输入改变了。到此，卷积的过程就结束清楚了。 这里要解释一下滤波器要按照什么标准来设置呢？其实滤波器的参数是学习出来的，并不是人为设置的。 池化（Pooling）这是着重讲解一下Max Pooling。 经过卷积过后，假设得到了44的矩阵 * 然后处理22的区域，选择出这4个当中最大的保留，其余全部舍弃。那么舍弃掉一部分数据的作用我首先想到的就是减少计算量，但是它最重要的作用还是下采样（downsamping）。并且池化可以滤除掉大部分冗余的信息，而保留少量的有用信息。 所以经过卷积和池化，实际上就在提取图像中有用信息，而去除掉了冗余的信息。 扁平化（Flatten）这个从图上就能看出比较简单，就是降维，转换为1维的vector。 CNN in Keras解释一下代码Convolution2D（25，3，3），25指的是滤波器的个数是25个，即输出结果的高度是25。3，3指的是滤波器矩阵是3x3的。input_shape(1,28,28) 中，1指的灰度图像，如果是3就是彩色图像，2828是图像矩阵的大小。MaxPooling2D(2,2)指的是最大池化法的方框设置为22，因此相当于在4个数值中选择只保留一个，*所以总体数据量就得除以4。然后经过卷积后的输出矩阵的大小是如何求得的，上面已经讲过不再赘述。","link":"/2021/10/06/CNN%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"title":"Hexo使用攻略-添加分类及标签","text":"1、创建“分类”选项1.1 生成“分类”页并添加tpye属性打开命令行，进入博客所在文件夹。执行命令 1$ hexo new page categories 成功后会提示： 1INFO Created: ~/Documents/blog/source/categories/index.md 根据上面的路径，找到index.md这个文件，打开后默认内容是这样的： 1234---title: 文章分类date: 2017-05-27 13:47:40--- 添加type: &quot;categories&quot;到内容中，添加后是这样的： 12345---title: 文章分类date: 2017-05-27 13:47:40type: &quot;categories&quot;--- 保存并关闭文件。 1.2 给文章添加“categories”属性打开需要添加分类的文章，为其添加categories属性。下方的categories: web前端表示添加这篇文章到“web前端”这个分类。注意：hexo一篇文章只能属于一个分类，也就是说如果在“- web前端”下方添加“-xxx”，hexo不会产生两个分类，而是把分类嵌套（即该文章属于 “- web前端”下的 “-xxx ”分类）。 123456---title: jQuery对表单的操作及更多应用date: 2017-05-26 12:12:57categories: - web前端--- 至此，成功给文章添加分类，点击首页的“分类”可以看到该分类下的所有文章。当然，只有添加了categories: xxx的文章才会被收录到首页的“分类”中。 2、创建“标签”选项2.1 生成“标签”页并添加tpye属性打开命令行，进入博客所在文件夹。执行命令 1$ hexo new page tags 成功后会提示： 1INFO Created: ~/Documents/blog/source/tags/index.md 根据上面的路径，找到index.md这个文件，打开后默认内容是这样的： 1234---title: 标签date: 2017-05-27 14:22:08--- 添加type: &quot;tags&quot;到内容中，添加后是这样的： 12345---title: 文章分类date: 2017-05-27 13:47:40type: &quot;tags&quot;--- 保存并关闭文件。 2.2 给文章添加“tags”属性打开需要添加标签的文章，为其添加tags属性。下方的tags:下方的- jQuery - 表格 - 表单验证就是这篇文章的标签了 12345678910---title: jQuery对表单的操作及更多应用date: 2017-05-26 12:12:57categories: - web前端tags:- jQuery- 表格- 表单验证--- 至此，成功给文章添加分类，点击首页的“标签”可以看到该标签下的所有文章。当然，只有添加了tags: xxx的文章才会被收录到首页的“标签”中。 细心的朋友可能已经发现，这两个的设置几乎一模一样！是的，没错，思路都是一样的。所以我们可以打开scaffolds/post.md文件，在tages:上面加入categories:,保存后，之后执行hexo new 文章名命令生成的文件，页面里就有categories:项了。 scaffolds目录下，是新建页面的模板，执行新建命令时，是根据这里的模板页来完成的，所以可以在这里根据你自己的需求添加一些默认值。","link":"/2021/10/04/Hexo%E4%BD%BF%E7%94%A8%E6%94%BB%E7%95%A5-%E6%B7%BB%E5%8A%A0%E5%88%86%E7%B1%BB%E5%8F%8A%E6%A0%87%E7%AD%BE/"},{"title":"Latex介绍与基础教程","text":"","link":"/2021/12/07/Latex%E4%BB%8B%E7%BB%8D%E4%B8%8E%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/"},{"title":"","text":"OLAP和数据立方体 [TOC] 数据仓库数据仓库的目的是构建面向分析的集成化数据环境，为企业提供决策支持（Decision Support）。其实数据仓库本身并不“生产”任何数据，同时自身也不需要“消费”任何的数据，数据来源于外部，并且开放给外部应用。因此数据仓库的基本架构主要包含的是数据流入流出的过程，可以分为三层——源数据、数据仓库、数据应用： 数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。 多维数据模型多维数据模型是为了满足用户从多角度多层次进行数据查询和分析的需要而建立起来的基于事实和维的数据库模型，其基本的应用是为了实现OLAP（Online Analytical Processing）。 事实表和维表事实表是用来记录具体事件的，包含了每个事件的具体要素，以及具体发生的事情；维表则是对事实表中事件的要素的描述信息。比如一个事件会包含时间、地点、人物、事件，事实表记录了整个事件的信息，但对时间、地点和人物等要素只记录了一些关键标记，比如事件的主角叫“Michael”，那么Michael到底“长什么样”，就需要到相应的维表里面去查询“Michael”的具体描述信息了。基于事实表和维表就可以构建出多种多维模型，包括星形模型、雪花模型和星座模型。这是一个最简单的星形模型的实例。事实表里面主要包含两方面的信息：维和度量，维的具体描述信息记录在维表，事实表中的维属性只是一个关联到维表的键，并不记录具体信息；度量一般都会记录事件的相应数值，比如这里的产品的销售数量、销售额等。维表中的信息一般是可以分层的，比如时间维的年月日、地域维的省市县等，这类分层的信息就是为了满足事实表中的度量可以在不同的粒度上完成聚合，比如2010年商品的销售额，来自上海市的销售额等。 数据立方体和OLAP数据立方体（Data Cube），只是多维模型的一个形象的说法。立方体其本身只有三维，但多维模型不仅限于三维模型，可以组合更多的维度。OLAP（On-line Analytical Processing，联机分析处理）是在基于数据仓库多维模型的基础上实现的面向分析的各类操作的集合。其与传统的OLTP（On-line Transaction Processing，联机事务处理）的区别如下： OLAP的基本操作OLAP的操作是以查询——也就是数据库的SELECT操作为主，但是查询可以很复杂，比如基于关系数据库的查询可以多表关联，可以使用COUNT、SUM、AVG等聚合函数。OLAP正是基于多维模型定义了一些常见的面向分析的操作类型是这些操作显得更加直观。 OLAP的多维分析操作包括：钻取（Drill-down）、上卷（Roll-up）、切片（Slice）、切块（Dice）以及旋转（Pivot）。 钻取（Drill-down）：在维的不同层次间的变化，从上层降到下一层，或者说是将汇总数据拆分到更细节的数据，比如通过对2010年第二季度的总销售数据进行钻取来查看2010年第二季度4、5、6每个月的消费数据，如上图；当然也可以钻取浙江省来查看杭州市、宁波市、温州市……这些城市的销售数据。 上卷（Roll-up）：钻取的逆操作，即从细粒度数据向高层的聚合，如将江苏省、上海市和浙江省的销售数据进行汇总来查看江浙沪地区的销售数据，如上图。 切片（Slice）：选择维中特定的值进行分析，比如只选择电子产品的销售数据，或者2010年第二季度的数据。 切块（Dice）：选择维中特定区间的数据或者某批特定值进行分析，比如选择2010年第一季度到2010年第二季度的销售数据，或者是电子产品和日用品的销售数据。 旋转（Pivot）：即维的位置的互换，就像是二维表的行列转换，如图中通过旋转实现产品维和地域维的互换。 维和立方维（Dimension）维是用于从不同角度描述事物特征的，一般维都会有多层（Level），每个Level都会包含一些共有的或特有的属性（Attribute），可以用下图来展示下维的结构和组成：以时间维为例，时间维一般会包含年、季、月、日这几个Level，每个Level一般都会有ID、NAME、DESCRIPTION这几个公共属性，这几个公共属性不仅适用于时间维，也同样表现在其它各种不同类型的维。 Hierarchy 上面这个结构的维是无法直接应用于OLAP的，OLAP需要基于有层级的自上而下的钻取，或者自下而上地聚合。所以每一个维必须有Hierarchy，至少有一个默认的，当然可以有多个，见下图：有了Hierarchy，维里面的Level就有了自上而下的树形结构关系，也就是上层的每一个成员（Member）都会包含下层的0个或多个成员，也就是树的分支节点。这里需要注意的是每个Hierarchy树的根节点一般都设置成所有成员的汇总（Total），当该维未被OLAP中使用时，默认显示的就是该维上的汇总节点，也就是该维所有数据的聚合（或者说该维未被用于细分）。Hierarchy中的每一层都会包含若干个成员（Member），还是以时间维，假设我们建的是2006-2015这样一个时间跨度的时间维，那么最高层节点仅有一个Total的成员，包含了所有这10年的时间，而年的那层Level中包含2006、2007…2015这10个成员，每一年又包含了4个季度成员，每个季度包含3个月份成员…… 每个Hierarchy都包含了一个树形结构，但维中也可以包含多个Hierarchy，正如上图所示，维中的Hierarchy相互独立地构建了自己的树形结构。还是以时间维为例，时间维可以根据日历（Calendar）时间组建日历的Hierarchy，也可以根据财务（Fiscal）时间组建财务的Hierarchy，而其中财务季度的划分可能并不与日历一致，基于这种多样的Hierarchy，我们在组建多维模型时可以按需选择合适的，比如给财务部的数据分析模型选用财务Hierarchy，而其他部门的分析人员显然希望看到日历样式的Hierarchy，这样就完美地满足了不同的需求。 立方（Cube）这里所说的立方其实就是多维模型中间的事实表（Fact Table），它会引用所有相关维的维主键作为自身的联合主键，加上度量（Measure）和计算度量（Calculated Measure）就组成了立方的结构：度量是用于描述事件的数字尺度，比如网站的浏览量（Pageviews）、访问量（Visits），再如电子商务的订单量、销售额等。度量是实际储存于物理表中的，而计算度量则没有，计算度量是通过度量计算得到的，比如同比（如去年同期的月利润）、环比（如上个月的利润）、利率（如环比利润增长率）、份额（如该月中某类产品利润所占比例）、累计（如从年初到当前的累加利润）、移动平均（如最近7天的平均利润额）等，这些计算度量在Oracle中都可以借助分析函数直接计算得到，相信大部分的OLAP组件都会提供类似在时间序列上的分析功能。而这些计算度量往往对于分析而言更具意义，立方中借助与各个维的关联关系从不同的角度和层面来展现这些度量。 数据立方体实例数据立方体是数据仓库和OLAP的基本工具。从概念上讲，数据立方体是一个多层次、多维的数据库，具有各种不同的粒度聚合。它是group-by操作符的泛化，包含与维列表的所有可能组合对应的group-bys。 例子：在一个marketing management data warehouse中，以模式 sales(Store, Product, Season, Sale) 进行数据收集。基本事实表如下图，属性Store, Product，Season被称为维度，Sale被称为度量。数据立方体是由{Store, Product, Season}的每个子集组成的groupBy分组（共有8个groupBy），使用诸如AVG(Sale)这样的聚合函数，进行分组聚合返回的结果集。每个group by对应于一组单元格。如下图所示，维度中的符号“*”表示将该维度一般化，以便匹配其域中的任何值。数据立方体如下图： 数据立方体物化分组的属性的组合叫方体（cuboid），数据单元（cell）就是方体中的一个元组，分组的属性有 i 个的方 体称为 i 维方体。例如创建一个数据仓库 sales 用来记录商店的销售，包括四个维 分别是 Product（产品）、Time（时间）、Location（城市）、Sales（销量），将 不同属性聚集数据可以形成 0 维方体，1 维方体，2 维方体，3 维方体，4 维方体 方体形成的数据立方体的格如图所示。数据立方体是由原始数据事实表中记录的所有方体构成的。通过原始数据来构建数据仓库时，直接获得的只有包含数据立方体中的基本方体（上述四维方体），而想要获得所有方体，必须通过相关操作聚集物化。对于一个 n 维数据立方体来说，包含的方体的总数为：其中 Li 为第 i 个维度的维度层次（level）数。 所以对于一个多维度、多层次的数据立方体，其方体的数量多，计算全部的方体是耗时耗空间的工作。因此对于不同体量的数据立方体应 该采取不同的物化策略，对于数据立方体的物化方法，大致包括：全物化、部分 物化、不物化这三种。 全物化是指计算并存储当前数据可以聚集物化获得的所有方体的结果，覆盖整个数据立方体。当维度、维度层次数量较多时，全物化需要花费大量的时间和 空间来计算和存储这些方体。全物化带来的好处主要是可以显著提高查询的效率。 由于所有的方体都已经被计算和存储，因而查询时可以直接获得结果。 部分物化是指针对数据立方体可以获得的方体，进行有选择性地计算并部分 存储，这种方法需要花费旳时间少于全物化，具体的时间与选择物化的方式有关。 对于查询而言，如果查询的数据是已经物化的，则查询效率高、若查询的数据没 有被物化，则查询效率与不物化相同。 不物化是指仅保留数据立方体中的基本方体，即最详细的记录，不计算任何其他聚集后的方体。这种方法虽然省时省空间，但是在查询时，只要查询记录不是基本方体，都需要进行相关的计算才能获得查询结果，查询效率最差。 上一节中的数据立方体实例即为全物化。 针对于数据立方体存储的研究，很多学者提出了不同的压缩模型，例如冰山立方体、封闭立方体、商立方体等等。封闭立方体在物化过程中，只保存了所有等价类的上界。因此在封闭立方体中，只包含封 闭单元。传统的封闭立方体虽然可以有效地对原始立方体进行压缩，但是它存在一个缺点：无法对非单调聚集函数进行支持。冰山立方体和封闭立方体都实现了数据立方体的压缩，但是没有考虑方体之间的语义关系。然而在大部分的数据查询 中 ， 都 需 要 通 过 方 体 间 的 语 义 来 获 得 结 果 。 在 这 个 前 提 下 Laks V.S.Lakshmanan等首次提出了商立方体的数据压缩模型。 参考博客：http://webdataanalysis.net/web-data-warehouse/data-cube-and-olap/http://webdataanalysis.net/web-data-warehouse/data-warehouse-frame/http://webdataanalysis.net/web-data-warehouse/multidimensional-data-model/","link":"/2023/02/19/OLAP%E5%92%8C%E6%95%B0%E6%8D%AE%E7%AB%8B%E6%96%B9%E4%BD%93/"},{"title":"Pytorch_DeepLizard教程1-4：PyTorch简介","text":"此笔记引用自文章知乎链接并修改 学习笔记教程的1-4集属于教程第一个部分的Section 1。 - Episode 1 of 33第一集是课程大纲。 - Episode 2 of 33第二集介绍PyTorch，其中引用了很多创始人的视频片段，还挺有意思。 这里我觉得比较有用的是PyTorch中这几个包的名称和用途： torch - The top-level PyTorch package and tensor library. torch.nn - A subpackage that contains modules and extensible classes for building neural networks. torch.autograd - A subpackage that supports all the differentiable Tensor operations in PyTorch. torch.nn.functional - A functional interface that contains typical operations used for building neural networks like loss functions, activation functions, and convolution operations. torch.optim - A subpackage that contains standard optimization operations like SGD and Adam. torch.utils - A subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier. torchvision - A package that provides access to popular datasets, model architectures, and image transformations for computer vision. 其中2-6都是torch这个包的子包，而torchvision是一个独立的包（未来可能会整合进torch），所以之前下载安装的时候也是安装的torch和torchvision两个包。 还有就是Pytorch哲学（有点Python之禅的感觉）： Philosophy of PyTorch Stay out of the way（上善若水，不要让学习PyTorch成为负担） Cater to the impatient（包容缺乏耐心之人） Promote linear code-flow（推进线性工作流）——还不太懂 Full interop with the Python ecosystem（融入Python生态） Be as fast as anything else（和别的DL框架一样快） - Episode 3 of 33第三集介绍PyTorch的安装，这部分已经在前面的笔记中包含了，可以参考： 季节：【从零开始的机器学习实践笔记】01b-利用Anaconda配置Python编程环境7 赞同 · 0 评论文章 视频最后聊到NVIDIA GPU的CUDA加速，还让老黄出来上了个镜。 - Episode 4 of 33第四集专门介绍GPU和CUDA，顺便给NVIDIA吹了一波彩虹屁（误）。 （1）并行计算与神经网络相比主流消费级CPU几个核心十几个线程这种水平，现在的高端GPU可能有数千个核心（当然跟CPU的核心性能不能同日而语），GPU是专门为了大规模并行计算而设计的处理器。 并行计算是一种计算类型，其中特定的计算被分解为可以同时执行的独立的较小计算。然后将所得的计算重新组合或同步，以形成原始较大计算的结果。 一幅图像成千上万的像素之间其实并没有相互依赖关系，完全可以并行计算，所以我们用GPU作为显卡的核心来计算和生成图像，GPU就是这么得名的。 而神经网络正是一个典型的并行计算任务，甚至是embarrassingly parallel的： An embarrassingly parallel task is one where little or no effort is needed to separate the overall task into a set of smaller tasks to be computed in parallel. 在这里以CNN处理图像问题举例，卷积层中，filter每次在原图像（蓝色）上平移一个像素进行一次计算，得到一个新的像素（绿色），而这个过程中每一次计算都是一个简单任务，且都与其他计算没有依赖关系，完全可以同时由不同的小的core并行计算。（好比做你100次加减法肯定快不过100个小学生同时做） （2）GPU和CUDACUDA是NVIDIA家的技术，虽然 AMD Yes但是在深度学习这个领域AMD还差得远。 CUDA是怎么回事可以看这个图： GPU是底层的硬件，CUDA是GPU之上用于支持GPU进行并行计算的软件和API，在CUDA的基础上NVIDIA开发了很多支持深度学习的库来供开发者使用，比如cuDNN。位于CUDA和cuDNN之上的是PyTorch，是最终支持深度学习的顶层应用程序。 这里要说明的是，你其实并不需要知道怎么下载安装和使用CUDA的API和库，因为PyTorch安装时就附带了（就像前面讲过的Anaconda一样贴心），并且已经将功能集成了进去，你只要有一块支持CUDA的NVIDIA显卡就行了。 （3）在PyTorch中使用GPU计算在PyTorch中使用GPU计算非常简单，只需要使用cuda()命令将tensor指定给GPU即可。 例如首先创建一个tensor并打印： 12&gt; t = torch.tensor([1,2,3])&gt; t 输出为： 1tensor([1, 2, 3]) 上述方法创建的tensor默认是使用CPU计算的，利用cuda()将其转移给GPU： 12&gt; t = t.cuda()&gt; t 输出为： 1tensor([1, 2, 3], device='cuda:0') 显示 ‘cuda:0’ 是因为PyTorch可以选择性指定给某一块GPU，对于有多块显卡的情况下可以指定某一块显卡计算。 这里要说明的是，使用GPU计算不一定比CPU快。因为将tensor从CPU和GPU之间转移的操作代价较高，所以对于简单的计算，多一步转移操作再用GPU计算可能反而没有直接在CPU中计算快。 ok，这一篇就到这，下一篇开始介绍张量（tensor）了。","link":"/2021/10/31/Pytorch_DeepLizard%E6%95%99%E7%A8%8B1-4%EF%BC%9APyTorch%E7%AE%80%E4%BB%8B/"},{"title":"Pytorch_DeepLizard教程25-26：训练CNN","text":"学习笔记这篇笔记讲的是单个epoch（episode 25）以及多epoch（episode 26）训练卷积神经网络所需的步骤。 - Episode 25 of 33到目前为止，在本系列中，我们了解了 Tensor，并且我们已经了解了 PyTorch 神经网络的所有知识。我们现在准备开始 训练过程。 准备数据 构建模型 训练模型 计算损失、梯度并更新权重 分析模型的结果 在训练期间，我们会向前传播。然后，假设我们从数据中获取一个batch的数据，传入构建好的CNN。经CNN预测获得输出后，我们将预测输出与实际标签进行比较，一旦我们知道预测值与实际标签的接近程度，我们就会调整网络内部的权重，使网络预测的值更接近到真实值（标签）。 所有这些都是针对单个batch的，我们对每个batch重复此过程，直到我们覆盖训练集中的每个样本。在我们完成了所有batch的这个过程并传递了训练集中的每个样本后，我们说一个 epoch已经完成。我们使用epoch这个词 来表示我们整个训练集被覆盖的时间段。 在整个训练过程中，我们会根据需要进行尽可能多的 epoch 以达到我们期望的准确度。 我们有以下步骤： 从训练集中获取一个batch的数据。 将这个batch传递到网络。 计算loss（预测值和真实值之间的差异）。 根据网络权重计算损失函数的梯度（gradient）。 使用梯度更新权重以减少loss。 重复步骤 1-5，直到完成一个 epoch。 对达到最小loss所需的尽可能多的 epoch 重复步骤 1-6。 训练过程 由于我们在上一集中禁用了 PyTorch 的梯度跟踪功能，因此我们需要确保将其重新打开（默认情况下是打开的）。 12&gt; torch.set_grad_enabled（True）0x15b22d012b0&gt; forward pass（前向传播）准备 12345&gt; network = Network() # 创建network实例&gt; train_loader = torch.utils.data.DataLoader(train_set, batch_size= 100 ) &gt; batch = next ( iter (train_loader)) # 获取一个batch &gt; images, labels = batch 计算loss 为此，我们将使用函数cross_entropy()，是PyTorch 的nn.functionalAPI 中提供的损失函数。一旦我们得到了loss，我们就可以打印它，并使用我们此前创建的函数检查正确预测的数量 。 12345&gt; preds = network(images) &gt; loss = F.cross_entropy(preds, labels) # 计算损失&gt; loss.item() 2.307542085647583 &gt; get_num_correct(preds, labels) 9 附函数 get_num_correct(preds, labels) 12def get_num_correct(preds, labels): return preds.argmax(dim=1).eq(labels).sum().item() 该cross_entropy()函数返回一个标量值，因此我们使用该item()方法将损失打印为 Python 中的数字。我们得到9即是传入一个batch（100个样本）后，CNN的预测有9个是正确的，因为我们有10个预测类，概率约为10%，这就是我们想通过随机猜测的期望。 计算梯度使用 PyTorch 计算梯度非常容易。由于我们的网络是继承自PyTorch 中的nn.Module，因此 PyTorch 在幕后创建了一个计算图。当我们的张量在我们的网络中向前流动时，所有的计算都被添加到图中。然后 PyTorch 使用计算图来计算loss函数相对于网络权重的梯度。 在计算梯度之前，让我们验证一下当前conv1层内没有梯度。梯度是可以在grad（梯度的缩写）属性中访问的张量。 12&gt; network.conv1.weight.gradNone 为了 计算梯度，我们backward()在损失张量上调用该方法，如下所示： 1loss.backward() # 计算梯度 现在，loss函数的梯度已存储在权重张量中。 12&gt; network.conv1.weight.grad.shape torch.Size([ 6 , 1 , 5 , 5 ]) 优化器使用这些梯度来更新相应的权重。为了创建我们的优化器，我们使用torch.optim（optim：optimizer的缩写，优化器）包,这个包包含了许多我们可以使用的优化算法。我们的例子将使用Adam优化算法。 更新权重给Adam类构造函数，然后我们将network的参数传递给`optim.Adam（这是优化器能够访问梯度的方式），再传递learning rate（ 学习率 ）这个超参数。 最后，我们更新权重所要做的就是告诉optimizer使用梯度向损失函数最小值的方向步进。 12optimizer = optim.Adam(network.parameters(), lr= 0.01 ) optimizer.step() # 更新权重 当step()函数被调用时，optimizer使用存储在网络参数中的梯度来更新权重。这意味着如果我们再次通过网络传递同一批，我们应该期望我们的损失会减少。检查这一点，我们可以看到确实如此： 12345&gt; preds = network(images) &gt; loss.item() &gt; loss = F.cross_entropy(preds, labels) 2.262690782546997 &gt; get_num_correct(preds, labels) 15 使用单个批次进行训练我们可以通过以下方式总结单个batch训练的代码： 123456789101112131415161718network = Network() train_loader = torch.utils.data.DataLoader(train_set, batch_size= 100 ) optimizer = optim.Adam(network.parameters(), lr= 0.01 ) batch = next ( iter (train_loader)) # 获取批量images,labels = batch preds = network(images) # Pass Batch loss = F.cross_entropy(preds, labels) # 计算损失loss.backward() # 计算梯度optimizer.step() # 更新权重print ( 'loss1:' , loss .item()) preds = network(images) loss = F.cross_entropy(preds, labels)打印( 'loss2:' , loss.item()) 输出: 12loss1: 2.3034827709198loss2: 2.2825052738189697 Episode 26 of 33在这一集中，我们将学习如何使用 Python 为卷积神经网络构建训练循环。 单批训练我们可以通过以下方式总结单批训练的代码： 123456789101112131415161718network = Network()train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)optimizer = optim.Adam(network.parameters(), lr=0.01)batch = next(iter(train_loader)) # Get Batchimages, labels = batchpreds = network(images) # Pass Batchloss = F.cross_entropy(preds, labels) # Calculate Lossloss.backward() # Calculate Gradientsoptimizer.step() # Update Weightsprint('loss1:', loss.item())preds = network(images)loss = F.cross_entropy(preds, labels)print('loss2:', loss.item()) 我们注意到的一件事是，每次运行此代码时，我们都会得到不同的结果。这是因为模型每次都是在顶部创建的，而我们从之前的帖子中知道模型权重是随机初始化的。 现在让我们看看如何修改此代码以使用所有批次进行训练，从而使用整个训练集 采用所有batch进行训练（一个epoch）现在，要使用数据加载器（data_loader）中可用的所有batch进行训练，我们需要进行一些更改并添加一行代码： 1234567891011121314151617181920212223242526network = Network()train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)optimizer = optim.Adam(network.parameters(), lr=0.01)total_loss = 0total_correct = 0for batch in train_loader: # Get Batch images, labels = batch preds = network(images) # Pass Batch loss = F.cross_entropy(preds, labels) # Calculate Loss optimizer.zero_grad() loss.backward() # Calculate Gradients optimizer.step() # Update Weights total_loss += loss.item() total_correct += get_num_correct(preds, labels)print( &quot;epoch:&quot;, 0, &quot;total_correct:&quot;, total_correct, &quot;loss:&quot;, total_loss) 我们将创建一个 for 循环来迭代所有batch，而不是从我们的数据加载器中获取单个batch。 由于我们的训练集中有样本，因此我们将进行迭代。出于这个原因，我们将从循环中删除打印语句，并跟踪总的loss并且最后打印它们的正确预测的总数。 60,000 / 100 = 600 关于这600个迭代需要注意的是，我们的权重将在循环结束时更新600次，即600个batch更新600次权重。如果我们提高batch_size，那么更新次数这个数字会下降，如果我们降低batch_size这个数字会上升。 最后，在我们对张量loss调用backward()方法后，我们知道梯度将被计算并添加到我们网络参数的属性grad中。出于这个原因，我们需要将这些梯度先归零。我们可以使用优化器附带的一个方法zero_grad()来做到这一点。 我们已准备好运行此代码。这次代码将花费更长的时间，因为循环正在处理600个batch。 1epoch: 0 total_correct: 42104 loss: 476.6809593439102 我们得到了结果，我们可以看到60000样本中预测正确的数是42104。 12&gt; total_correct / len (train_set) 0.7017333333333333 仅在一个epoch之后（一次完整的数据传递）预测效果就很好了。即使我们做了一个 epoch，我们仍然必须记住权重是被更新了600次，这个更新次数取决于我们的batch大小。 多epoch训练要执行多个 epoch，我们所要做的就是将此代码放入 for 循环中。我们还将把epoch number添加到打印语句中。 123456789101112131415161718192021network = Network() train_loader = torch.utils.data.DataLoader(train_set, batch_size= 100 ) optimizer = optim.Adam(network.parameters(), lr= 0.01 ) for epoch in range ( 10 ): total_loss = 0 total_correct = 0 for batch in train_loader: # Get Batch images, labels = batch preds = network(images) # Pass Batch loss = F.cross_entropy(preds, labels) # 计算损失 optimizer.zero_grad() loss.backward()# 计算梯度 optimizer.step() # 更新权重 total_loss += loss.item() total_correct += get_num_correct(preds, labels) print ( &quot;epoch&quot; , epoch, &quot;total_correct:&quot; , total_correct, &quot;loss:&quot; , total_loss ) 我们能得到每个epoch的结果： 12345678910epoch 0 total_correct: 43301 loss: 447.59147948026657epoch 1 total_correct: 49565 loss: 284.43429669737816epoch 2 total_correct: 51063 loss: 244.08825492858887epoch 3 total_correct: 51955 loss: 220.5841210782528epoch 4 total_correct: 52551 loss: 204.73878084123135epoch 5 total_correct: 52914 loss: 193.1240530461073epoch 6 total_correct: 53195 loss: 184.50964668393135epoch 7 total_correct: 53445 loss: 177.78808392584324epoch 8 total_correct: 53629 loss: 171.81662507355213epoch 9 total_correct: 53819 loss: 166.2412590533495 我们可以看到正确预测的数量增加，loss减少。 完整的训练loop我们就可以将网络、优化器和train_loader从训练循环单元格中提取出来。 1234567891011121314151617181920212223for epoch in range(10): total_loss = 0 total_correct = 0 for batch in train_loader: # Get Batch images, labels = batch preds = network(images) # Pass Batch loss = F.cross_entropy(preds, labels) # Calculate Loss optimizer.zero_grad() loss.backward() # Calculate Gradients optimizer.step() # Update Weights total_loss += loss.item() total_correct += get_num_correct(preds, labels) print( &quot;epoch&quot;, epoch, &quot;total_correct:&quot;, total_correct, &quot;loss:&quot;, total_loss ) 我们现在应该对训练循环以及我们如何使用 PyTorch 构建它们有了很好的理解。PyTorch 很酷的一点是，我们可以像使用forward()函数一样调试训练循环代码 。 在下一篇文章中，我们将看到如何获得训练集中每个样本的预测，并使用这些预测来创建混淆矩阵。下一篇见！","link":"/2021/11/08/Pytorch_DeepLizard%E6%95%99%E7%A8%8B25-26%EF%BC%9A%E8%AE%AD%E7%BB%83CNN/"},{"title":"Pytorch_DeepLizard教程20-22：构建CNN（下）Debug与批处理","text":"此笔记引用自文章知乎链接并修改 学习笔记这一篇笔记是构建 CNN 这个 section 的最后两集。再次感叹一下 deeplizard 这个教程真的是保姆级的，太良心了！（确实确实！！！） 首先说一下我把23、24两集的笔记前后顺序调整了一下，我觉得这样更合适： 第 24 集讲的是利用 VS Code（软件安装与配置在前面的笔记中有）的 debug 功能，通过监视神经网络中前向传播的 tensor，来进一步解释各个网络层的作用以及输出 tensor shape 的变化。但是举例用的还是单张图片。 第 23 集是在第 22 集的基础上，讲了如何一次性向网络中输入一批图片（batch）进行前向传播和预测（其实就多一步 Load 就好了）。 - Episode 24 of 33这一集建议直接去看视频，跟着他的操作来就行了： CNN Output Size Formula - Bonus Neural Network Debugging Sessionwww.youtube.com/watch?v=cin4YcGBh3Q&amp;list=PLZbbT5o_s2xrfNyHZsM6ufI0iZENK9xgG&amp;index=24 （1）代码笔记还是要写的，还是先把代码都贴进来： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# 导入要用的包import torchimport torch.nn as nnimport torch.nn.functional as Fimport torchvisionimport torchvision.transforms as transformstorch.set_printoptions(linewidth=120)# 构建CNN的类class Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, t): # (1) input layer t = t # (2) hidden conv layer t = self.conv1(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2) # (3) hidden conv layer t = self.conv2(t) t = F.relu(t) t = F.max_pool2d(t, kernel_size=2, stride=2) # (4) hidden linear layer t = t.reshape(-1, 12 * 4 * 4) # 这里的 -1 作用是自动适配 batch size t = self.fc1(t) t = F.relu(t) # (5) hidden linear layer t = self.fc2(t) t = F.relu(t) # (6) output layer t = self.out(t) # t = F.softmax(t, dim=1) return t# 网络实例化network = Network()# 导入数据集并转换为 tensortrain_set = torchvision.datasets.FashionMNIST( root='./data' ,train=True ,download=True ,transform=transforms.Compose([ transforms.ToTensor() ]))# 提取单张图片sample = next(iter(train_set)) image, label = sample # 输入网络并获得结果（记得unsqueeze）output = network(image.unsqueeze(0)) # 在这一行设置断点，进入网络调试print(output) （2）调试及监视 tensor 变化先解释一下 VS Code 里面调试模式下 F5、F10、F11 几个按键的作用吧： F5 应该就是从程序开始一直执行到有断点这一行暂停；F10 是执行一行并跳过；F11是执行一行并打开这一行中的调用。 所以我们看到视频里是先按了一次 F5 执行到断点的这一行。然后在这一行按了一次 F11，因为这一行采用了 call 的方法，所以进入到了 nn.Module 基类中的 call() 函数内。在 call() 函数内走到了调用 forward() 函数那一行，再按了一次 F11。因为扩展自 nn.Module 类的 Network 类中重写了 forward() 函数，所以这次 F11 走到了 Network() 中我们自己写的 forward() 函数里（就是上面代码中）。 进入 forward() 函数后，我们按 F10 一行一行执行就可以了，因为只关注结果，不需要再用 F11 去跟踪到底调用了什么函数。但是在开始 F10 之前，按视频里加了 t.shape 和 t.min() 两个变量的监控，然后就可以监测在神经网络中前向传播的 tensor 的 shape 和元素最小值的变化了。 这个表里总结了每经过一层或者一次操作之后 t.shape 的变化： 因为比较简单，上一篇笔记里面也解释过了，如果还不太懂的话查一下卷积层和池化层是怎么操作的就知道了，这里就不解释了。 之所以监视 t.min() 的变化，是为了看 ReLU 的作用，因为 ReLU 这个激活函数的作用就是把小于0的值改写为0，所以每过一次 ReLU，负的最小值就变成0了。 （3）CNN Output Size Formula最后还给了一个公式可以用来方便的计算经过卷积和池化操作之后的 tensor shape 变化： 各参数含义如下： O —— 输出的单张图片（二阶张量）的单边像素数； n —— 输入的单张图片（二阶张量）的单边像素数； f ——filter（卷积核/池化核）的单边像素数； p —— 补零（zero padding）单边像素数； s —— stride 的像素数。 上面的公式是对正方形的图片和 filter 来说的，如果不是正方形的，各个参数改成各边对应的就可以了。 - Episode 23 of 33这一集把上面的一次输入单张图片改成了一次输入一个 batch 而已。 （1）批量处理为了方便把代码再贴一遍吧： 12345678910111213141516171819202122232425262728293031323334353637import torchimport torch.nn as nnimport torch.nn.functional as Fimport torchvisionimport torchvision.transforms as transformstorch.set_printoptions(linewidth=120)class Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, t): t = F.relu(self.conv1(t)) t = F.max_pool2d(t, kernel_size=2, stride=2) t = F.relu(self.conv2(t)) t = F.max_pool2d(t, kernel_size=2, stride=2) t = t.reshape(-1, 12*4*4) # 这里的 -1 作用是自动适配 batch size t = F.relu(self.fc1(t)) t = F.relu(self.fc2(t)) t = self.out(t) return tnetwork = Network()train_set = torchvision.datasets.FashionMNIST( root='./data' ,train=True ,download=True ,transform=transforms.Compose([ transforms.ToTensor() ])) 接下来只要多一步把 train_set 按需要的 batch 传递给 DataLoader，就可以从 DataLoader 生成的数据 list 中提取出一个 batch 了： 123456789# 将 train_set 按 batch size 传递给 DataLoaderdata_loader = torch.utils.data.DataLoader( train_set, batch_size=10)# 从 data_loader 提取出一个 batchbatch = next(iter(data_loader))images, labels = batch 这里要说明的是，因为这次直接提取了一个 batch 出来，本身就已经是四阶张量，不需要再 unsqueeze 了： 12345&gt; images.shapetorch.Size([10, 1, 28, 28])&gt; labels.shapetorch.Size([10]) 输入网络并查看预测结果： 1234567891011121314151617181920&gt; preds = network(images)&gt; preds.shapetorch.Size([10, 10])&gt; predstensor( [ [ 0.1072, -0.1255, -0.0782, -0.1073, 0.1048, 0.1142, -0.0804, -0.0087, 0.0082, 0.0180], [ 0.1070, -0.1233, -0.0798, -0.1060, 0.1065, 0.1163, -0.0689, -0.0142, 0.0085, 0.0134], [ 0.0985, -0.1287, -0.0979, -0.1001, 0.1092, 0.1129, -0.0605, -0.0248, 0.0290, 0.0066], [ 0.0989, -0.1295, -0.0944, -0.1054, 0.1071, 0.1146, -0.0596, -0.0249, 0.0273, 0.0059], [ 0.1004, -0.1273, -0.0843, -0.1127, 0.1072, 0.1183, -0.0670, -0.0162, 0.0129, 0.0101], [ 0.1036, -0.1245, -0.0842, -0.1047, 0.1097, 0.1176, -0.0682, -0.0126, 0.0128, 0.0147], [ 0.1093, -0.1292, -0.0961, -0.1006, 0.1106, 0.1096, -0.0633, -0.0163, 0.0215, 0.0046], [ 0.1026, -0.1204, -0.0799, -0.1060, 0.1077, 0.1207, -0.0741, -0.0124, 0.0098, 0.0202], [ 0.0991, -0.1275, -0.0911, -0.0980, 0.1109, 0.1134, -0.0625, -0.0391, 0.0318, 0.0104], [ 0.1007, -0.1212, -0.0918, -0.0962, 0.1168, 0.1105, -0.0719, -0.0265, 0.0207, 0.0157] ]) 注意输出是一个 10 x 10 的二阶张量，两个 axis 的含义分别为：[batch size, number of prediction classes]。也就是说每一行是一张图片的预测结果，10行对应10张不同的图片。 （2）结果分析首先我们要做的是利用 argmax() 函数找到每一张图片的预测类别（即寻找每一行中最大值元素对应的索引序号）： 1234567# 预测结果&gt; preds.argmax(dim=1)tensor([5, 5, 5, 5, 5, 5, 4, 5, 5, 4])# 实绩标签&gt; labelstensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5]) 这里 argmax() 的参数 dim=1 怎么理解呢？因为二阶张量的两个索引参数，第一个参数是按行索引，先找到某一行；第二个索引参数是在找到的一行中在按列索引。所以我们指定 dim=1 跳过第一个索引参数（跳过行与行之间的比较），而是从第二个索引参数开始（在同一行的各列，即各个元素之间比较大小），这样输出的结果就是每一行中的最大值按列的索引号组成的一阶张量。 然后可以把据预测结果和标签对比看哪些预测对了： 12&gt; preds.argmax(dim=1).eq(labels)tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0], dtype=torch.uint8) 这里可以用之前 tensor 操作里面讲过的 sum() 函数求和，结果是一个表示预测正确数量的零阶张量： 12&gt; preds.argmax(dim=1).eq(labels).sum()tensor(1) 还可以再用前面讲过的 item() 函数把数值从零阶张量里面取出来。为了方便，把这一串操作定义成一个函数： 12345&gt; def get_num_correct(preds, labels): return preds.argmax(dim=1).eq(labels).sum().item()&gt; get_num_correct(preds, labels)1 到此呢 CNN 网络构建和前向传播的部分就都结束了。下一篇笔记将会开始进入神经网络的训练部分。","link":"/2021/11/04/Pytorch_DeepLizard%E6%95%99%E7%A8%8B23-24%EF%BC%9A%E6%9E%84%E5%BB%BACNN%EF%BC%88%E4%B8%8B%EF%BC%89Debug%E4%B8%8E%E6%89%B9%E5%A4%84%E7%90%86/"},{"title":"Pytorch_DeepLizard教程8-9：在PyTorch钟创建tensor","text":"此笔记引用自文章知乎链接并修改 学习笔记教程8-9集讲了在PyTorch中创建一个torch.Tensor的PyTorch类的几种方法，以及他们的区别。 （1）PyTorch中的tensor attributes上一篇讲了tensor的三个通用的属性—— rank、axes、shape，接下来的这几个是PyTorch中torch.Tensor类（class）的属性： torch.dtype torch.device torch.layout 例如对于我们创建的一个空的tensor t： 1234&gt; import torch&gt; t = torch.Tensor()&gt; type(t)torch.Tensor 查看其默认的三个属性： 123456&gt; print(t.dtype)&gt; print(t.device)&gt; print(t.layout)torch.float32cputorch.strided **torch.dtype** 查看的是 tensor 中数据的 data type（数据类型），这里默认是32位浮点数。 **torch.device**是 tensor 分配给的硬件，决定了在哪里进行 tensor 的计算。 可以采用如下的方式来将 device 指定为GPU： 123&gt; device = torch.device('cuda:0')&gt; devicedevice(type='cuda', index=0) 如前所述，PyTorch支持使用多张显卡计算，因此对于多张显卡的情况可以通过不同的 index 序号来指定显卡（想peach）。 因此综合上述两个属性，PyTorch中的 tensor 会有下面8种数据类型，每种数据类型根据 device 不同又有 CPU 和 GPU 之分： Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point torch.float16 torch.HalfTensor torch.cuda.HalfTensor 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 torch.LongTensor torch.cuda.LongTensor 这里需要注意的是，tensor 之间的的运算和操作必须是在具有相同 dtype 和相同 device 属性的 tonser 之前才能进行。 **torch.layout** 是指 tensor 在内存中的储存方式，默认为strided ，教程里没解释说暂时不用了解。不过这里有维基百科的链接： Stride of an array - wikipediaen.wikipedia.org/wiki/Stride_of_an_array （2）利用已有 data 创建 PyTorch tensor 的方法利用已有 data 创建torch.Tensor的4种主要方法： torch.Tensor(data) torch.tensor(data) torch.as_tensor(data) torch.from_numpy(data) 创建 tensor 的实例如下，首先用 numpy 创建一个 array（1阶张量）： 12345&gt; import torch&gt; import numpy as np&gt; data = np.array([1,2,3])&gt; type(data)numpy.ndarray 然后分别用4种方法转化为torch.Tensor： 12345678910111213&gt; o1 = torch.Tensor(data)&gt; o2 = torch.tensor(data)&gt; o3 = torch.as_tensor(data)&gt; o4 = torch.from_numpy(data)&gt; print(o1)&gt; print(o2)&gt; print(o3)&gt; print(o4)tensor([1., 2., 3.])tensor([1, 2, 3], dtype=torch.int32)tensor([1, 2, 3], dtype=torch.int32)tensor([1, 2, 3], dtype=torch.int32) 可以看到第一种和其他三种的结果不一样，区别在于，torch.Tensor(data)是torch.Tensor这个类的 class constructor，使用默认的参数创建一个 tensor。如前所述torch.Tensor类的默认数据类型是 float32（32位浮点数），所以输出的是 [1., 2., 3.]，小数点就代表浮点数。 查看默认数据类型： 12&gt; torch.get_default_dtype()torch.float32 验证 torch.Tensor 创建的数据类型： 12&gt; o1.dtype == torch.get_default_dtype()True 后面三种全部都是 factory function，他们是根据输入的 data 创建一个torch.Tensor返回给 caller，并且直接继承 data 的属性（type inference），也可以用给定参数指定属性，所以看到后面三个的输出数据类型都是 int32（32位整形）。 指定创建的 tensor 的数据类型为 float32： 12&gt; torch.tensor(data, dtype=torch.float32)&gt; torch.as_tensor(data, dtype=torch.float32) 其中，第二种torch.tensor() 把大写的“T”换成了小写作为区别。在PyTorch中创建一个tensor类通常采用torch.tensor()函数，因为它相比第一种有更好的说明文档和更多的可选配置参数。 三四种和前两种的区别在于，前两种是把原来的 numpy.ndarray 类型的 data 在内存中复制一份，存在不同的地址，这样修改原来的 data 不会影响新创建的 tensor；而后两种新创建的 tensor 是和原来的 data 共享内存的，仅仅是创建了一个新的内存地址指针，省略了拷贝数据的过程（对于大型数据会有效率优势和内存占用）。 示例： 1234567891011121314151617&gt; print('old:', data)old: [1 2 3]&gt; data[0] = 0&gt; print('new:', data)new: [0 2 3]&gt; print(o1)&gt; print(o2)&gt; print(o3)&gt; print(o4)tensor([1., 2., 3.])tensor([1, 2, 3], dtype=torch.int32)tensor([0, 2, 3], dtype=torch.int32)tensor([0, 2, 3], dtype=torch.int32) 上面这个示例把原来 data 的第一个元素从1修改为了0，然后发现由前两种方法创建的 tensor 没有变化（因为复制到新的内存地址了）；而后两种方法创建的 tensor 元素都发生了变化，说明确实是和原来的 data 共用的内存地址。 这就是数据存储方式的区别： 那么后两种又有什么区别呢，其实从名称能看出： torch.from_numpy()这个函数只接受numpy.ndarray类型数据的输入，但是torch.as_tensor()函数接受各种各样的类似数组的对象的输入，包括其他PyTorch张量。因此采用torch.as_tensor()就可以了。 Attention这里要说明的是，在写程序的时候建议还是优先采用torch.tensor()这个函数创建 tensor，保证程序没有bug；后续可以从性能优化的角度再修改 tensor 的创建方式，采用torch.as_tensor()。 最后附上文档给出的关于torch.as_tensor()的tips，我就不翻译了： Since numpy.ndarray objects are allocated on the CPU, the as_tensor() function must copy the data from the CPU to the GPU when a GPU is being used. The memory sharing of as_tensor() doesn’t work with built-in Python data structures like lists. The as_tensor() call requires developer knowledge of the sharing feature. This is necessary so we don’t inadvertently make an unwanted change in the underlying data without realizing the change impacts multiple objects. The as_tensor() performance improvement will be greater if there are a lot of back and forth operations between numpy.ndarray objects and tensor objects. However, if there is just a single load operation, there shouldn’t be much impact from a performance perspective. （3）利用预设创建PyTorch tensor这部分就很简单了。 **torch.eye()**函数可以创建单位二阶张量（单位矩阵），括号内指定axis的length。例如创建 2x2 单位阵： 12345&gt; print(torch.eye(2))tensor([ [1., 0.], [0., 1.]]) **torch.zeros()**函数创建全零张量，例如创建 2x2 全零二阶张量： 12345&gt; print(torch.zeros([2,2]))tensor([ [0., 0.], [0., 0.]]) **torch.ones()**函数创建全1张量， 例如创建 2x2 全1张量： 12345&gt; print(torch.ones([2,2]))tensor([ [1., 1.], [1., 1.]]) torch.rand()函数创建随机数值张量，例如创建 2x2 随机数据张量： 12345&gt; print(torch.rand([2,2]))tensor([ [0.0465, 0.4557], [0.6596, 0.0941]])","link":"/2021/11/01/Pytorch_DeepLizard%E6%95%99%E7%A8%8B8-9%EF%BC%9A%E5%9C%A8PyTorch%E4%B8%AD%E5%88%9B%E5%BB%BAtensor/"},{"title":"","text":"URLNet 定义成二元分类问题，判断输入URL 是否为恶意的URL，后面的工作就跟常规的深度学习任务近似，训练模型，降低loss等 2.2 Lexical Features 传统方法提取特征有三个主要的限制 缺乏单词在URL中出现的顺序的信息，有一些策略通过对URL的每一段创建单独的字典来利用顺序信息，但是这样虽然能够区分出现在顶级域中的“com”和出现在URL路径中的“com”，但是也没有考虑到单词或者字符在URL中的特定片段中出现的顺序。 无法从稀有词中获取信息。 无法解释测试URL中出现的新词。","link":"/2023/02/19/URLNet/"},{"title":"Pytorch_DeepLizard教程5-7：tensor基本概念","text":"此笔记引用自文章知乎链接并修改 学习笔记第5-7集属于Part 1. Section 2，介绍深度学习中张量（tensor）的基本概念。 - Episode 5 of 33什么是张量？神经网络中的输入、输出和转换都使用张量表示，因此，神经网络编程大量使用张量。 张量是神经网络使用的主要 数据结构。 张量的概念是其他更具体概念的数学概括。让我们看一下张量的一些特定实例。 number scalar array vector 2d-array matrix 让我们将上面的示例张量列表组织成两组： number, array, 2d-array scalar, vector, matrix 第一组三个术语（数字、数组、二维数组）是计算机科学中通常使用的术语，而第二组（标量、向量、矩阵）是数学中通常使用的术语。 我们经常看到这样的事情，不同的研究领域对同一个概念使用不同的词。在深度学习中，我们通常将所有这些都称为张量。 让我们进一步研究这些术语。当我们从左向右移动时，每组中的术语彼此对应。为了显示这种对应关系，我们可以重塑我们的术语列表，以获得三组每组两个术语： number, scalar array, vector 2d-array, matrix 访问元素所需的索引 Indexes required Computer science Mathematics 0 number scalar 1 array vector 2 2d-array matrix 在三维和以上之后，就不再专门取名字了，在数学中统一用 tensor（张量）这个术语来称呼，对应计算机的术语就是 nd-array（n维数组）。实际上标量、向量、矩阵都是张量的特殊形式，分别是0维张量、1维张量、2维张量。 计算机科学在计算机科学中，我们不再使用诸如数字、数组、二维数组之类的词，而开始使用多维数组或 nd-array一词 。这n 告诉我们访问结构中特定元素所需的索引数量。 需要索引 计算机科学 数学 n 数组 nd-张量 让我们说清楚。出于神经网络编程的实际目的，张量和 nd 数组是一回事。 张量和 nd-array是一回事！ 所以张量是多维数组或简称 nd 数组。我们说张量是泛化的原因是因为我们对所有的值都使用了张量这个词n 像这样： 标量是一个 0 维张量 向量是一个 1 维张量 矩阵是一个 2 维张量 一个 nd 数组是一个 n 维张量 张量允许我们删除这些特定的术语而只使用一个 n 确定我们正在使用的维度数量。 - Episode 6 of 33这一集讲张量的三个属性 Rank Axes Shape （1）Rank在前面说了，指张量的维度数量（阶数），虽然是同一个单词但是和矩阵的秩不一样，张量的rank就是指它的维度数量。 张量的rank告诉我们需要多少个索引来引用张量中的特定元素。 （2）Axis指某一维度的长度，也就是这一维度中的元素数量（axes是复数形式）。 比如一个3x4的矩阵，rank为2，第一个维度的axis为3，第二个维度的axis为4。 （3）Shape就是字面意思，各个维度都有多少个元素，刚刚说的3*4就是这个矩阵（二阶张量）的shape。 Python和PyTorch中的操作： 在Python中定义一个矩阵/二维数组/二阶张量dd： 12345&gt; dd = [[1,2,3],[4,5,6],[7,8,9]] 如果只用一个索引序号，会索引出沿第一个axis的元素，每个元素都是一个array（按行索引）： 12345678&gt; dd[0][1, 2, 3]&gt; dd[1][4, 5, 6]&gt; dd[2][7, 8, 9] 如果用两个索引序号，就会索引出某行某列的一个元素，是一个number： 12345678&gt; dd[0][0]1&gt; dd[1][0]4&gt; dd[0][1]2 将前面创建的矩阵转换为torch.Tensor对象，以便后续的操作： 12&gt; t = torch.tensor(dd)&gt; t 可以看到输出为： 12345tensor([[1, 2, 3],[4, 5, 6],[7, 8, 9]]) 查看t的数据类型： 1&gt; type(t) 输出为： 1torch.Tensor 查看t的shape： 1&gt; t.shape 输出为：（PyTorch中shape和size是相同的概念） 1torch.Size([3,3]) 可以通过“t.shape”这个变量的长度得知张量的阶数： 1&gt; len(t.shape) 输出为： 12 然后视频讲了一下张量的reshape，比如把3行3列（3x3）的矩阵reshape成1行9列（1x9），要注意reshape前后元素总量不变： 1&gt; t.reshape(1,9) 输出会变成： 1tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]]) - Eposide 7 of 33这一集讲了一个实例，将图像作为一个tensor输入到CNN中。 （1）CNN的输入tensor一般CNN的输入是一个4阶张量，分别用一个字母代表每个axis上的长度，那么它的shape为： 1[B,C,H,W] [Batch, Channels, Height, Width] 其中 H 和 W 分别代表输入的单张图片的 height（高）和 width（宽）的像素数，比如常见的224x224。 C 代表 color，是指图片的颜色通道数量。对于彩色图片，通常是 C = 3（分别是RGB通道）；如果是灰度图片，那么 C = 1。但 C 只在输入层中是这个含义，图片进入CNN传播之后这个axis的意义会变化。 于是可以通过 C、H、W 这三个axis来索引单张图片某一个像素点的像素值，通过 C 索引颜色通道，通过 H 和 W 索引这个颜色通道下第几行第几列的亮度数值。 B 代表 batch size，因为一次向神经网络中会不只输入一张图片，而是同时输入一批图片进行并行计算，因此 B 的数值就代表一次输入了几张图片。 （2）color channel的变化刚才说color channel在图片在CNN的传播中会发生变化，是因为CNN中使用不同的filter对图像进行卷积操作，每个filter进行卷积之后都会输出一个output channel，称为feature map。于是原来的每一个color channel就被替换成了多个feature map。 假设CNN中使用了 F 个filter，那么经过一次卷积之后tensor的shape会变成： 1[B, F*C, H, W] 当然根据卷积操作的不同，输出tensor中的 H 和 W 长度也可能会发生变化。","link":"/2021/10/31/Pytorch_DeepLizard%E6%95%99%E7%A8%8B5-7%EF%BC%9Atensor%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2023/02/19/hello-world/"},{"title":"hexo设置icarus主题","text":"Hexo配置p.s. 如果遇见头像设置完而没有成功显示，则注意gravatar属性后不要跟任何值，为空，avatar才有效！ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# Sitetitle: #主页标题subtitle: #副标题description: #网站描述description主要用于SEOkeywords: #博客关键字author: #作者，左下角显示language: zh_Hans # 选择中文简体timezone: 'Asia/Shanghai' #时区:国内选择上海# Urlurl: http://yoursite.com #填自己的github pages网址 root: / #网站根目录permalink: :year/:month/:day/:title/ #文章的 永久链接 格式permalink_defaults: #永久链接中各部分的默认值pretty_urls: #改写 permalink 的值来美化 URLtrailing_index: false # 比如，一个页面的永久链接是 http://example.com/foo/bar/index.html 是否在 永久链接中保留尾部的 index.html，设置为 false 时去除trailing_html: true #是否在永久链接中保留尾部.html, 设置为 false 时去除# Directorysource_dir: source #资源文件夹，这个文件夹用来存放内容。public_dir: public #公共文件夹，这个文件夹用于存放生成的站点文件。tag_dir: tags #标签文件夹archive_dir: archives #归档文件夹category_dir: categories #分类文件夹code_dir: downloads/code #Include code 文件夹，source_dir 下的子目录i18n_dir: :lang #国际化（i18n）文件夹skip_render: #跳过指定文件的渲染。匹配到的文件将会被不做改动地复制到 public 目录中。您可 使用 glob 表达式来匹配路径。# Writingnew_post_name: :year-:month-:day-:title.md #生成yyyy-MM-dd-博文名称的名称有助于我们管理自己的博 文。 default_layout: post #预设布局titlecase: false #把标题转换为 title caseexternal_link: #在新标签中打开链接 enable: true #在新标签中打开链接 field: site #对整个网站（site）生效或仅对文章（post）生效 exclude: '' #需要排除的域名。主域名和子域名如 www 需分别配置filename_case: 0 #把文件名称转换为 (1) 小写或 (2) 大写render_drafts: false #显示草稿post_asset_folder: false #启动 Asset 文件夹 new 文件的同时，xxxx.md文件还有一个同名的文件夹relative_link: false #把链接改为与根目录的相对位址future: true #显示未来的文章highlight: enable: true #开启代码块高亮 line_number: true #显示行数 auto_detect: false #如果未指定语言，则启用自动检测 tab_replace: '' #用 n 个空格替换 tabs；如果值为空，则不会替换 tabs wrap: true # 将代码块包装到&lt;table&gt; hljs: false # CSS类使用hljs-*前缀# Home page setting# path: Root path for your blogs index page. (default = '')# per_page: Posts displayed per page. (0 = disable pagination)# order_by: Posts order. (Order by date descending by default)index_generator: path: '' per_page: 10 order_by: -date# Category &amp; Tagdefault_category: uncategorized #默认分类category_map: #分类别名tag_map: #标签别名# Metadata elementsmeta_generator: true # Meta generator 标签。 值为 false 时 Hexo 不会在头部插入该标签# Date / Time format## Hexo uses Moment.js to parse and display date Hexo 使用 Moment.js 来解析和显示时间## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-MM-DD #日期格式time_format: HH:mm:ss #时间格式use_date_for_updated: false #启用以后，如果Front Matter中没有指定 updated， post.updated 将会使用date的值而不是文件的创建时间。在Git工作流中这个选项会很有用# Pagination## Set per_page to 0 to disable paginationper_page: 10 #每页显示的文章量 (0 = 关闭分页功能)pagination_dir: page #分页目录# Include / Exclude file(s)## include:/exclude: options only apply to the 'source/' folderinclude: #Hexo 默认会忽略隐藏文件和文件夹（包括名称以下划线和 . 开头的文件和文件夹，Hexo 的 _posts 和 _data 等目录除外）。通过设置此字段将使 Hexo 处理他们并将它们复制到 source 目录下。exclude: #Hexo 会忽略这些文件和目录ignore: #Ignore files/folders# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: icarus #当前主题名称。值为false时禁用主题# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: #部署部分的设置 type: git repo: https://github.com/CodePandaes/CodePandaes.github.io.git #github中仓库地址 branch: master 替换icarus主题icarus这个主题比较小众，个人也很喜欢yilia这个主题，或者选择官方主题，打开git bash下载主题icarus 12$ cd /d/WorkPlace/myHexo$ git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarus 下载好的主题在themes文件夹下 修改配置文件完成后，git bash中执行命令hexo clean(清除缓存文件)，然后执行hexo g 和hexo s 重新发布 1theme: icarus #在Hexo配置文件中把landscape切换为icarus icarus配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159version: 2.6.0 #版本favicon: /images/panda.ico #浏览器图标meta: # Additional HTML meta tags in an array.canonical_url: # canonical_url of your siterss: # Path or URL to RSS atom.xmllogo: /images/logo.ico #logo# Open Graph metadata# https://hexo.io/docs/helpers.html#open-graphopen_graph: # Facebook App ID fb_app_id: # Facebook Admin ID fb_admins: # Twitter ID twitter_id: # Twitter site twitter_site: # Google+ profile link google_plus: navbar: # 导航栏 # 菜单（显示名称：对应文件夹） menu: 主页: / 归档: /archives 分类: /categories 标签: /tags 关于我: /about # github账号 links: Download on GitHub: icon: fab fa-github url: 'https://github.com/ppoffice/hexo-theme-icarus'# 页脚图标链接footer: # Links to be shown on the right of the footer section links: Creative Commons: icon: fab fa-creative-commons url: 'https://creativecommons.org/' Attribution 4.0 International: icon: fab fa-creative-commons-by url: 'https://creativecommons.org/licenses/by/4.0/' Download on GitHub: icon: fab fa-github url: 'https://github.com/ppoffice/hexo-theme-icarus'# 文章显示设置article: # Code highlight settings highlight: theme: atom-one-light # 代码主题atom-one-light亮色，atom-one-dark暗色 clipboard: true # 显示代码copy按钮 # Default folding status of the code blocks. Can be &quot;&quot;, &quot;folded&quot;, &quot;unfolded&quot; fold: unfolded thumbnail: true # 是否显示文章主图 readtime: true # 是否显示估算阅读时间# Search plugin settings 搜索插件设置search: type: insight # 插件名# 评论插件设置comment: #可选valine，disqus（科学上网）等 # Name of the comment plugin #type: valine #app_id: 不为空 #app_key: 不为空 #notify: true #verify: true #placeholder: # Name of the comment plugin type: # Donation entries 打赏功能donate: - type: alipay # 支付宝 qrcode: '/' # 支付宝图片URL - type: alipay # 微信 qrcode: '/' # 微信图片URL -# 分享插件设置share: # Share plugin name 插件类型，有多种，可选，自行百度 type: # Sidebar settings.sidebar: # left sidebar settings 左侧边栏设置 left: # 是否不随页面滚动 sticky: true # right sidebar settings 右侧边栏设置 right: sticky: false# Sidebar widget settings 边栏小部件设置widgets: - type: profile # 个人信息 position: left # 部件位置（左） author: 程序熊猫 # 作者名（字符串） author_title: Developer #作者身份描述（字符串） location: 湖北·武汉 #作者当前居住地 avatar: '/images/touXiang.jpeg' #头像（可用本地图片或网络图片链接） gravatar: # 要在配置文件小部件中显示的Gravatar的电子邮件地址 avatar_rounded: false #显示圆形或方形的化身图像 follow_link: 'https://github.com/CodePandaes' #关注我的链接，可设为你的GitHub主页 #个人介绍部件底部图标社交链接 social_links: Github: icon: fab fa-github url: 'https://github.com/CodePandaes' Facebook: icon: fab fa-facebook url: 'https://facebook.com' # Twitter: # icon: fab fa-twitter # url: 'https://twitter.com' # Dribbble: # icon: fab fa-dribbble # url: 'https://dribbble.com' # RSS: # icon: fas fa-rss # url: / - # Widget name 组件toc type: toc # Where should the widget be placed, left or right position: left - # Widget name 标签 type: tag # Where should the widget be placed, left or right position: right - # Widget name 分类 type: category # Where should the widget be placed, left or right position: right - # Widget name 标签云 type: tagcloud # Where should the widget be placed, left or right position: right - # Widget name 近期文章 type: recent_posts # Where should the widget be placed, left or right position: left - # Widget name 归档 type: archive # Where should the widget be placed, left or right position: right - # Widget name 外部链接 type: links # Where should the widget be placed, left or right position: right # Links to be shown in the links widget links: polaris_973的博客: 'http://polaris973.xyz'","link":"/2021/10/04/hexo%E8%AE%BE%E7%BD%AEicarus%E4%B8%BB%E9%A2%98/"},{"title":"k近邻法","text":"$k$近邻法 $k$近邻法属于分类方法 不具有显式的学习方法，而通常是通过“多数表决”等方式（voting）进行预测 $k$值的选取、距离度量的选取、分类决策规则是方法的三个基本要素 $k$近邻算法算法的基本流程很容易理解，具体展开如下图所示，但是需要注意的是对于不同的数据集合适的距离度量很有可能是不相同的。 $k$值的选取 $k$值如果选择较小，那么“学习”的近似误差（approximation error）减小、估计误差（estimation error）增大，即对噪声敏感，换句话说容易发生过拟合 而如果$k$值选择较大，那么估计误差减小，但近似误差增大，从而预测错误率升高 总而言之，一般来讲，$k$值的选取是一个调参的过程 分类决策规则 $k$近邻法的实现：$kd$树构造$kd$树$kd$树的作用是减少模型学习过程中距离计算和$k$近邻搜索的计算开销 搜索$kd$树","link":"/2023/02/21/k%E8%BF%91%E9%82%BB%E6%B3%95/"},{"title":"opencv基础用法(二)：颜色检测","text":"Colour detection 颜色检测[TOC] 接下来了解一种颜色检测的方法。 目标此次目的是将图片中的橙色捕捉（检测）出来。 1. 了解HSV图像颜色模型：HSV(Hue, Saturation, Value)是根据颜色的直观特性创建的一种颜色空间, 也称六角锥体模型(Hexcone Model)。 这个模型中颜色的参数分别是：色调（Hue），取值为[0-180]；饱和度（Saturation），取值为[0-255]；明度（Value），取值为[0-255]。 读取lambo图片，并转化为HSV图像格式： 12345678import cv2import numpy as npimg = cv2.imread(&quot;Resources/lambo.PNG&quot;)imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # 将RGB颜色模型的图像转换为HSVcv2.imshow(&quot;img&quot;, img)cv2.imshow(&quot;HSV_img&quot;, imgHSV)cv2.waitKey(0) result: 2. 滑动条的使用cv2.createTrackbar() 这个函数用于创建一个滑动条，其中有五个参数： 第一个参数：滑动条的名字 第二个参数：滑动条被放置的窗口名字 第三个参数：滑动条的默认位置 第四个参数：滑动条的最大值 第五个参数：回调函数，每次滑动条的滑动都会调用该函数，该函数通常会有一个参数，这个参数就是滑动条的位置。 cv2.getTrackbarPos() 这个函数用于获得滑动条的值，其中有两个参数： 第一个参数：所要回去值的那个滑动条的名字 第二个参数：滑动条被放置的窗口名字 创建滑动条示例代码 123456789import cv2cv2.namedWindow(&quot;TrackBars&quot;) # 创建一个名为TrackBars的窗口cv2.resizeWindow(&quot;TrackBars&quot;, 640, 240) # 窗口大小为640x240cv2.createTrackbar(&quot;Hue Min&quot;, &quot;TrackBars&quot;, 0, 179, empty) # Hue Min：色调最小值cv2.createTrackbar(&quot;Hue Max&quot;, &quot;TrackBars&quot;, 19, 179, empty) # Hue Max：色调最大值cv2.createTrackbar(&quot;Sat Min&quot;, &quot;TrackBars&quot;, 110, 255, empty) # Sat Min：饱和度（Saturation）最小值cv2.createTrackbar(&quot;Sat Max&quot;, &quot;TrackBars&quot;, 240, 255, empty) # Sat Max：饱和度（Saturation）最搭值cv2.createTrackbar(&quot;Val Min&quot;, &quot;TrackBars&quot;, 153, 255, empty) # Val Min：明度（Value）最小值cv2.createTrackbar(&quot;Val Max&quot;, &quot;TrackBars&quot;, 255, 255, empty) # Val Max：明度（Value）最大值 result： 获取滑动条位置数值示例代码 12345678910# 获取上文创建的窗口滑动条数值并输出while True: h_min = cv2.getTrackbarPos(&quot;Hue Min&quot;, &quot;TrackBars&quot;) h_max = cv2.getTrackbarPos(&quot;Hue Max&quot;, &quot;TrackBars&quot;) s_min = cv2.getTrackbarPos(&quot;Sat Min&quot;, &quot;TrackBars&quot;) s_max = cv2.getTrackbarPos(&quot;Sat Max&quot;, &quot;TrackBars&quot;) v_min = cv2.getTrackbarPos(&quot;Val Min&quot;, &quot;TrackBars&quot;) v_max = cv2.getTrackbarPos(&quot;Val Max&quot;, &quot;TrackBars&quot;) print(h_min, h_max, s_min, s_max, v_min, v_max) cv2.waitKey(1) 定义掩膜（mask）那么掩膜是什么呢？可访问所给参考链接进行学习。 图像中的掩膜(Mask)是什么 使用cv2.inRange()函数 mask = cv2.inRange(hsv, lower,upper)函数很简单，参数有三个第一个参数：hsv指的是原图 第二个参数：lower指的是图像中低于这个lower的值，图像值变为0 第三个参数：upper指的是图像中高于这个upper的值，图像值变为0 而在lower～upper之间的值变成255 示例代码 12345678910111213while True: img = cv2.imread(path) imgHSV = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) h_min = cv2.getTrackbarPos(&quot;Hue Min&quot;, &quot;TrackBars&quot;) h_max = cv2.getTrackbarPos(&quot;Hue Max&quot;, &quot;TrackBars&quot;) s_min = cv2.getTrackbarPos(&quot;Sat Min&quot;, &quot;TrackBars&quot;) s_max = cv2.getTrackbarPos(&quot;Sat Max&quot;, &quot;TrackBars&quot;) v_min = cv2.getTrackbarPos(&quot;Val Min&quot;, &quot;TrackBars&quot;) v_max = cv2.getTrackbarPos(&quot;Val Max&quot;, &quot;TrackBars&quot;) print(h_min, h_max, s_min, s_max, v_min, v_max) lower = np.array([h_min, s_min, v_min]) upper = np.array([h_max, s_max, v_max]) mask = cv2.inRange(imgHSV, lower, upper) # 定义掩膜 result： 可以手动调整滑动条取得HSV的数值并实时更新Mask效果 到这里已经可以将橙色部分捕捉并采用cv2.inRange()函数将该区域转成白色，其它颜色为黑；用此方法定义一个掩膜 利用掩膜得到新图像（颜色检测关键步骤，实质是按位与运算）使用cv2.bitwise_and()函数 是对二进制数据进行“与”操作，即对图像（灰度图像或彩色图像均可）每个像素值进行二进制“与”操作 利用掩膜（mask）进行“与”操作，即掩膜图像白色区域是对需要处理图像像素的保留，黑色区域是对需要处理图像像素的剔除，其余按位操作原理类似只是效果不同而已。 完整代码 1234567891011121314151617181920212223242526272829303132333435363738import cv2import numpy as npdef empty(a): passpath = 'Resources/lambo.png'cv2.namedWindow(&quot;TrackBars&quot;)cv2.resizeWindow(&quot;TrackBars&quot;,640,240)cv2.createTrackbar(&quot;Hue Min&quot;,&quot;TrackBars&quot;,0,179,empty)cv2.createTrackbar(&quot;Hue Max&quot;,&quot;TrackBars&quot;,19,179,empty)cv2.createTrackbar(&quot;Sat Min&quot;,&quot;TrackBars&quot;,110,255,empty)cv2.createTrackbar(&quot;Sat Max&quot;,&quot;TrackBars&quot;,240,255,empty)cv2.createTrackbar(&quot;Val Min&quot;,&quot;TrackBars&quot;,153,255,empty)cv2.createTrackbar(&quot;Val Max&quot;,&quot;TrackBars&quot;,255,255,empty)while True: img = cv2.imread(path) imgHSV = cv2.cvtColor(img,cv2.COLOR_BGR2HSV) h_min = cv2.getTrackbarPos(&quot;Hue Min&quot;,&quot;TrackBars&quot;) h_max = cv2.getTrackbarPos(&quot;Hue Max&quot;, &quot;TrackBars&quot;) s_min = cv2.getTrackbarPos(&quot;Sat Min&quot;, &quot;TrackBars&quot;) s_max = cv2.getTrackbarPos(&quot;Sat Max&quot;, &quot;TrackBars&quot;) v_min = cv2.getTrackbarPos(&quot;Val Min&quot;, &quot;TrackBars&quot;) v_max = cv2.getTrackbarPos(&quot;Val Max&quot;, &quot;TrackBars&quot;) print(h_min,h_max,s_min,s_max,v_min,v_max) lower = np.array([h_min,s_min,v_min]) upper = np.array([h_max,s_max,v_max]) mask = cv2.inRange(imgHSV,lower,upper) imgResult = cv2.bitwise_and(img,img,mask=mask) # 图像二进制数值进行按位“与”运算进行颜色检测操作获取结果图片 cv2.imshow(&quot;Original&quot;,img) cv2.imshow(&quot;HSV&quot;,imgHSV) cv2.imshow(&quot;Mask&quot;, mask) cv2.imshow(&quot;Result&quot;, imgResult) cv2.waitKey(1) result：","link":"/2021/10/14/opencv%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E9%A2%9C%E8%89%B2%E6%A3%80%E6%B5%8B/"},{"title":"","text":"什么是差分隐私差分隐私顾名思义就是用来防范差分攻击的。 举个简单的例子，假设现在有一个婚恋数据库，2个单身8个已婚，只能查有多少人单身。刚开始的时候查询发现，2个人单身；现在张三跑去登记了自己婚姻状况，再一查，发现3个人单身。所以张三单身。 这里张三作为一个样本的的出现，使得攻击者获得了奇怪的知识。而差分隐私需要做到的就是使得攻击者的知识不会因为这些新样本的出现而发生变化。 那怎么做到呢？加入随机噪声。比如刚才的例子，本来两次查询结构是确定的2和3，现在加入随机噪声后，变成了两个随机变量，画出它们概率分布图。 现在，如果张三不在数据库的话，得到结果可能是2.5 张三在的话，得到的结果也可能是2.5 两个数据集查询得到某一个结果的概率很接近，以至于我们根本分不清这个结果来自于哪一个数据集，这样也就实现了攻击者的知识不会因为张三这个样本的出现与否而发生变化。 这些只是概念上的理解，总结一下就是对查询的结果加入噪声，使得攻击者无法辨别某一样本是否在数据集中。 公式化的理解刚才只是讲到概念上的理解，现在从数学上理解一下。上面的查询函数可以用 表示（这里暂时只考虑输出结果为1维的情况），随机噪声可以用 表示，最终得到的查询结果就是 ， 对于两个汉明距离为1的数据集 ，对于任意的输出集合，应该有： 首先还是得回到图1-2，我们的目的是使的这两个分布尽可能地接近，那么衡量两个分布 的差异自然可以用到我们熟悉的KL-Divergence | KL散度：","link":"/2023/02/19/%E4%BB%80%E4%B9%88%E6%98%AF%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/"},{"title":"v2rayN 配置使用教程及可能碰到的问题","text":"windows简单配置v2rayN及使用下载v2rayN下载分为两个部分，软件和内核，不过好在作者都打包在一起了，也不用麻烦去寻找！ 点开下载地址，选择版本。 下载地址：https://github.com/2dust/v2rayN/releases 带Core的压缩包是自带内核的，另一个则是没有内核的，新手操作推荐下载带内核的版本，即下载名字后带Core的zip包。 如电脑报毒，需要暂时关闭Windows的系统防护，软件是没有问题的。 软件安装配置首先将程序解压出来 在文件夹中找到v2rayN.exe打开就能启动程序了，也可以自己创建快捷方式 双击打开程序 右键任务栏 新版本将HTTP代理改成了系统代理，这三个选项可以简单理解为 清除系统代理——不开启代理 自动配置系统代理——开启代理 不改变系统设置——不改变现有代理状态 建议使用的时候，将启用路由高级功能勾上，系统自带全局和绕过中国大陆（推荐）两种规则，右键单击【设为活动路由】启用。想要自建规则可以点击高级功能设置。 添加服务器点击服务器就可以通过各种方式添加各种服务器了。 以添加VMESS服务器举例 分别将地址、端口、用户ID、额外ID填写完毕并确认保存即可。 保存完毕便会自动启动服务，右键任务栏的客户端选择【自动配置系统代理】即可。 自己所遇到的问题解答Q：为什么配置成功，节点经测试也没有问题（可以自己测试服务器延迟查看），手机可以成功上网，但是电脑却无法成功上网？ A：自己网上查找资料解答，很多是说自己电脑日期时间与节点（服务器）日期时间不匹配造成。这类问题的解决方法便是将自己电脑时间调整到节点当地标准时间。 但这并不能解决我的问题。 后面突然想到会不会是之前用的一些代理设置的插件有鬼，便打开chrome浏览器的插件管理查看一番。最终发现端倪： 没错，就是它Proxy SwitchyOmega，是之前需要使用到的一款代理设置管理插件。启用后默认是【直接连接】而不是【系统代理】， 所以浏览器无法访问到谷歌等外网。至此破案，所以在这里也提供一个思路，即是查看自己是否有类似的插件或程序，自己进行开关或设置。 Q：手机如何快速配置客户端 A：下载v2rayNG安卓（IOS没有客户端，需要其它的付费APP） 下载地址：https://github.com/2dust/v2rayNG/releases 友情提示：请选择合适的版本！或者直接下载第一个apk文件 软件页面 打开软件后点击右上角 + 号，得到下面界面， 推荐直接扫描二维码或剪贴板导入，那么二维码和剪贴板数据哪里来呢？ 回去打开电脑的v2RayN，点击界面的【分享】即可得到","link":"/2021/10/18/v2rayN-%E9%85%8D%E7%BD%AE%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E5%8F%8A%E5%8F%AF%E8%83%BD%E7%A2%B0%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"},{"title":"决策树","text":"决策树 可用于分类与回归，主要讨论分类决策树方法 可被认为是if-then规则的集合，也可以认为是定义在特征空间和类空间上的条件概率分布 通常包括：特征选择、决策树生成、决策树修剪 ID3算法和C4.5算法 决策树模型与学习决策树模型 决策树可被视为是if-then规则的集合，这其中有一个重要的性质：互斥并且完备。其意思是说：每个实例都会被决策树种一条路径或者规则集合中的一条规则所覆盖，且只被一条路径或一条规则所覆盖。 决策树还表示给定特征条件下类的条件概率分布。假设X为表示特征的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为P(Y|X)。 决策树学习 决策树学习本质上是从训练数据集中归纳出一组分类规则。注意的是：从训练数据集中学习出的（正确分类训练数据的）决策树可能有多个，也可能一个都没有。在此前提下，我们需要的是一个与训练数据矛盾较小的决策树，同时泛化能力较好。 决策树的学习目标使用损失函数，通常为正则化的极大似然函数 选取最优决策树是NP完全问题，现实中一般只能近似求解。 学习步骤 构建根节点 选择最有特征，以此分割训练集 若子集被正确分类，构建叶节点，否则，继续选择新的最优特征 重复以上两步，直到所有训练数据子集被正确分类 特征选择信息增益 熵表示随机变量不确定性的度量，设 X 是一个取有限个值的离散随机变量，其概率分布为 P(X=x_i)=p_i,i=1,2,\\dots,n​ 则随机变量 X 的熵定义为 H(X)=-\\sum \\limits_{i=1}^np_i \\log p_i​ 上式中的对数以 2 为底或以 e 为底，这时熵的单位分别为比特（bit）或纳特（nat）。熵只取决于 X 的分布，而与 X 的取值无关，所以也可以将 X 记作 H(p)，即 H(p)=-\\sum \\limits_{i=1}^np_i \\log p_i, 0 \\leq H(p) \\leq \\log n 条件熵表示在已知随机变量 X 的条件下随机变量 Y 的不确定性。定义为 X 给定条件下 Y 的条件概率分布的熵对 X 的数学期望 H(Y|X)=\\sum \\limits_{i=1}^n p_iH(Y|X=x_i)当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵即分别称为经验熵和经验条件熵。 信息增益：得知特征 X 而使类 Y 的信息不确定性减少的程度，定义为： g(D,A)=H(D)-H(D|A)​ D 为训练数据集，A 为某一特征。 ​ 一般地，熵 H(Y) 与条件熵 H(Y|X) 之差称为互信息（mutual information）。那么此时决策树学习中的信息增益等价于训练数据集中类与特征的互信息。此时即得知：信息增益大的特征被视为具有更强的分类能力，这也是特征选择的原则。 算法 信息增益比上述以信息增益大小作为划分训练集的依据，存在偏向于选择特征取值多的特征的问题。所以可以使用信息增益比（information gain ratio）予以校正替代。 g_R(D,A)=\\frac{g(D,A)}{H_A(D)}\\\\ H_A(D)=-\\sum \\limits_{i=1}^n \\frac{|D_i|}{|D|} \\log_2 \\frac{|D_i|}{|D|}，n是特征A的取值个数决策树的生成ID3算法算法核心过程是在决策树各个节点上应用信息增益准则选择最合适的特征，递归地构建决策树。相当于使用极大似然法进行概率模型的选择。 C4.5算法与 ID3 算法的区别或者说改进在于，其特征选择使用的是信息增益比准则。 决策树的剪枝如果仅让决策树按如上算法进行生成，这样往往造成的结果是过拟合数据。那么在决策树上阻止产生过拟合的方法便是简化生成的树，这个简化过程称为剪枝。 简单来讲，一颗“优秀”的决策树应该是在具有好的拟合和泛化能力的同时，又“不高”（深度小）又“不宽”（叶节点少）。 剪枝方式根据剪枝的时间可以分为预剪枝和后剪枝，即预剪枝是在生成决策树的过程中对每个节点在划分前进行估计，若是当前结点的划分不能提升泛化能力，则停止划分，使当前节点为叶节点；后剪枝是指在生成一颗完整的决策树之后，自底向上地对内部节点进行考察，若一内部节点变为叶节点可以提升泛化能力，则将其替换为叶节点。 预剪枝基于阈值的预剪枝方法的大致过程 限定决策树的深度 设定一个阈值 设置某个指标，比较结点划分前后的泛化能力 基于测试集上的误差率的预剪枝方法上述是B站简博士的补充内容，但是个人认为这里的测试集应该是验证集 测试集上的误差率：测试集中错误分类的实例占比 基本流程即是比较内部节点划分前后在测试集的误差率是否降低，如果误差率降低则选择节点划分，如果误差率反而升高了则不划分。","link":"/2023/02/26/%E5%86%B3%E7%AD%96%E6%A0%91/"},{"title":"利用GitHub+hexo 搭建个人博客网站教程","text":"搭建步骤： 获得个人网站域名 GitHub创建个人仓库 安装Git 安装Node.js 安装Hexo 推送网站 绑定域名 更换主题 获得个人网站域名 直接阿里云购买（其它也行）。申请入口：域名注册 购买域名这也是我们整个搭建过程中惟一一个需要花钱的地方。如果你已经有了空闲域名就无需购买，直接使用即可。 GitHub创建个人仓库 登录到GitHub,如果没有GitHub帐号，使用你的邮箱注册GitHub帐号：Build software better, together 点击GitHub中的New repository创建新仓库，仓库名应该为：用户名.http://github.io 这个用户名使用你的GitHub帐号名称代替，这是固定写法，比如我的仓库名为： polaris973.github.io 安装Git 什么是Git ?简单来说Git是开源的分布式版本控制系统，用于敏捷高效地处理项目。我们网站在本地搭建好了，需要使用Git同步到GitHub上。如果想要了解Git的细节，参看廖雪峰老师的Git教程：Git教程 从Git官网下载：Git - Downloading Package 现在的机子基本都是64位的，选择64位的安装包，下载后安装，在命令行里输入git测试是否安装成功，若安装失败，参看其他详细的Git安装教程。安装成功后，将你的Git与GitHub帐号绑定，鼠标右击打开Git Bash 或者在菜单里搜索Git Bash，设置user.name和user.email配置信息： 12git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub注册邮箱&quot; 生成ssh密钥文件： 1ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot; 然后直接三个回车即可，默认不需要设置密码然后找到生成的.ssh的文件夹中的id_rsa.pub密钥，将内容全部复制 打开GitHub_Settings_keys 页面，新建new SSH Key Title为标题，任意填即可，将刚刚复制的id_rsa.pub内容粘贴进去，最后点击Add SSH key。在Git Bash中检测GitHub公钥设置是否成功，输入 ssh git@github.com ： 如上则说明成功。这里之所以设置GitHub密钥原因是，通过非对称加密的公钥与私钥来完成加密，公钥放置在GitHub上，私钥放置在自己的电脑里。GitHub要求每次推送代码都是合法用户，所以每次推送都需要输入账号密码验证推送用户是否是合法用户，为了省去每次输入密码的步骤，采用了ssh，当你推送的时候，git就会匹配你的私钥跟GitHub上面的公钥是否是配对的，若是匹配就认为你是合法用户，则允许推送。这样可以保证每次的推送都是正确合法的。 安装Node.js Hexo基于Node.js，Node.js下载地址：Download | Node.js 下载安装包，注意安装Node.js会包含环境变量及npm的安装，安装后，检测Node.js是否安装成功，在命令行中输入 node -v : 检测npm是否安装成功，在命令行中输入npm -v : 到这了，安装Hexo的环境已经全部搭建完成。 安装Hexo Hexo就是我们的个人博客网站的框架， 这里需要自己在电脑常里创建一个文件夹，可以命名为Blog，Hexo框架与以后你自己发布的网页都在这个文件夹中。创建好后，进入文件夹中，按住shift键，右击鼠标点击命令行 使用npm命令安装Hexo，输入： 1npm install -g hexo-cli 这个安装时间较长耐心等待，安装完成后，初始化我们的博客，输入： 1hexo init blog 注意，这里的命令都是作用在刚刚创建的Blog文件夹中。 为了检测我们的网站雏形，切换目录到blog文件夹中，分别按顺序输入以下三条命令： 12345hexo new test_my_sitehexo ghexo s 这些命令在后面作介绍，完成后，打开浏览器输入地址： localhost:4000 可以看出我们写出第一篇博客，只不过我下图是我修改过的配置，和你的显示不一样。 现在来介绍常用的Hexo 命令 npm install hexo -g #安装Hexonpm update hexo -g #升级hexo init #初始化博客 命令简写hexo n “我的博客” == hexo new “我的博客” #新建文章hexo g == hexo generate #生成hexo s == hexo server #启动服务预览hexo d == hexo deploy #部署 hexo server #Hexo会监视文件变动并自动更新，无须重启服务器hexo server -s #静态模式hexo server -p 5000 #更改端口hexo server -i 192.168.1.1 #自定义 IPhexo clean #清除缓存，若是网页正常情况下可以忽略这条命令 刚刚的三个命令依次是新建一篇博客文章、生成网页、在本地预览的操作。 推送网站 上面只是在本地预览，接下来要做的就是就是推送网站，也就是发布网站，让我们的网站可以被更多的人访问。在设置之前，需要解释一个概念，在blog根目录里的_config.yml文件称为站点配置文件，如下图 进入根目录里的themes文件夹，里面也有个_config.yml文件，这个称为主题配置文件，如下图 下一步将我们的Hexo与GitHub关联起来，打开站点的配置文件_config.yml，翻到最后修改为： deploy:type: gitrepo: 这里填入你之前在GitHub上创建仓库的完整路径，记得加上 .gitbranch: main 参考如下： 保存站点配置文件。 其实就是给hexo d 这个命令做相应的配置，让hexo知道你要把blog部署在哪个位置，很显然，我们部署在我们GitHub的仓库里。最后安装Git部署插件，输入命令： 1npm install hexo-deployer-git --save 这时，我们分别输入三条命令： 123hexo clean hexo g hexo d 其实第三条的 hexo d 就是部署网站命令，d是deploy的缩写。完成后，打开浏览器，在地址栏输入你的放置个人网站的仓库路径，即 http://xxxx.github.io 你就会发现你的博客已经上线了，可以在网络上被访问了。 绑定域名 虽然在Internet上可以访问我们的网站，但是网址是GitHub提供的:http://xxxx.github.io (知乎排版可能会出现”http://&quot;字样) 而我们想使用我们自己的个性化域名，这就需要绑定我们自己的域名。这里演示的是在阿里云万网的域名绑定，在国内主流的域名代理厂商也就阿里云和腾讯云。登录到阿里云，进入管理控制台的域名列表，找到你的个性化域名，进入解析 然后添加解析 185.199.108.153是GitHub的地址，你也可以ping你的 http://xxxx.github.io 的ip地址，填入进去。CNAME的记录值是：你的用户名.http://github.io 这里千万别弄错了。第二步，登录GitHub，进入之前创建的仓库，点击settings，设置Custom domain，输入你的域名 点击save保存。第三步，进入本地博客文件夹 ，进入blog/source目录下，创建一个记事本文件，输入你的域名，对，只要写进你自己的域名即可。如果带有www，那么以后访问的时候必须带有www完整的域名才可以访问，但如果不带有www，以后访问的时候带不带www都可以访问。所以建议，不要带有www: 保存，命名为CNAME ，注意保存成所有文件而不是txt文件。 完成这三步，进入blog目录中，按住shift键右击打开命令行，依次输入： 123hexo cleanhexo ghexo d 这时候打开浏览器在地址栏输入你的个性化域名将会直接进入你自己搭建的网站。 发布文章 我们开始正式发布上线博客文章，在命令行中输入： 1hexo n &quot;博客名字&quot; 我们会发现在blog根目录下的source文件夹中的_post文件夹中多了一个 博客名字.md 文件，使用Markdown编辑器打开，就可以开始你的个人博客之旅了，Markdown常用的样式也就十来种，完全能够满足一般博文的样式要求，这是我的一篇博文内容示例： 通过带有预览样式的Markdown编辑器实时预览书写的博文样式，也可以通过命令 hexo s —debug 在本地浏览器的localhost:4000 预览博文效果。写好博文并且样式无误后，通过hexo g、hexo d 生成、部署网页。随后可以在浏览器中输入域名浏览。","link":"/2021/10/04/%E5%88%A9%E7%94%A8GitHub+hexo%20%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E7%BD%91%E7%AB%99%E6%95%99%E7%A8%8B/"},{"title":"刷题笔记——【数据结构：链表】","text":"链表的概念我们知道数组是很常用的数据储存方式，而链表就是继数组之后，第二种最通用的数据储存方式了。数组需要存放在连续的空间，计算机很容易实现。而链表的好处是不用确定空间长度，不够的时候，直接申请新的节点，帮助插入。所以链表可以更灵活地进行内存分配。 链表（linked list）是一种序列形的数据结构，其中包含了很多通过链接 (link) 被串起来的节点。每个节点有一个数据域，储存着节点的数值，还有一个指针域，指向下一个节点。 简单来说，链表是由一系列节点组成的，每个节点通过链接和其他节点链接。每个节点包含两个属性，一个是数值，记录了节点的数据，另一个是指针，记录了下一个节点的位置。正因为节点的指针能够记录其他节点的位置，所以我们不需要将节点按顺序排列。 链表的类型链表有很多不同的类型，但本质上都是一系列通过链接相连的节点。 单链表：链表中最简单的一种是单向链表，它包含两个域，一个信息域和一个指针域。这个链接指向列表中的下一个节点，而最后一个节点则指向一个空值。 双链表：双链表是为了解决链表节点不知道前面节点的尴尬，于是在节点的定义中，存在一个父节点，和一个子节点。这样就能顺着父节点往回找。 循环链表：在一个 循环链表中, 首节点和末节点被连接在一起。这种方式在单向和双向链表中皆可实现。 块状链表：块状链表结合了数组和链表的特性，将连续成段的数组通过链接串起来。块状链表的特点是插入很灵活，寻找特定元素也比正常链表快速。 链表是一种重要的基础数据结构，可以用来生成其它类型的数据结构，比如之后讲到的堆、栈、树和图等等。单向链表也是链表中最基础的类型，其它变种也是基于它的，在接下来的一部分，我会着重讲解单向链表。 链表的基本操作每种数据结构都有其对应的操作，而链表包含以下基本操作： 插入：将一个新元素插入链表的任意位置。 删除：将一个元素从链表中删除。 查找（遍历）：查找一个特定的元素。 更新：更新一个节点上的元素。 单向链表的实现以下是链表的定义： 12345678910111213141516public class LinkedList { static class ListNode( int val; ListNode next; public ListNode(int val) { this.val = val; } ListNode head; ListNode tail; int size; public LinkedList() { head = null; tail = null; size = 0; }} 其中ListNode是节点的定义，其中的属性val就是数据，而next就是下一个节点。而 LinkedList 则是单向链表类，其中包含头节点head和尾节点tail。在初始化的时候，我们将头尾节点都设为null，size初始化为0。 插入在一个链表中插入新元素分为以下三种情况： 插入到链表的最前头，作为新的头结点。 插入到链表中间的位置。 插入到链表的尾部，作为链表中最后的元素。 虽然新元素的插入位置不固定，但是链表插入的思想的固定的。插入只需要两步：将新节点的next指针指向插入位置后的节点，再将插入节点前的next指针指向新插入的节点。 比如，我们在链表 {1, 2, 3, 4} 的基础上分别实现在头部、中间、尾部插入新元素5，过程如下图所示： 要注意的是，我们必须先执行步骤1，再执行步骤2；如果先执行步骤2，否则会导致插入位置后续的节点无法被找到。以下是代码： 12345678910111213141516171819202122232425262728293031323334353637// insert the element to the specific position, position starts from index 0public void insert(int position, int number) { if (position &gt; size) { return; } ListNode newNode = new ListNode(number); if (position == 0) { newNode.next = head; head = newNode; if(tail == null) { tail = newNode; } size++; } else if (position == size) { this.append(number); } else { ListNode prev = head; for (int i = 0; i &lt; position - 1; i++) { prev = prev.next; } ListNode next = prev.next; newNode.next = next; prev.next = newNode; size++; }}// append the new element to the end of the listpublic void append(int number) { ListNode newNode = new ListNode(number); if(tail == null) { tail = newNode; } else { tail.next = newNode; tail = newNode; } size++;} 删除删除元素如下图所示，只要找到我们要删除的节点，然后将前面节点的next指针指向被删除节点的下一节点即可： 123456789101112131415161718192021222324public void delete(int number) { if(head != null &amp;&amp; head.val == number) { // delete the head node head = head.next; size--; if(size == 0) { // corner case: no element is left tail = head; } } else { ListNode prev = head; ListNode cur = head; while (prev != null &amp;&amp; cur != null) { if (cur.val == number) { if(cur == tail) { // corner case: delete the last element tail = prev; } prev.next = cur.next; size--; return; } prev = cur; cur = cur.next; } }} 查找查找元素比较简单，只要从头节点开始遍历，找到与目标值相对应的节点，返回其位置即可： 12345678910public int search(int number) { ListNode cur = head; for(int index = 0; cur != null; index++) { if(cur.val == number) { return index; } cur = cur.next; } return -1;} 更新更新链表和查找相似，只要找到对应的节点后，改变节点的值即可： 1234567891011public int update(int oldValue, int newValue) { ListNode cur = head; for(int index = 0; cur != null; index++) { if(cur.val == oldValue) { cur.val = newValue; return index; } cur = cur.next; } return -1;} Java完整代码 GitHub完整代码链接 LeetCode题目 Design Linked List (707) Merge Two Sorted Lists (21) 参考视频：https://www.bilibili.com/video/BV1xa411A76q?p=4 Reverse Linked List (206) 参考视频：https://www.bilibili.com/video/BV1KZ4y157Up?from=search&amp;seid=2806720048252943383&amp;spm_id_from=333.337.0.0 Linked List Cycle (141) Linked List Cycle II (142) Plus One Linked List (369) LRU Cache (146)","link":"/2021/12/06/%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0%E2%80%94%E2%80%94%E3%80%90%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%EF%BC%9A%E9%93%BE%E8%A1%A8%E3%80%91/"},{"title":"","text":"多分类模型Accuracy, Precision, Recall和F1-score的超级无敌深入探讨前言众所周知，机器学习分类模型常用评价指标有Accuracy, Precision, Recall和F1-score，而回归模型最常用指标有MAE和RMSE。但是我们真正了解这些评价指标的意义吗？ 在具体场景（如不均衡多分类）中到底应该以哪种指标为主要参考呢？多分类模型和二分类模型的评价指标有啥区别？多分类问题中，为什么Accuracy = micro precision = micro recall = micro F1-score? 什么时候用macro, weighted, micro precision/ recall/ F1-score? 主要分为以下两点： 二分类模型的常见指标快速回顾 多分类模型的常见指标详细解析 在探讨这些问题前，让我们先回顾一下最常见的指标Accuracy到底有哪些不足。 Accuracy是分类问题中最常用的指标，它计算了分类正确的预测数与总预测数的比值。但是，对于不平衡数据集而言，Accuracy并不是一个好指标。为啥？ 假设我们有100张图片，其中91张图片是「狗」，5张是「猫」，4张是「猪」，我们希望训练一个三分类器，能正确识别图片里动物的类别。其中，狗这个类别就是大多数类 (majority class)。当大多数类中样本（狗）的数量远超过其他类别（猫、猪）时，如果采用Accuracy来评估分类器的好坏，那么即便模型性能很差 (如无论输入什么图片，都预测为「狗」)，也可以得到较高的Accuracy Score（如91%）。此时，虽然Accuracy Score很高，但是意义不大。当数据异常不平衡时，Accuracy评估方法的缺陷尤为显著。 因此，我们需要引入Precision （精准度），Recall （召回率）和F1-score评估指标。考虑到二分类和多分类模型中，评估指标的计算方法略有不同，我们将其分开讨论。 二分类模型的常见指标快速回顾在二分类问题中，假设该样本一共有两种类别：Positive和Negative。当分类器预测结束，我们可以绘制出混淆矩阵（confusion matrix）。其中分类结果分为如下几种： True Positive (TP): 把正样本成功预测为正。 True Negative (TN)：把负样本成功预测为负。 False Positive (FP)：把负样本错误地预测为正。 False Negative (FN)：把正样本错误的预测为负。 在二分类模型中，Accuracy，Precision，Recall和F1 score的定义如下： 其中，Precision着重评估在预测为Positive的所有数据中，真实Positve的数据到底占多少？Recall着重评估：在所有的Positive数据中，到底有多少数据被成功预测为Positive? 举个例子，一个医院新开发了一套癌症AI诊断系统，想评估其性能好坏。我们把病人得了癌症定义为Positive，没得癌症定义为Negative。那么， 到底该用什么指标进行评估呢？ 如用Precision对系统进行评估，那么其回答的问题就是： 1在诊断为癌症的一堆人中，到底有多少人真得了癌症？ 如用Recall对系统进行评估，那么其回答的问题就是： 1在一堆得了癌症的病人中，到底有多少人能被成功检测出癌症？ 如用Accuracy对系统进行评估，那么其回答的问题就是： 1在一堆癌症病人和正常人中，有多少人被系统给出了正确诊断结果（患癌或没患癌）？ OK，那啥时候应该更注重Recall而不是Precision呢？ 当False Negative (FN)的成本代价很高 (后果很严重)，希望尽量避免产生FN时，应该着重考虑提高Recall指标。 在上述例子里，False Negative是得了癌症的病人没有被诊断出癌症，这种情况是最应该避免的。我们宁可把健康人误诊为癌症 (FP)，也不能让真正患病的人检测不出癌症 (FN) 而耽误治疗离世。在这里，癌症诊断系统的目标是：尽可能提高Recall值，哪怕牺牲一部分Precision。 那啥时候应该更注重Precision而不是Recall呢？ 当False Positive (FP)的成本代价很高 (后果很严重)时，即期望尽量避免产生FP时，应该着重考虑提高Precision指标。 以垃圾邮件屏蔽系统为例，垃圾邮件为Positive，正常邮件为Negative，False Positive是把正常邮件识别为垃圾邮件，这种情况是最应该避免的（你能容忍一封重要工作邮件直接进了垃圾箱，被不知不觉删除吗？）。我们宁可把垃圾邮件标记为正常邮件 (FN)，也不能让正常邮件直接进垃圾箱 (FP)。在这里，垃圾邮件屏蔽系统的目标是：尽可能提高Precision值，哪怕牺牲一部分recall。 而F1-score是Precision和Recall两者的综合。 举个更有意思的例子（我拍脑袋想出来的，绝对原创哈），假设检察机关想将罪犯捉拿归案，需要对所有人群进行分析，以判断某人犯了罪（Positive），还是没犯罪（Negative）。显然，检察机关希望不漏掉一个罪人（提高recall），也不错怪一个好人（提高precision），所以就需要同时权衡recall和precision两个指标。 尤其在上个世纪，中国司法体制会更偏向Recall，即「天网恢恢，疏而不漏，任何罪犯都插翅难飞」。而西方司法系统会更偏向Precision，即「绝不冤枉一个好人，但是难免有罪犯成为漏网之鱼，逍遥法外」。到底是哪种更好呢？显然，极端并不可取。Precision和Recall都应该越高越好，也就是F1应该越高越好。 呼，二分类问题的常见指标和试用场景终于讲完了。咦，说好的快速回顾呢？ 多分类模型的常见指标解析在多分类（大于两个类）问题中，假设我们要开发一个动物识别系统，来区分输入图片是猫，狗还是猪。给定分类器一堆动物图片，产生了如下结果混淆矩阵。 在混淆矩阵中，正确的分类样本（Actual label = Predicted label）分布在左上到右下的对角线上。其中，Accuracy的定义为分类正确（对角线上）的样本数与总样本数的比值。Accuracy度量的是全局样本预测情况。而对于Precision和Recall而言，每个类都需要单独计算其Precision和Recall。 比如，对类别「猪」而言，其Precision和Recall分别为: 也就是， （P代表Precision） （R代表Recall） 如果想评估该识别系统的总体功能，必须考虑猫、狗、猪三个类别的综合预测性能。那么，到底要怎么综合这三个类别的Precision呢？是简单加起来做平均吗？通常来说， 我们有如下几种解决方案（也可参考scikit-learn官网）： Macro-average方法 该方法最简单，直接将不同类别的评估指标（Precision/ Recall/ F1-score）加起来求平均，给所有类别相同的权重。该方法能够平等看待每个类别，但是它的值会受稀有类别影响。 2. Weighted-average方法 该方法给不同类别不同权重（权重根据该类别的真实分布比例确定），每个类别乘权重后再进行相加。该方法考虑了类别不平衡情况，它的值更容易受到常见类（majority class）的影响。 (W代表权重，N代表样本在该类别下的真实数目) 3. Micro-average方法 该方法把每个类别的TP, FP, FN先相加之后，在根据二分类的公式进行计算。 其中，特别有意思的是，Micro-precision和Micro-recall竟然始终相同！这是为啥呢？ 这是因为在某一类中的False Positive样本，一定是其他某类别的False Negative样本。听起来有点抽象？举个例子，比如说系统错把「狗」预测成「猫」，那么对于狗而言，其错误类型就是False Negative，对于猫而言，其错误类型就是False Positive。于此同时，Micro-precision和Micro-recall的数值都等于Accuracy，因为它们计算了对角线样本数和总样本数的比值，总结就是： 最后，我们运行一下代码，检验手动计算结果是否和Sklearn包结果一致： 123456789101112131415161718192021222324import numpy as npimport seaborn as snsfrom sklearn.metrics import confusion_matriximport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.metrics import accuracy_score, average_precision_score,precision_score,f1_score,recall_score# create confusion matrixy_true = np.array([-1]*70 + [0]*160 + [1]*30)y_pred = np.array([-1]*40 + [0]*20 + [1]*20 + [-1]*30 + [0]*80 + [1]*30 + [-1]*5 + [0]*15 + [1]*20)cm = confusion_matrix(y_true, y_pred)conf_matrix = pd.DataFrame(cm, index=['Cat','Dog','Pig'], columns=['Cat','Dog','Pig'])# plot size settingfig, ax = plt.subplots(figsize = (4.5,3.5))sns.heatmap(conf_matrix, annot=True, annot_kws={&quot;size&quot;: 19}, cmap=&quot;Blues&quot;)plt.ylabel('True label', fontsize=18)plt.xlabel('Predicted label', fontsize=18)plt.xticks(fontsize=18)plt.yticks(fontsize=18)plt.savefig('confusion.pdf', bbox_inches='tight')plt.show() 123456789101112print('------Weighted------')print('Weighted precision', precision_score(y_true, y_pred, average='weighted'))print('Weighted recall', recall_score(y_true, y_pred, average='weighted'))print('Weighted f1-score', f1_score(y_true, y_pred, average='weighted'))print('------Macro------')print('Macro precision', precision_score(y_true, y_pred, average='macro'))print('Macro recall', recall_score(y_true, y_pred, average='macro'))print('Macro f1-score', f1_score(y_true, y_pred, average='macro'))print('------Micro------')print('Micro precision', precision_score(y_true, y_pred, average='micro'))print('Micro recall', recall_score(y_true, y_pred, average='micro'))print('Micro f1-score', f1_score(y_true, y_pred, average='micro')) 运算结果完全一致，OK，机器学习多分类模型的常见评估指标已经基本介绍完毕，大家如果有疑问尽情在评论区留言哈，我争取尽快回复大家 :)","link":"/2023/02/19/%E5%A4%9A%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8BAccuracy,%20Precision,%20Recall%E5%92%8CF1-score%E7%9A%84%E8%B6%85%E7%BA%A7%E6%97%A0%E6%95%8C%E6%B7%B1%E5%85%A5%E6%8E%A2%E8%AE%A8/"},{"title":"如何理解线性代数（知乎陈二喜）","text":"线性代数的本质 - 系列合集狂吹这个关于线性代数理解的可视化视频！！ 注：此文为知乎作者陈二喜所写，由于该作者账号注销，便保存以之后拜读，侵权则删 0， 引言 人类在探索每一个科学问题的时候，为了简化问题，都会把具体科学问题看作一个机器。给这个机器输入一个条件，机器会运转，对条件进行加工，然后输出一种现象。 通过研究输入与输出，有时候可以推测出机器内部的构造，这就是所谓的科学。比如牛顿，他发现：给物体一个力，就能使物体产生一个加速度，力越大，加速度就越大。 当然，有时候研究了输入与输出，依然没有搞清楚机器内部的原理，只是知道一个大概的规律，那么就干脆先不管内部原理，先把这个规律为自己所用。这就是所谓的工程。比如，人们通过做实验发现，给机翼一个气流，机翼就能够产生一个升力，人们并不能解释升力是怎么产生的，但是不妨碍自己使用，于是给一个驾驶舱装上两个翅膀，飞机就上天了。 人类探索自然运行的原理，归根结底是想利用这些原理，对万物进行定量控制。 定量控制的意思是说：牛顿写出《原理》这本书的时候，不能够含糊其辞的说，给物体很大的力，物体就能产生很大的加速度。而是必须告诉大众：给一个多少Kg的物体多少N的力能够产生多少 的加速度。 这时候，数学应运而生。简而言之，数学就是人类在解释这个世界是怎样运行的时候，人为发明的一种工具，有了这种工具，我们可以不用那么含糊其辞。 于是，就有了函数。 于是就有了F=Ma，于是就有了各种各样的公式、定理及定律。 1， 什么是线性代数 函数研究的是，输入一个数，经过函数运算之后，产出一个数。而有时候我们研究的问题太复杂，需要输入很多个数，经过运算之后，产出很多个数。这时候，线性代数应运而生。 很多个数，我们可以用括号括起来，形成一个数组。在几何学上，数组被称作向量，向量就是一个有大小有方向的直线段。 所以，线性代数就是：输入一段直线，经过加工之后，产出一段直线。 线性的意思就是，你往机器里扔进去直线，产出的肯定也是直线。 当然，在数学上，线性有着及其严格的定义，并不是像我刚才说的那么简单。不过，正由于线性的严格定义，才能够实现：输入一段直线，产出一段直线。 与函数相类似，用图描述线性代数就是： 输入叫向量，内部原理叫矩阵，输出叫向量。 2， 矩阵是怎么对直线进行加工的？ 通过函数表达式y=5x+9我们可以一目了然地知道，输入的自变量x是怎样一步步被加工，最后输出因变量y的。 同样，我们通过观察矩阵，也可以一目了然地知道，输入的直线是怎样一步步被加工的。 假如输入的直线为[1,2]。 插一句，向量[1,2]的全称其实是1i+2j，i和j叫做基向量。意思是说，我们目前所写出来的向量，是以这两个向量作为基本原料，拼凑组合出来的。 假如用于加工向量的矩阵为[0,1 -1,0]， 那么这个矩阵所代表的加工过程就是，把基向量i，换成矩阵中的第一列，把基向量j换成矩阵中的第二列。然后再以新的基向量为原料，重新利用[1，2]拼凑一个新的向量。用新的基向量拼凑出来的新向量就是输出。 通过展示矩阵对向量的加工过程，我们可以“看出”上面例子的解。 下面，我们用熟悉的口诀“左行乘右列”来检验一下上面的答案是否靠谱。 其实，计算所用的口诀就来源于上述加工过程。 同理，稍微复杂一些的三维向量遇到三维矩阵后的加工过程如下图： 3， 行列式是什么？ 矩阵对向量进行加工，行列式能够描述这种加工作用的强弱。 上文提到，矩阵对向量加工是通过改变基向量来实现的。以二维为例，默认的基向量张成的面积为1，经过矩阵变换之后形成的新的基向量张成的面积变为了S，那么这个矩阵的行列式就为S。 有时候，矩阵的行列式为0，说明新的基向量张成的面积为0，说明新的基向量发生了重合。 有时候，矩阵的行列式为负数，说明线性空间发生了翻转。也就是说，本来，默认的两个基向量，j在i的逆时针方向，经过矩阵加工之后，线性空间发生了翻转，导致i在j的逆时针方向。如下图： 4， 什么叫单位矩阵？ 矩阵能够对向量进行加工，产生一个新的向量。但有一种矩阵比较特殊，无论给它输入什么样的向量，加工后产生的向量都与原来的相同，这种矩阵叫单位矩阵。 既然矩阵对向量的加工作用是通过改变基向量来实现的，如果想保持输入与输出相等，那么只需要保证矩阵不会改变基向量即可。 所以，二阶单位矩阵，三阶单位矩阵以及n阶单位矩阵可写为： 5， 什么叫逆矩阵？ 矩阵对向量具有加工作用，两个矩阵相乘，则表示的是两种加工作用的叠加。也就是说： 如果上图中向量1等于向量3，那么就说明，向量经过矩阵1和矩阵2的加工之后，又变成了原来的自己。进一步说明，矩阵1和矩阵2对于向量的加工作用刚好相反。那么就说矩阵1和矩阵2互为逆矩阵。 明白了原理，也就知道如何求解逆矩阵了。 插个题外话：为什么行列式为0的矩阵没有逆矩阵？ 因为行列式如果为0，表明矩阵在在对向量变换的过程中，将向量空间压缩到了一个更低的维度上。以二维矩阵为例： 向量降维后，将无法再还原回原来的样子。 就好比有一个三维长方体，从大部分角度观察，都是一个三维结构，但是当正视俯视侧视时，你只能观察到一个二维矩形。我们是无法通过这个二维矩形的样子，来推测出原来的长方体的。 6， 什么是秩 矩阵可以将一个向量进行加工，变成另外一个向量。 比如一个3阶矩阵，可以对很多三维向量进行加工，变成很多新的三维向量。 有时候，所有的这些新的三维向量，最终都落在一条直线上，即1维。 有时候，所有的新的三维向量最终都落在一个二维平面上，即2维。 有时候，所有的新的三维向量最终都落在三维空间上，即3维。 以上情况分别对应于秩为1，2，3。 总之，秩就是描述这个矩阵会不会将输入的向量空间降维。如果没有降维，这种情况称为满秩。 7， 什么是特征向量、特征值？ 矩阵能够对向量进行加工，变成一个新的向量。 有时候会出现这种情况： 对于某一个矩阵，输入一个向量，经过矩阵的加工后，新生成的向量与原来的向量共线。也就是说这个矩阵对这个特定的向量的加工过程中没有改变其方向。 那么，这个不会被改变方向的向量叫做这个矩阵的特征向量。 虽然不会被改变方向，但是改变了大小，新的向量长度是原来的向量的长度的 倍，这个 叫做特征向量的特征值。 8，有所疏漏，写的不全，想听哪里在评论区留言。","link":"/2021/10/24/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%EF%BC%88%E7%9F%A5%E4%B9%8E%E9%99%88%E4%BA%8C%E5%96%9C%EF%BC%89/"},{"title":"数据挖掘学习_Apriori算法","text":"Assocation Role 关联规则目的：在庞大的元素库中找出元素之间的关联关系 这个规则主要应用在商品的推销上（啤酒与尿布） 还可以延伸用于文字分析，把单个单词看做元素，对每篇文章分析即可得出单词之间的关联关系 Support&amp;Confidence of Association Role 支持度 置信度名词定义： 支持度：在关联规则分析中就是频率 —&gt; 关联发生的次数 / 总次数 置信度：为在某元素出现的情况下关联发生的概率，即条件概率 —&gt; 关联发生的次数 / 包含某一元素的次数 注意规则顺序，顺序相反分母会变化，Confidence也会变化 频繁项集：项集I的相对支持度满足预定义的最小支持度阈值，则I是频繁项集 强关联规则：同时满足最小支持度和最小置信度的关联规则称为强关联规则 关联规则挖掘关联规则挖掘是一个两步的过程： 找出所有的频繁项集 由频繁项集产生强关联规则 主要挑战是：常产生大量满足最小支持度阈值的项集，当最小支持度设置得很低得时候尤为如此。这是因为如果一个项集是频繁得，则它的每一个子集也是频繁的。一个长项集将包含组合个数较短的频繁子项集。 宏观处理： 设置一个支持度、置信度的一个限值 把所有频繁的组合找出来 生成频繁组合的所有非空子集，并写出所有可能的子集关联规则 计算所有子集关联规则的支持度与置信度 比较子集的支持度、置信度与设定的限值，若超过则认为关联性强 About Assocation Role 关联规则的误区规则的关联性很强并不一定代表该规则有意义（当两个元素频率差别特别大时尤为明显） 关联规则就是一个条件概率，两件事物相关，并不一定代表他们之间有代表因果关系，不应做过多解释。 Apriori algorithm Apriori算法元素因为排列组合，所有可能的非空关联的数量非常庞大，为 2^d-1所以不能用传统方法解决。 Apriori算法原理： 任何一个频繁项的子集必须频繁 如果某个规则不频繁，那么他的超集必须不频繁 具体做法就是： ​ 首先找出频繁1-项集，记为L1；然后利用L1来产生候选项集C2，对C2中的项进行判定挖掘出L2，即频繁2-项集；不断如此循环下去直到无法发现更多的频繁k-项集为止。每挖掘一层Lk就需要扫描整个数据库一遍。算法利用了一个性质：任一频繁项集的所有非空子集也必须是频繁的。 核心思想：尽量避免生成不频繁集 优点：易编码 缺点：本算法需要频繁扫描数据库，而这种操作在大型数据库中成本较高 适用数据类型：数值型 或者 标称型数据。 代码python实现","link":"/2021/10/31/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AD%A6%E4%B9%A0_Apriori%E7%AE%97%E6%B3%95/"},{"title":"感知机","text":"感知机 感知机是二类分类的线性分类模型，属于判别模型 感知机学习旨在求出将训练数据进行线性划分的分离超平面，优化方法是随机梯度下降法 感知机学习算法有两种算法，分为原始形式和对偶形式 是神经网络和支持向量机的基础 当训练数据集线性可分时，学习算法是收敛的，但存在无穷多个解，其解由于不同的初值或迭代顺序而可能有所不同 定义 学习策略 定义（经验）损失函数并极小化Loss 感知机学习的损失函数一般定义为： ​ 其中，M是误分类点的集合。 学习算法 原始形式 对偶形式 ​","link":"/2023/02/18/%E6%84%9F%E7%9F%A5%E6%9C%BA/"},{"title":"数据挖掘学习_EFIM Algorithm","text":"EFIM Algorithm该算法相比于HUI-Miner算法，主要的贡献有 提出了两个更为紧凑的上界来更有效地裁剪搜索空间：sub-tree utility 以及 local utility。 提出一种基于数组（utility bin array）的效用计数技术，从而能够在线性时间内获取后续需要的效用上界值。 提出高效的数据库映射和事务合并技术从而能够提高算法的性能。 定义参数定义eu(i)：元素i的效用值 iu(i, T)：交易集T内元素i的数量 u(i, T)：交易集T内所有元素i的总效用值，u(i, T) = iu(i, T) × eu(i) u(X, T)：交易集T中集合X的总效用值 u(X)：数据库DB中所有集合X的总效用值 tu(T)：交易集T的总效用值 twu(X)：数据库DB中包含集合X的所有交易集的总效用值 re(X,Tc)：是在各事务项中 大于 项集X的所有项的效用值之和 剩余效用边界(remaining utility upper-bound)：这个概念是区别于“两步走”算法的关键之处，因为这个边界值比 TWU 是更为紧凑，能够裁剪更精细。定义为reu(X)=re(X)+u(X) E(α)：根据深度优先搜索顺序得到的可用于扩展α的所有项的集合，即是按order顺序大于α的所有项的集合。 Extension of an itemset：在集合枚举树中出现在α的子树中的项集可称为项集的α的拓展。 Extension of an itemset 包含于 E(α) EFIM论文笔记","link":"/2021/12/05/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AD%A6%E4%B9%A0_EFIM%20Algorithm/"},{"title":"数据挖掘学习_FP-Growth算法","text":"FP-Growth算法原理总结 作为一个挖掘频繁项集的算法，Apriori算法需要多次扫描数据，I/O是很大的瓶颈。为了解决这个问题，FP Tree算法（也称FP Growth算法）采用了一些技巧，无论多少数据，只需要扫描两次数据集，因此提高了算法运行的效率。下面我们就对FP Tree算法做一个总结。 1. FP Tree数据结构 为了减少I/O次数，FP Tree算法引入了一些数据结构来临时存储数据。这个数据结构包括三部分，如下图所示： 第一部分是一个项头表。里面记录了所有的1项频繁集出现的次数，注意的是需要按照次数降序排列。比如上图中B在所有10组数据中出现了8次，因此排在第一位，这部分好理解。第二部分是FP Tree，它将我们的原始数据集映射到了内存中的一颗FP树，这个FP树比较难理解，它是怎么建立的呢？这个我们后面再讲。第三部分是节点链表。所有项头表里的1项频繁集都是一个节点链表的头，它依次指向FP树中该1项频繁集出现的位置。这样做主要是方便项头表和FP Tree之间的联系查找和更新，也好理解。 下面我们讲项头表和FP树的建立过程。 2. 项头表的建立 FP树的建立需要首先依赖项头表的建立。首先我们看看怎么建立项头表。 我们第一次扫描数据，得到所有频繁1项集的的计数。然后删除支持度低于阈值的项，将1项频繁集放入项头表，并按照支持度降序排列。接着第二次也是最后一次扫描数据，将读到的原始数据剔除非频繁1项集，并按照支持度降序排列。Attention！ 上面这段话很抽象，我们用下面这个例子来具体讲解。我们有10条数据，首先第一次扫描数据并对1项集计数，我们发现O，I，L，J，P，M, N都只出现一次，支持度低于20%的阈值，因此他们不会出现在下面的项头表中。剩下的A,C,E,G,B,D,F按照支持度的大小降序排列，组成了我们的项头表。 接着我们第二次扫描数据，对于每条数据剔除非频繁1项集，并按照支持度降序排列。比如数据项ABCEFO，里面O是非频繁1项集，因此被剔除，只剩下了ABCEF。按照支持度的顺序排序，它变成了ACEBF。其他的数据项以此类推。为什么要将原始数据集里的频繁1项数据项进行排序呢？这是为了我们后面的FP树的建立时，可以尽可能的共用祖先节点。 通过两次扫描，项头表已经建立，排序后的数据集也已经得到了，下面我们再看看怎么建立FP树。 3. FP Tree的建立 有了项头表和排序后的数据集，我们就可以开始FP树的建立了。开始时FP树没有数据，建立FP树时我们一条条的读入排序后的数据集，插入FP树，插入时按照排序后的顺序，插入FP树中，排序靠前的节点是祖先节点，而靠后的是子孙节点。如果有共用的祖先，则对应的公用祖先节点计数加1。插入后，如果有新节点出现，则项头表对应的节点会通过节点链表链接上新节点。直到所有的数据都插入到FP树后，FP树的建立完成。 似乎也很抽象，我们还是用第二节的例子来描述。 首先，我们插入第一条数据ACEBF，如下图所示。此时FP树没有节点，因此ACEBF是一个独立的路径，所有节点计数为1, 项头表通过节点链表链接上对应的新增节点。 接着我们插入数据ACG，如下图所示。由于ACG和现有的FP树可以有共有的祖先节点序列AC，因此只需要增加一个新节点G，将新节点G的计数记为1。同时A和C的计数加1成为2。当然，对应的G节点的节点链表要更新 同样的办法可以更新后面8条数据，如下8张图。由于原理类似，这里就不多文字讲解了，大家可以自己去尝试插入并进行理解对比。相信如果大家自己可以独立的插入这10条数据，那么FP树建立的过程就没有什么难度了。 4. FP Tree的挖掘​ 寻找条件模式基 我们辛辛苦苦，终于把FP树建立起来了，那么怎么去挖掘频繁项集呢？看着这个FP树，似乎还是不知道怎么下手。下面我们讲如何从FP树里挖掘频繁项集。得到了FP树和项头表以及节点链表，我们首先要从项头表的底部项依次向上挖掘。对于项头表对应于FP树的每一项，我们要找到它的条件模式基。所谓条件模式基是以我们要挖掘的节点作为叶子节点所对应的FP子树。得到这个FP子树，我们将子树中每个节点的的计数设置为叶子节点的计数，并删除计数低于支持度的节点。从这个条件模式基，我们就可以递归挖掘得到频繁项集了。 实在太抽象了，之前我看到这也是一团雾水。还是以上面的例子来讲解。我们看看先从最底下的F节点开始，我们先来寻找F节点的条件模式基，由于F在FP树中只有一个节点，因此候选就只有下图左所示的一条路径，对应{A:8,C:8,E:6,B:2, F:2}。我们接着将所有的祖先节点计数设置为叶子节点的计数，即FP子树变成{A:2,C:2,E:2,B:2, F:2}。一般我们的条件模式基可以不写叶子节点，因此最终的F的条件模式基如下图右所示。 通过它，我们很容易得到F的频繁2项集为{A:2,F:2}, {C:2,F:2}, {E:2,F:2}, {B:2,F:2}。递归合并二项集，得到频繁三项集为{A:2,C:2,F:2}，{A:2,E:2,F:2},…还有一些频繁三项集，就不写了。当然一直递归下去，最大的频繁项集为频繁5项集，为{A:2,C:2,E:2,B:2,F:2} F挖掘完了，我们开始挖掘D节点。D节点比F节点复杂一些，因为它有两个叶子节点，因此首先得到的FP子树如下图左。我们接着将所有的祖先节点计数设置为叶子节点的计数，即变成{A:2, C:2,E:1 G:1,D:1, D:1}。此时E节点和G节点由于在条件模式基里面的支持度低于阈值，被我们删除，最终在去除低支持度节点并不包括叶子节点后D的条件模式基为{A:2, C:2}。通过它，我们很容易得到D的频繁2项集为{A:2,D:2}, {C:2,D:2}。递归合并二项集，得到频繁三项集为{A:2,C:2,D:2}。D对应的最大的频繁项集为频繁3项集。 同样的方法可以得到B的条件模式基如下图右边，递归挖掘到B的最大频繁项集为频繁4项集{A:2, C:2, E:2,B:2}。 继续挖掘G的频繁项集，挖掘到的G的条件模式基如下图右边，递归挖掘到G的最大频繁项集为频繁4项集{A:5, C:5, E:4,G:4}。 E的条件模式基如下图右边，递归挖掘到E的最大频繁项集为频繁3项集{A:6, C:6, E:6}。 C的条件模式基如下图右边，递归挖掘到C的最大频繁项集为频繁2项集{A:8, C:8}。 至于A，由于它的条件模式基为空，因此可以不用去挖掘了。 至此我们得到了所有的频繁项集，如果我们只是要最大的频繁K项集，从上面的分析可以看到，最大的频繁项集为5项集。包括{A:2, C:2, E:2,B:2,F:2}。 通过上面的流程，相信大家对FP Tree的挖掘频繁项集的过程也很熟悉了。 5. FP Tree算法归纳这里我们对FP Tree算法流程做一个归纳。FP Tree算法包括以下几步： 得到项头表：扫描数据，得到所有频繁一项集的的计数。然后删除支持度低于阈值的项，将1项频繁集放入项头表，并按照支持度降序排列。 扫描数据，将读到的原始数据剔除非频繁1项集，并按照支持度降序排列。 建立FP树：读入排序后的数据集，插入FP树，插入时按照排序后的顺序，插入FP树中，排序靠前的节点是祖先节点，而靠后的是子孙节点。如果有共用的祖先，则对应的公用祖先节点计数加1。插入后，如果有新节点出现，则项头表对应的节点会通过节点链表链接上新节点。直到所有的数据都插入到FP树后，FP树的建立完成。 寻找条件模式基：从项头表的底部项依次向上找到项头表项对应的条件模式基。从条件模式基递归挖掘得到项头表项项的频繁项集（可以参见第4节对F的条件模式基的频繁二项集到频繁5五项集的挖掘）。 如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集。 6. FP tree算法总结 FP Tree算法改进了Apriori算法的I/O瓶颈，巧妙的利用了树结构，这让我们想起了BIRCH聚类，BIRCH聚类也是巧妙的利用了树结构来提高算法运行速度。利用内存数据结构以空间换时间是常用的提高算法运行时间瓶颈的办法。 在实践中，FP Tree算法是可以用于生产环境的关联算法，而Apriori算法则做为先驱，起着关联算法指明灯的作用。除了FP Tree，像GSP，CBA之类的算法都是Apriori派系的。","link":"/2021/11/01/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AD%A6%E4%B9%A0_FP-Growth%E7%AE%97%E6%B3%95/"},{"title":"数据挖掘学习_HUI-Miner算法","text":"本文贡献 提出了一种新的结构——效用表（utility list）。不仅存储关于项集的使用程序信息，还存储关于项集是否该被修剪的启发式信息。 一种高效的算法HUI-Miner。不同于原有的算法，HUI-Miner不需要生成候选高效用项集，在从数据库构建初始效用列表后，HUI-Miner 可以从这些效用列表中挖掘高效用项集。 在各种数据库上进行了广泛的实验，以比较HUI-Miner与最先进的算法。实验结果表明，HUI-Miner算法优于上述算法。 问题定义 problem definition参数定义eu(i)：元素i的效用值 iu(i, T)：交易集T内元素i的数量 u(i, T)：交易集T内所有元素i的总效用值，u(i, T) = iu(i, T) × eu(i) u(X, T)：交易集T中集合X的总效用值 u(X)：数据库DB中所有集合X的总效用值 tu(T)：交易集T的总效用值 twu(X)：数据库DB中包含集合X的所有交易集的总效用值 T/X：T/X表示集合T中所有在（经过排序后）集合X后面的元素的集合！ ru(X, T)：集合T中，T/X的效用值总和 i项扩展：若k项集的子树节点拥有（k+i）项，则称其为 k 项集的 i 项扩展。 相关工作 related work这一节讨论了在高效用项集挖掘中出现的一些算法。包括算法思想继承自Apriori的一些算法，指出其与Apriori算法一样需要多次扫描数据库生成候选集、消除候选集。再讨论了基于FP-Growth算法生成FP-tree，减少扫描数据库的次数从而得到性能提升。但这些算法在大多数情况下还是会生成大量的候选项集。 效用表结构为了挖掘高效用项集，许多以前的算法直接在原始数据库上执行。尽管基于FP-Growth的算法从前缀树生成候选项集，但他们必须通过扫描数据可来计算候选项的准确效用。在本节，作者提出并建议使用一个称为效用表（utility-list）的数据结构来维护数据库的效用信息。 初始化效用表 initial utility-lists在HUI-Miner中，每个项集都保存一个utility-list。通过两次数据库扫描操作来初始化存储效用信息的效用表。首先，通过数据库扫描所有项的twu(i)。如果项目i的TWU低于所给出的阀值，则在后续挖掘中将不再考虑任何包含有 i 的项目集。 对于那些TWU超过阀值的项目，将他们按照TWU的大小升序排列。假设minutil为30的情况下，算法将不再考虑项f和g。如下图 剩下的项将按上面所提到的升序进行排序：e&lt;c&lt;b&lt;a&lt;d 定义8：将TWU小于minutil的所有项从交易中删除被称为“revised”修订，剩余项按TWU大小升序排序。 再次扫描数据库，算法修订每个交易来初始化效用表。 定义9。给定一个项目集X和一个带有X的事务(或项目集)T，该事务(或项目集)T中在X之后的所有项目的集合表示为T/X。 例如，图4中 T2/{eb} = {ad} 和 T2/{c} = {bad}。 定义10。项目集X在交易T中的剩余效用记为ru(X, T)，是 X/T 中所有项的效用之和。 项集X的效用表的每个元素都包含三个字段:tid、iutil和rutil。 字段tid表明一个包含X的交易项T 字段iutil是X在T中的效用，即u(X, T) 字段rutil是X在T中的剩余效用，即ru(X, T) 在第二次数据库扫描过程中，算法构建了如图5所示的初始效用表。 以项集{c}的效用表举例。 在T1中，u({c}， T1) = 2， ru({c}， T1) = u(b, T1) + u(d, T1) = 2 + 5 = 7，因此元素&lt; 1,2,7 &gt;就是{c}的效用表(表示， 1表示T1)。 在T2中，u({c}， T2) = 3，ru({c}， T2) = u(b, T2) + u(a, T2) + u(d, T2) = 2 + 4 + 5 = 11，因此元素&lt; 2,3,11 &gt;也属于{c}的实用列表。其余的可以用同样方法算出来。 2-项集的效用表 utility-lists of 2-itemsets不需要数据库扫描，可以通过{x}的效用列表和{y}的效用表两者的交集来构造2-itemset {xy}的效用列表。 该算法通过比较两个效用列表中的TID来识别公共交易项。假设公用事业列表的长度分别为m和n，假设效用列表的长度分别为m和n，那么最多(m + n)比较就足以识别，因为实用列表中的所有tid都是有序的。识别过程实际上是一个双向比较。例如，图5中项目集{e}和{c}的效用列表之间的tid比较如图6(a)所示。 对于每个交易t，算法将生成元素E并将其添加到{xy}的效用列表中。E的tid字段是t的tid, E的iutil是{x}和{y}效用列表中与t相关的iutil之和。假设x在y之前，然后将E的rutil赋值为{y}的效用列表中与t相关的rutil。 通过比较两个效用列表中的tid，取交集生成新的多元效用列表， 上图(a)第一行是交易表T中包含{e}的项的下标，第二行是交易表T中包含{c}的项的下标(对照Fig.4)， 上图(b)其中 每一项的Iutils取两者Iutils的和，Rutils取两者中小的Rutils(对照Fig.5，因为它们都是排好序的，前面的项与后面的项结合，必然是会导致剩余项的效用值减少(Rutils)，剩下没合并的项少了) Iutils这一列的值相加得到的和就是该项集的效用值 iutils和rutils这两列的值相加得到的就是该项集的扩展项集的ru值 k-项集的效用表 utility-lists of k-itemsets（k&gt;=3）类似于构造2-itemset的效用列表。例如，为了构造{eba}的效用列表，我们可以在图6(b)中将{eb}和{ea}的效用列表取交集，得到的效用列表如图7(a)所示。图4中的数据库视图T2和T5中确实出现了Itemset {eba}，但是T2和T5中项集的效用分别是10和17，而不是14和25。 造成这种误算的原因是计算中有重复计算，即{e}的效用被加了两遍，所以需要剪掉u(e,T)。 HUI-Miner在从数据库构建初始效用列表之后，HUI-Miner算法可以有效地从效用表中挖掘所有高效用项目集，就像Eclat算法挖掘频繁项目集[26]一样。本节首先介绍了HUI-Miner的搜索空间，然后提出了算法的剪枝策略。最后，给出了HUI- Miner算法和一些实现细节。 搜索空间 search space高实用项集挖掘问题的搜索空间可以表示为集枚举树（如下图）。给定一组项I = {i1, i2, i3，…， in}和所有项的总顺序(假设i1 &lt; i2 &lt;···&lt; in)，则表示所有项集的集合枚举树可以构造如下： 首先，创建树的根；（根节点为空） 其次，分别创建根的n个子节点，表示n个1-项集; 第三，对于表示项集{is···ie}(1≤s≤e &lt; n)的节点，表示项集{is···iei(e+1)}， {is···iei(e+2)}，…， {is···iein}被创建。 重复执行第三步，直到创建所有叶节点。 例如，给定I = {e, c, b, a, d}和e &lt; c &lt; b &lt; a &lt; d，图8描绘了表示I的所有项目集的集合枚举树 剪枝策略 pruning strategy穷举搜索可以发现所有高效用项集，但非常耗时，因为对于许多数据库，项的数量都很大。对于一个有n个条目的数据库，穷举搜索必须检查2的n次方个项集。 为了减少搜索空间，我们可以利用项集的效用列表中的iutils和rutils。一个项集的效用列表中所有iutils的和就是该项集的效用，所以该项集如果效用大于给定的minutil就被认为是高效用的。效用列表中所有iutils和rutils的总和为HUI-Miner提供了关于是否应该修剪项集的关键信息。 剪枝策略为： 若该项目集对应效用列表所有iutils的总和大于阀值，则该项集为高效用项集。 若该项目集对应效用列表所有iutils与rutils的总和大于阀值，则该项目集需要进一步判定。 若该项目集对应效用列表所有iutils与rutils的总和小于阀值，则该项目集及其所有扩展都为低效用，对其进行剪枝。 例如，考虑图6(b)中的实用程序列表。Itemset {ec}应该被修剪，因为它的实用列表中所有iutils和rutils的和(即24)小于minutil(即30)。因此，不需要检查itemset {ec}的7个扩展项(见图8)。 HUI-Miner算法 HUI-Miner Algorithm伪代码 同时为了提高效率，减少对效用列表的扫描次数，集合枚举树中一个节点的所有子节点所代表的项集具有相同的前缀项集。为此，我们将扩展项与前缀项分开存储，Fig.7(b)中的表可以构造成如下图结构：","link":"/2021/11/15/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%AD%A6%E4%B9%A0_HUI-Miner%E7%AE%97%E6%B3%95/"},{"title":"逻辑斯谛回归与最大熵模型","text":"逻辑斯谛回归和最大熵模型逻辑（斯谛）回归统计书里先从逻辑斯谛分布开始介绍 差点让我以为我没学过 回去翻了吴恩达佬的讲解 直接放图记忆好了 From sweet Andrew Ng. 一图带走版本 再放一个知乎人话版：用人话讲明白逻辑回归Logistic regression - 知乎 (zhihu.com) 最大熵模型最大熵模型基于最大熵原理。 最大熵原理熵是一个体系内混乱程度的度量，亦可用作信息量或者说信息的不确定性大小的度量：熵越大表示信息量越大，或者说不确定性越大。 那么，在概率模型学习过程中，选择可能的概率模型（分布）时，一般认为熵最大的模型时最好的模型。在这个过程中，通常使用约束条件来确定概率分布的集合。 假设离散随机变量 X 的概率分布是 P(X)，则其熵是： H(P)=-\\sum \\limits_xP(x) \\log P(x) \\\\ 0 \\leq H(P) \\leq \\log |X|最大熵原理认为要选择的最优概率模型首先要满足已有约束条件。但在没有其它更多信息的情况下，不确定的部分都是”等可能的“（等可能意味着混乱程度（不确定性）最大，熵最大）。”等可能“不易操作，最大熵原理通过熵的最大化表示等可能性，即间接地通过优化熵（是个可优化的数值指标）来得到最终模型。 最大熵模型的定义假设所需分类模型是一个条件概率分布P(Y|X)，X\\in\\mathcal X \\subseteq \\R^n 表示输入，Y\\in \\mathcal Y 表示输出，\\mathcal X 和 \\mathcal Y 分别是输入和输出的集合。 给定训练数据集，可以确定联合分布P(X,Y)的经验分布和边缘分布 P(X) 的经验分布，分别以 \\tilde P(X,Y) 和 \\tilde P(X) 表示。看到经验二字，就应联想到训练集，在这里， \\tilde P(X=x,Y=y)=\\frac{\\mathcal v(X=x,Y=y)}{N}\\\\ \\tilde P(X=x)=\\frac{\\mathcal v(X=x)}{N}其中，\\mathcal v(X=x,Y=y) 是训练数据中样本(x,y) 出现的频数，\\mathcal v(X=x) 表示训练数据中 x 出现的频数，N 表示训练数据样本数（简而言之就是数个数）。 特征函数引入特征函数f(x,y) 描述输入 x 和输出y 之间的某一事实。定义为： f(x,y)=\\begin{cases} 1& \\text{x与y满足某一事实}\\\\ 0& \\text{否则} \\end{cases}即当输入 x 和输出y 满足这一事实时取值为1，否则为0。","link":"/2023/03/02/%E9%80%BB%E8%BE%91%E6%96%AF%E8%B0%9B%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/"},{"title":"朴素贝叶斯法","text":"朴素贝叶斯法 基于贝叶斯定理和特征条件独立假设的分类方法 本质上是一种生成方法，即重点在生成联合概率分布，使用后验概率最大化对实例进行分类 贝叶斯公式和全概率公式 概率估计方法有极大似然估计法和贝叶斯估计等 朴素贝叶斯法的学习与分类学习方法 通过训练集学习联合概率分布P(X,Y) 学习先验概率分布 P(Y=c_i), i=1,2,\\dots,K 学习条件概率分布 P(X=x|Y=c_i)=P(X^{(1)}=x^{(1)},\\dots,X^{(n)}=x^{(n)}|Y=c_i) 值得注意的是，条件概率分布P(X=x|Y=c_i)的参数计算数量级是指数级的，那么由于数据集特征的个数增加，对概率分布的计算是计算上不可行的。假设x^{(j)}的取值有S_j个，j=1,2,\\dots,n时，Y的取值有K个，那么条件概率分布参数个数为K \\prod \\limits_{j=1}^nS_j 因此，朴素贝叶斯法的“朴素”（naïve）便体现在对条件概率分布做了条件独立性的假设，指的是各个特征间的取值互不影响，由此可得 P(X=x|Y=c_i)=P(X^{(1)}=x^{(1)},\\dots,X^{(n)}=x^{(n)}|Y=c_i)=\\prod \\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_i) 得到联合概率分布 P(X,Y)=P(X=x|Y=c_i)P(Y=c_i),i=1,2,\\dots,K朴素贝叶斯分类已知： 存在K类c_1,c_2,\\dots,c_K，给定一个新的实例x=(x^{(1)},x^{(2)},\\dots,x^{(n)}) 问：该实例归属于哪一类？ 后验概率 P(Y=c_i|X=x)=\\frac{P(Y=c_i)\\prod \\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_i)}{\\sum \\limits_{i=1}^KP(Y=c_i)\\prod \\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_i)} 分类 y=\\arg \\max P(Y=c_i)\\prod \\limits_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_i)后验概率最大化朴素贝叶斯法选择将实例分到后验概率最大的类中，这实际上等价于期望风险最小化。 假设损失函数为0-1损失函数： L(Y,f(X))=\\begin{cases} 1& \\text{Y!=f(X)}\\\\ 0& \\text{Y=f(X)} \\end{cases} 这时，期望风险函数定义为： R_{exp}(f)=E[L(Y,f(X))] 由于，我们知道此时的期望在贝叶斯法中时对联合概率分布P(X,Y)求的，那么还得乘上条件概率分布P(c_i|X)，再加上，总共有K个类，因此条件期望： R_{exp}(f)=E_x\\sum_{i=1}^K[L(c_i,f(X))]P(c_i|X) 为使期望风险最小化，只需对X=x逐个极小化，由此： f(x)=\\arg\\min_{y \\in Y}\\sum_{i=1}^KL(c_i,y)P(c_i|X=x)\\\\ =\\arg\\min_{y \\in Y}\\sum_{i=1}^KP(y \\neq c_i|X=x)\\\\ =\\arg\\min_{y \\in Y}\\sum_{i=1}^K(1-P(y=c_i|X=x)) 那么，可以看出，原来的期望风险最小化实际上等价于最大化后验概率，即 f(x)=\\arg\\max_{y \\in Y}\\sum_{i=1}^KP(y=c_i|X=x)朴素贝叶斯法的参数估计极大似然估计用样本（训练集）估计总体，使得经验风险最小化，这个过程也称模型训练过程 先验概率（联想数盒子中巧克力个数） P(Y=c_k)=\\frac{\\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2,\\dots,K​ 设第j个特征x^{(j)}可能取值的集合为\\{a_{j1},a_{j2},\\dots,a_{jS_j}\\} 条件概率（联想数盒子中巧克力个数）的极大似然估计是： P(X^{(j)}=a_{jl}|Y=c_K)=\\frac{\\prod \\limits_{i=1}^N I(X^{(j)}_i=a_{jl},y_i=c_k)}{\\sum \\limits_{i=1}^N I(y_i=c_k)},I=1,2,\\dots,S_j j=1,2,\\dots,n; l=1,2,\\dots,S_j; k=1,2,\\dots,K​ 式子中，x_i^{(j)}是指第i个样本的第j个特征；a_{il}是第j个特征的第l个可能取值；I是指示函数 极大似然估计的原理接下来，复习以下极大似然估计的原理，其本质是概率最大化 原理：使得似然（Likelihood）函数（即联合密度函数）达到最大的参数值 假设X的密度函数为f(X,\\beta)，这里的 \\beta 即是参数，如果简单随机样本X_1,X_2,\\dots,X_N相互独立，则其联合密度函数为： L(x_1,x_2,\\dots,x_N;\\beta)=\\prod \\limits_{i=1}^Nf(x_i,\\beta) 当(X_1,X_2,\\dots,X_N)取固定值(x_1,x_2,\\dots,x_N)时L(x_1,x_2,\\dots,x_N)是关于 \\beta 的函数，即样本的似然函数 \\beta 的极大似然估计 \\hat{\\beta}，即是指 \\hat\\beta=\\arg \\max_{\\beta \\in \\Theta}L(x_1,x_2,\\dots,x_N;\\beta) 记似然函数L(\\beta)=L(x_1,\\dots,x_N;\\beta) 实现方法： 遍历法：遍历参数空间，按极大似然估计的原则求解参数 求解析解 : 直接通过似然函数 L(\\beta) 求解，求得 L(\\beta) 的极大值点 迭代法 具体可参考视频链接讲解：朴素贝叶斯法：极大似然估计之实现篇 by 简博士 朴素贝叶斯法的学习与分类算法 实际上就是上述过程的结合，具体如《统计学习方法（第2版）》中相应算法所示： 贝叶斯估计用极大似然估计可能会出现所要估计的概率值为0的情况。这时会影响到后验概率的计算结果，使分类产生偏差。解决这一问题的方法是采用贝叶斯估计。具体地，先验概率分布的贝叶斯估计是： P_\\lambda(Y=c_k)=\\frac{\\sum_{i=1}{N}I(y_i=c_k)+\\lambda}{N+K\\lambda}条件概率的贝叶斯估计是： P_\\lambda(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=c_k) +\\lambda}{\\sum_{i=1}^N I(y_i=c_k)+S_j\\lambda}如上式所示，条件概率的贝叶斯估计只是在原极大似然估计式的分子分母分别加上了分量，在随机变量的各个取值的频数上赋予一个整数\\lambda > 0。 注：\\lambda=0时，等价于极大似然估计，\\lambda=1时，称为拉普拉斯平滑（Laplacian smoothing）","link":"/2023/02/24/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/"},{"title":"Pytorch_DeepLizard教程10-13：PyTorch中的tensor操作","text":"此笔记引用自文章知乎链接并修改 学习笔记教程10-13集讲了在PyTorch中对tensor的几种操作。 在Pytorch中对tensor的操作主要有以下四种类型： Reshaping operations Element-wise operations Reduction operations Access operations - Episode 10 of 33这一集主要讲的是reshaping。 （1）Reshape 操作先创建一个具有浮点数数据类型的3x4的二阶张量方便举例： 123456&gt; import torch&gt; t = torch.tensor([ [1,1,1,1], [2,2,2,2], [3,3,3,3]], dtype=torch.float32) 张量 t 的 shape 或者 size 为： 12345&gt; t.size()torch.Size([3, 4])&gt; t.shapetorch.Size([3, 4]) 前面在《教程5-7：tensor基本概念》中介绍过要注意reshape前后元素总量不变，而查看一个张量中的元素总量可以用以下两种方法： ① 计算各个 tensor 各个 axis 上的长度乘积： 12&gt; torch.tensor(t.shape).prod()tensor(12) ② 或者直接用 numel() 函数（number of elements的缩写）： 12&gt; t.numel()12 二阶张量 t 中一共有12个元素，那么对t进行 reshape 可能的结果是将 3x4 改成 1x12、2x6、3x4、4x3、6x2、12x1，当然也可以用 t.reshape(2,2,3) 这样的方式 reshape 为 2x2x3 的三阶张量。 另外PyTorch中还有一个 view() 函数和 reshape() 具有相同的作用。 （2）Flatten 操作在 reshape 的各种操作中，最常用的一种操作叫做 flatten 操作，就是把一个n阶的张量变形为只有一行的一阶张量，即变为 array 类型。这个操作通常在CNN中由卷积层传递到全连接层的时候发生。上一篇说过，卷积层的输出是很多的 feature map，每一个 feature map 都是一个二阶张量，但是全连接层的输入只能是数组，因此需要把卷积层输出的高阶张量压平变为一维数组，才能输入全连接层。 这里要说明的是，对二阶张量进行 flatten 的操作，是第一行数据不动，将第二行数据拼接至第一行数据末尾，再将第三行数据拼接到第二行数据的末尾，以此类推。 比如采用 reshape() 函数对张量 t 进行变换： 1234&gt; print(t.reshape([1,12]))&gt; print(t.reshape([1,12]).shape)tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])torch.Size([1, 12]) 这里要注意的是，输出的张量有两层中括号，而且 shape 为“[1, 12]”，表明其在计算机中存储的形式仍然为二阶张量，只不过有一个 axis 的长度为 1 而已。要进一步将其转为一阶张量，需要用到 squeeze() 函数： 1234&gt; print(t.reshape([1,12]).squeeze())&gt; print(t.reshape([1,12]).squeeze().shape)tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])torch.Size([12]) 这时注意到输出结果变为了一阶张量，因此 squeeze() 函数的作用就是去掉高阶张量中所有长度为 1 的 axes，使高阶张量降低阶数为低阶张量。 类似的还有 unsqueeze() 函数，为张量添加一个长度为1的 axis，相当于 squeeze() 的逆操作： 1234&gt; print(t.reshape([1,12]).squeeze().unsqueeze(dim=0))&gt; print(t.reshape([1,12]).squeeze().unsqueeze(dim=0).shape)tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])torch.Size([1, 12]) 可以定义一个 flatten() 函数包含上述 reshape 和 squeeze 两步： 1234def flatten(t): t = t.reshape(1, -1) t = t.squeeze() return t 这里在 reshape() 函数中的第二个参数填写了“-1”，是让计算机自己根据张量中的元素数量决定一维数组的长度。可以验证： 12&gt; flatten(t)tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]) （3）Concatenating 操作（级联）视频最后讲了一下张量的拼接，称为 concatenating，对应的函数是 cat()。 例如现在有两个二阶张量： 12345678&gt; t1 = torch.tensor([ [1,2], [3,4]])&gt; t2 = torch.tensor([ [5,6], [7,8]]) 使用 cat() 函数同时指定拼接的方向（维数），可以得到一个拼接后的新张量。 沿第一个维度（行）拼接为4行2列： 12345&gt; torch.cat((t1, t2), dim=0)tensor([[1, 2], [3, 4], [5, 6], [7, 8]]) 沿第二个维度（列）拼接为2行4列： 123&gt; torch.cat((t1, t2), dim=1)tensor([[1, 2, 5, 6], [3, 4, 7, 8]]) - Episode 11 of 33这一集就讲了一个事情——如何只对一个高阶张量的部分维度进行 flatten 操作。 上一集只讲了对一张图片的二阶张量进行展平，但前面的章节说过，一次输入CNN的是不是一张而是一批图像，输入的是一个具有 [B, C, H, W] 形状的四阶张量。如果直接对这个张量进行展平，会把所有图像的所有颜色通道（或者 feature map）全都混在一起。而我们希望的是仅仅对一个 feature map 进行展平，而不把不同的图片、不同的颜色通道和不同的 feature map 都混合在一起。 所以我们要做的其实只是把 [B, C, H, W] 这个四阶张量的 H 和 W 两个维度展平。 为了方便讲解，举了一个 3x1x4x4 的四阶张量作为例子（灰度图片只有一个颜色通道）。 首先创建3个 4x4 的二阶张量代表3张 4x4 的图片，数字 i 只属于第 i 张图片： 1234567891011121314151617181920t1 = torch.tensor([ [1,1,1,1], [1,1,1,1], [1,1,1,1], [1,1,1,1]])t2 = torch.tensor([ [2,2,2,2], [2,2,2,2], [2,2,2,2], [2,2,2,2]])t3 = torch.tensor([ [3,3,3,3], [3,3,3,3], [3,3,3,3], [3,3,3,3]]) 为了将这三个二阶张量堆栈起来组成一个 batch，可以用 torch.stack() 函数： 1234&gt; t = torch.stack((t1, t2, t3))&gt; t.shapetorch.Size([3, 4, 4]) 此时还缺一个中间的颜色通道维度，采用 reshape() 函数来添加： 12&gt; t = t.reshape(3,1,4,4)&gt; t 最后创建的四阶张量 t 为（注释标明了每一个维度的含义）： 12345678910111213141516171819202122232425262728tensor(# Batch[ # Channel [ # Height [ # Width [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1] ] ], [ [ [2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2], [2, 2, 2, 2] ] ], [ [ [3, 3, 3, 3], [3, 3, 3, 3], [3, 3, 3, 3], [3, 3, 3, 3] ] ]]) 因为灰度图片只有一个颜色通道，所以要把 [B, C, H, W] 中从第二个维度起的后三个维度 C、H、W 都展平，这里采用PyTorch自带的 torch.flatten() 添加一个 start_dim=1 的参数即可（上一集中为了讲解方便自己定义了 flatten() 函数，但PyTorch中其实自带了功能更强的 torch.flatten() 函数）： 1234567891011&gt; t.flatten(start_dim=1).shapetorch.Size([3, 16])&gt; t.flatten(start_dim=1)tensor([ [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]]) 可以看到输出结果保留了 batch 的维度，而把后面的维度都展平了，这样同一 batch 中的不同图片数据就不会混在一起了。 对于有三个颜色通道的 RGB 图片，不同的颜色通道（或者 feature map）也不应该混淆在一起，这时候把 torch.flatten() 函数的参数改为 start_dim=2 即可。 - Episode 12 of 33这一集讲的是张量的 element-wise operations（按元素逐项操作）。 An element-wise\\ operation operates on corresponding elements between tensors. 而所谓的 corresponding elements 是指在张量中的位置相同，就是在各个 axis 上的索引编号相同的元素。 举例来说，有两个tensor： 123456789&gt; t1 = torch.tensor([ [1,2], [3,4]], dtype=torch.float32)&gt; t2 = torch.tensor([ [9,8], [7,6]], dtype=torch.float32) 那么 1 和 9 就是 corresponding elements，因为索引的 indexes 是一样的： 123456&gt; t1[0][0]tensor(1.)&gt; t2[0][0]tensor(9.) 所以通过这一点强调的是，所有的 element-wise operations 都只能在具有相同 shape 的张量之间进行（不然元素对应不上啊）。 这里介绍的 element-wise operations 分为三类，一类是 arithmetic operations（算术操作），一类是 comparison operations（比较操作），还有一类是 function operations（函数操作，其实也可以归到算术操作里）。 （1）Arithmetic Operations算数操作说的自然就是加减乘除之类的一些运算了呗，比如： 123456789101112131415&gt; t1 + t2tensor([[10., 10.], [10., 10.]])&gt; t1 - t2tensor([[-8., -6.], [-4., -2.]])&gt; t1 * t2tensor([[ 9., 16.], [21., 24.]])&gt; t1 / t2tensor([[0.1111, 0.2500], [0.4286, 0.6667]]) 所以可以看到，PyTorch中张量间的加减乘除四则运算都是 element-wise 的，相当于 matlab 中加“.”前缀的运算符。 不过和 matlab 一样的是，PyTorch中张量和数的运算也是 element-wise 的，如下： 123456789101112131415&gt; print(t1 + 2)tensor([[3., 4.], [5., 6.]])&gt; print(t1 - 2)tensor([[-1., 0.], [ 1., 2.]])&gt; print(t1 * 2)tensor([[2., 4.], [6., 8.]])&gt; print(t1 / 2)tensor([[0.5000, 1.0000], [1.5000, 2.0000]]) 也可以用函数命令来运算，结果是一样的： 1234567&gt; print(t1.add(2))&gt; print(t1.sub(2))&gt; print(t1.mul(2))&gt; print(t1.div(2)) 但 t1 是一个 2x2 的二阶张量，而数 2 是一个零阶的张量，没有 shape，和之前说的 element-wise operations 需要在 shape 一样的张量之间进行不相符。这是因为接下来要介绍的PyTorch中的一个重要概念： （2）Broadcasting TensorsBroadcasting 广播 Broadcasting tensor 就是把两个 shape 不一样的 tensor 匹配成 shape 一样的 sensor 的过程，匹配的方法是将 shape 小的 tensor 进行复制和拼接，使小的 tensor 变成和大的 tensor 具有一样的 shape。这是PyTorch在 shape 不同的张量之间进行操作之前会进行的一个步骤，即： 123&gt; np.broadcast_to(2, t1.shape)array([[2, 2], [2, 2]]) 对于不同 shape 的 tensor 之间的操作也是如此： 12345678910&gt; t1 = torch.tensor([ [1,1], [1,1]], dtype=torch.float32)&gt; t1.shapetorch.Size([2, 2])&gt; t2 = torch.tensor([2,4], dtype=torch.float32)&gt; t2.shapetorch.Size([2]) t1 与 t2 相加时，PyTorch对 t2 进行了 broadcast 以匹配 t 的 shape： 1234567&gt; np.broadcast_to(t2.numpy(), t1.shape)array([[2., 4.], [2., 4.]], dtype=float32)&gt; t1 + t2tensor([[3., 5.], [3., 5.]]) 在深度学习中，broadcasting 主要用在数据预处理（data preprocessing）和正则化（normalization routines）之中。 熟练掌握 broadcasting 的优势在于，在编程之中可以省略原本需要很多行代码进行手工张量增广和形状匹配的工作，而让PyTorch默认去完成，使代码更加简洁高效。 这篇教程中给出了有关 broadcasting 的更详细介绍： Broadcasting Explained - deeplizarddeeplizard.com/learn/video/6_33ulFDuCg （3）Comparison Operations比较操作就是两个张量之间按元素进行比较了（比如比大小），返回值的数据类型是 torch.bool（布尔运算值），即 True 和 False。 比如有这么一个 tensor： 12345&gt; t = torch.tensor([ [0,5,0], [6,0,7], [0,8,0]], dtype=torch.float32) 各项比较操作分别为： 123456789101112131415161718192021222324&gt; t.eq(0) # equal to 判断是否相等tensor([[True, False, True], [False, True, False], [True, False, True]])&gt; t.ge(0) # greater than or equal to 大于等于tensor([[True, True, True], [True, True, True], [True, True, True]])&gt; t.gt(0) # greater than 大于tensor([[False, True, False], [True, False, True], [False, True, False]])&gt; t.lt(0) # less than 小于tensor([[False, False, False], [False, False, False], [False, False, False]])&gt; t.le(7) # less than or equal to 小于等于tensor([[True, True, True], [True, True, True], [True, False, True]]) 可以发现PyTorch也是先对 number 类型的零维张量进行了 broadcasting 之后才进行比较操作。 （4）Element-wise Operations using Functions下面这些常用函数操作也都是 element-wise 的： 12345678910111213141516171819&gt; t.abs() # absolute 取绝对值tensor([[0., 5., 0.], [6., 0., 7.], [0., 8., 0.]])&gt; t.sqrt() # squrt root 求平方根tensor([[0.0000, 2.2361, 0.0000], [2.4495, 0.0000, 2.6458], [0.0000, 2.8284, 0.0000]])&gt; t.neg() # negative 取负tensor([[-0., -5., -0.], [-6., -0., -7.], [-0., -8., -0.]])&gt; t.neg().abs() # 先取负再取绝对值tensor([[0., 5., 0.], [6., 0., 7.], [0., 8., 0.]]) 最后补充一下，以下三个术语说的是同一个意思： Element-wise Component-wise Point-wise Just keep this in mind if you encounter any of these terms in the wild. 最后这个“in the wild”真是太逗了，你细品~ - Episode 13 of 33这一集讲PyTorch中的最后两种张量操作： Reduction operations Access operations （1）Reduction Operations A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor. 前面提过，张量是深度学习中使用的数据结构，我们使用张量不仅是用来进行数据的存储，更重要的是进行数据的访问和管理。 Reshaping operations 让我们能够重新排列元素；element-wise operations 让我们能够在两个张量的元素之间执行运算，reduction operations 让我们能够对单个张量内的元素执行运算。 以一个 3x3 二阶张量举例： 12345&gt; t = torch.tensor([ [0,1,0], [2,0,2], [0,3,0]], dtype=torch.float32) 所谓的 reduction operations 其实特别简单： 1234567891011&gt; t.sum() # 求元素之和tensor(8.)&gt; t.prod() # 求元素乘积tensor(0.)&gt; t.mean() # 求元素平均值tensor(.8889)&gt; t.std() # 求元素标准差tensor(1.1667) 之所以称之为 reduction operations 是因为他们的输出从原来输入的二阶张量变为了零阶张量，张量中总的元素数量减少了： 12345678&gt; t.numel()9&gt; t.sum().numel()1&gt; t.sum().numel() &lt; t.numel()True 上述的操作在不指定 axis 时都是对张量中所有元素进行操作的，也可以指定 reduction operations 作用的 axis，得到的输出就会是一个不是只包含一个元素的张量了。 以一个 3x4（3行4列）的张量为例： 1234567891011121314151617&gt; t = torch.tensor([ [1,1,1,1], [2,2,2,2], [3,3,3,3]], dtype=torch.float32)&gt; t.sum(dim=0)tensor([6., 6., 6., 6.])&gt; t.sum(dim=0).shapetorch.Size([4])&gt; t.sum(dim=1)tensor([ 4., 8., 12.])&gt; t.sum(dim=1).shapetorch.Size([3]) 可以看到用 dim = 0 指定沿第一个 axis 操作，也就是按行数索引的方向相加，其实就是求每列的和，所以输出有张量有4个元素（4列）；与之相反 dim = 1 指定沿列索引的方向操作，就是求每行的和，所以输出有张量有3个元素（3行）。 这里要说明的是PyTorch中进行 reduction operations 之后会自动把输出的张量进行 squeeze() 操作，因此可以看到输出的直接就是一阶张量（都是一行，不区分行列），不再保留原来的行列信息，因此操作的时候需要自己想清楚。（可能是 matlab 惯的臭毛病吧） （2）Argmax tensor reduction operation然后这里要讲一个特别的 reduction operation 叫做 Argmax（其实数学上应该加个空格吧 Arg max）。 数学上，我们用 max y(x) 来表示求函数 y(x) 的最大值，而 Arg max y(x) 就是求当 y 取得最大值的时候，对应的 x 的取值是多少。反映在张量之中，就是求张量中某个最大的元素的位置上（就是对应的索引序号）。 比如这个张量，最大值 5 在第 3 行第 4 列： 1234567891011t = torch.tensor([ [1,0,0,2], [0,3,3,0], [4,0,0,5]], dtype=torch.float32)&gt; t.max()tensor(5.)&gt; t.argmax()tensor(11) 可以看到 argmax() 函数输出的只有一个零阶张量，而非刚才说的第 3 行第 4 列两个值，这是因为PyTorch先对高阶张量进行了 flatten() 操作，然后才去比较大小： 12345&gt; t.flatten()tensor([1., 0., 0., 2., 0., 3., 3., 0., 4., 0., 0., 5.])&gt; t.flatten().argmax()tensor(11) 同样 argmax() 函数也可以按指定的 axis 方向进行操作，比如： 1234567891011&gt; t.max(dim=0)(tensor([4., 3., 3., 5.]), tensor([2, 1, 1, 2]))&gt; t.argmax(dim=0)tensor([2, 1, 1, 2])&gt; t.max(dim=1)(tensor([2., 3., 5.]), tensor([3, 1, 3]))&gt; t.argmax(dim=1)tensor([3, 1, 3]) 这里 max() 函数会返回两个张量，第一个是按指定的 axis 方向找出的最大元素值，第二个是这些最大元素在指定的 axis 方向上的索引号。 argmax() 在神经网络中的用处在于：图像分类神经网络的输出层是一个包含 k 个元素的一阶张量（其中 k 是图片的类别数），张量中的元素是对应各个类别的 prediction values（翻译成置信概率？），所有的 prediction values 之和为 1。我们会取置信概率最大的那一种作为最终预测的类别（比如 a 类预测值 0.05，b 类 0.95，那就认为是 b 类）。这样就需要使用 argmax() 函数来找出最大的 prediction value 对应的是哪一类。 （3）Access Operations最后就是讲一下如何访问和提取张量中的数据了。 比如求张量元素的平均值： 1234567891011&gt; t = torch.tensor([ [1,2,3], [4,5,6], [7,8,9]], dtype=torch.float32)&gt; t.mean()tensor(5.)&gt; type(t.mean())torch.Tensor 可以看到输出的数据类型仍然是一个张量（虽然是零阶张量），可以通过 item() 函数提取出零阶张量中的数据： 12345&gt; t.mean().item()5.0&gt; type(t.mean().item())float 对于输出为含有多个元素的张量时，可以采用 tolist() 将其转为 Python list： 12&gt; t.mean(dim=0).tolist()[4.0, 5.0, 6.0] 或者用 numpy() 将其装换为 numpy 数组： 12&gt; t.mean(dim=0).numpy()array([4., 5., 6.], dtype=float32) 关于 numpy 中高级索引的资料： Advanced indexing and slicingdocs.scipy.org/doc/numpy/reference/arrays.indexing.html 那么至此deeplizard教程的 Part I，就是关于 PyTorch 和 tensor 的介绍就结束了。接下来会进入 Part II 去实际搭建一个CNN来进行图像分类。下一集将从数据集的介绍开始。","link":"/2021/11/01/Pytorch_DeepLizard%E6%95%99%E7%A8%8B10-13%EF%BC%9APyTorch%E4%B8%AD%E7%9A%84tensor%E6%93%8D%E4%BD%9C/"},{"title":"Pytorch_DeepLizard教程14-16：训练数据与预处理","text":"此笔记引用自文章知乎链接并修改 学习笔记这里开始进入整个教程的第二部分，分为四个 section，分别为： Prepare the data Build the model Train the model Analyze the model’s results 这篇笔记是 section 1 的3集的，这一部分主要讲数据集和数据的预处理。 - Episode 14 of 33我的妈呀，杰克马居然都上镜了： If you want to make machine smart, the machine must drink data. 这一集主要讲了两个内容：在人工智能中数据集扮演了什么样的角色，以及本教程采用的 Fashion-MNIST数据集的由来。 （1）人工智能中数据集扮演了什么样的角色简单概括一下，视频大概讲了这么几个事情吧： 统计学习尤其是深度学习方法的出现，使得传统的软件开发范式发生了转变。过去写软件是直接写代码来实现功能，是1.0时代；而2.0时代不再用代码本身来规定行为，而是用数据来约束行为，用代码来构建框架（比如神经网络），来让机器自己学习如何行为。 所以对使用的数据集有更好的理解，有助于模型的训练。 对数据及的关注点：- 谁创建了数据集？- 数据集是如何创建的？- 使用了哪些转换？- 数据集的目的/用途？- 可能导致什么意外结果？- 数据集是否有偏差？- 数据集存在道德问题吗？ 另一方面就是，人们对人工智能历来有一种恐惧的观点，因为觉得是机器来代替人做决定。但实际上机器是通过数据来学习的，所以其实真正要规范的是收集和使用数据的人和机构，要确保他们不用数据干坏事。在一些细分领域几大互联网巨头以及开始领跑，但是在更多的市场及公共领域，比如医疗、教育、金融调控、信用，还没有 clear winner。 （2）Fashion-MNIST 数据集(“MNIST”的读法是“M-Nist”) 介绍 Fashion-MNIST 之前，首先要介绍 MNIST 数据集。 MNIST MNIST，Modified National Institute of Standards and Technology database，前面加了“modified ”是因为这个数据集已经是在原始的 NIST 数据集上修改过的版本。 简单来说 MNIST 就是一个包含了 0-9 十个数字（十个类别）的手写图片数据集，都是灰度图片，每张图片 28x28 像素，每个类别 7000 张图片，一共 70000 张。并且划分了 60000 张图片作为训练集，10000 张图片作为测试集。 MNIST MNIST 在图像分类领域非常流行，主要有两个原因：一是这个数据集特别简单，适合新手上手；二是学术圈为了比较各自的算法优劣，会在相同的数据集上训练算法，就是 MNIST。（但是现在好像用 ImageNet 更多了） MNIST 也有它的问题就是太简单了（图像分类领域的“hello world”），所以有一帮人就开发了 Fashion-MNIST 想要来取代 MNIST。 Fashion-MNIST Fashion-MNIST 是一个德国的时装公司 Zalando 下面的研究院 Zalando Research 开发的，它用10类服装的图片取代了十类手写数字图片。十个类别分别是： Fashion-MNIST Fashion-MNIST 的设计理念就是作为 MNIST 的直接取代（direct dropin replacement），就是说以前使用 MNIST 的模型，除了数据集的链接（URL），其他什么都不用改。但替换之后的图像分类有了更高的难度。所以 Fashion-MNIST 和 MNIST 一样，都是灰度图片，28x28 像素，每类 7000 张，一共 70000 张，其中训练集 60000 张，测试集 10000 张。 数据集链接：https://github.com/zalandoresearch/fashion-mnist Fashion-MNIST 是直接从 Zalando 网站上的商品图片提取出来制作的，包括以下7步：① 转换为PNG；② 裁剪；③ 长边缩放为28像素；④ 锐化；⑤ 补足空白；⑥ 取负片；⑦ 取灰度。 关于 Fashion-MNIST 的更多细节可以参阅官网文档： Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithmsarxiv.org/pdf/1708.07747.pdf - Episode 15 of 33这一集讲数据的预处理，和如何通过 PyTorch 的 torchvision 包获取 Fashion-MNIST 数据集。 torchvision 包是 PyTorch 的 computer vision package for machine learning，但是目前 torchvision 和 torch 还是分开下载安装的。 一般而言，对一个数据集的预处理流程为 ETL，即包含 extract、transform、load 三个步骤。对于本教程涉及的 Fashion-MNIST 数据集，分别如下： Extract（获取） - Get the Fashion-MNIST image data from the data source. Transform（转换） - Transform image data into a desirable PyTorch tensor format. Load（载入） - Put data into a suitable structure to make it easily accessible. 关于 ETL 流程的更多解释：ETL - 维基百科 以及一个关于数据科学的补充介绍：General Data Science Pipeline 完成 ETL 流程之后，就可以开始构建和训练深度学习模型。PyTorch具有一些内置的软件包和类（class），使 ETL 过程非常容易实现。 然后……这里的车速突然就加快了，我看了好几遍才跟上，慢慢来： （1）包的导入首先需要把所有需要的PyTorch包导入： 1234567import torchimport torch.nn as nnimport torch.optim as optimimport torch.nn.functional as Fimport torchvisionimport torchvision.transforms as transforms 对各个包的描述如下： torch - The top-level PyTorch package and tensor library. torch.nn - A subpackage that contains modules and extensible classes for building neural networks. torch.optim - A subpackage that contains standard optimization operations like SGD and Adam. torch.nn.functional - A functional interface that contains typical operations used for building neural networks like loss functions and convolutions. torchvision - A package that provides access to popular datasets, model architectures, and image transformations for computer vision. torchvision.transforms - An interface that contains common transforms for image processing. 以及一些其他的常用Python科学计算包： 12345678910import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.metrics import confusion_matrix# from plotcm import plot_confusion_matriximport pdbtorch.set_printoptions(linewidth=120) 上面的pdb是Python debugger；被注掉的一行导入的是一个用于绘制混淆矩阵的本地文件（将来会用到）；最后一行用来设置PyTorch打印语句的选项（每行宽度）。 （2）在Pytorch中进行 ETL对于 ETL 流程，PyTorch 提供了两个类（class）： 关于 Python 中的类是什么，我是看的这个介绍视频，还比较清楚： IT教头王进带你入门Python第四讲：快速理解Python中的类_哔哩哔哩 (゜-゜)つロ 干杯~-bilibiliwww.bilibili.com/video/av70535119 我的理解就是类是比函数更高一个层级的模块化工具，它实现了函数功能和数据的集成。 函数比较好理解，函数内部是编写好的功能，但函数本身不存储数据，调用函数时给函数传递一个输入，它经过计算后返回一个输出。 而类比函数更高一个层级，它内部可以以类的属性存储数据，也可以在类的内部定义和调用函数对输入和存储的数据进行计算。类就是一个模块的模板，在主程序中调用时将类实例化（同一个模板可以有多个实例来储存和处理不同的数据？），并自动执行 init() 函数中的代码，对类中存储的数据还可以直接通过访问类的属性来提取。具体可以看上面视频。 （这里是我猜想，没有去验证：就像之前通过 .shape 直接输出张量的 shape，其实就是创建张量时在 init() 函数中自动计算了 shape 并存储在 .shape 属性中，然后只是通过访问 .shape 属性将数据导了出来。） 大佬不用看的废话说完，接着说上面两个类。 torch.utils.data.Dataset 是一个 abstract class，所谓 abstract class： An abstract class is a Python class that has methods we must implement, so we can create a custom dataset by creating a subclass that extends the functionality of the Dataset class. 我的理解就是 abstract class 是一个预定义好的包含必要函数的类，如果你想有更多函数的话，可以利用它创建一个子类，并在子类中根据自己的需求来丰富它的函数（这样你不会改动最原来的这个相当于模板的 abstract class？）。 所以为了使用 PyTorch 创建自定义的数据集，我们通过创建子类并继承 Dataset 中的函数，来实现 Dataset 的扩展，然后就可以传递给 DataLoader 对象。 一般来说方法如下图（可以不看）： len() 和 getitm() 是其中两个必要的函数，前者的功能是计算数据集的长度，后者的功能是在数据集中按指定的索引编号将数据取出。 但是！但是！但是！↓ ↓ ↓ （3）利用torchvison包获取和处理数据集（E+T）torchvision 包中提供了以下面向机器视觉的深度学习的有关资源： Datasets (like MNIST and Fashion-MNIST) Models (like VGG16) Transforms Utils 所以在 PyTorch 中我们可以利用 torchvision 包来获取这些资源。 前面说过 Fashion-MNIST 的设计理念是直接 dropin 替换掉 MNIST，只要在 PyTorch 中把数据集的链接（URL）改了就可以。 视频中去找了 torchvision 包中 Fashion-MNIST 的源码来验证（如果你用的是 Anaconda 安装的 PyTorch 并且使用了默认路径的话，源码地址在这：C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py），然后发现里面的 Fashion-MNIST 类确实是 extend from MNIST 类，而且确实是只改了 url（我这里是0.5.0版本的 torchvision，下面还多了每个类别的描述）： 于是，如果需要利用 torchvision 获取并创建 Fashion-MNIST 数据集的一份实例（instance），这个过程中同时完成的数据集的获取（E）和转化（T），代码如下： 12345678train_set = torchvision.datasets.FashionMNIST( root='./data' ,train=True ,download=True ,transform=transforms.Compose([ transforms.ToTensor() ])) 返回的状态应该是这样的： 上面各个参数解释如下： 因为希望将图片数据集转换为张量，所以在 transform 中使用了 transforms.ToTensor()；将此数据集命名为train_set，是因为我们希望将其作为训练数据；另外数据集仅会被下载一次，程序下载之前会检查本地有没有。 之后将获取的 train_set 打包给 DataLoader，使得数据集可以通过 DataLoader 方便的访问和加载（L）： 1train_loader = torch.utils.data.DataLoader(train_set) 至此已经完成了数据集的 Extract（利用url从网页下载）和 Transform（上面的transforms.ToTensor()），并且已经打包给了 DataLoader，可以通过 DataLoader 来实现 Load，比如设置 batch_size 和 shuffle（洗牌，图片的随机重排？）： 1234train_loader = torch.utils.data.DataLoader(train_set ,batch_size=1000 ,shuffle=True) 小结：利用torch.utils.data包中的 Datasets 和 DataLoaders，配合 torchvision 包，可以大大简化 ETL 工作流程。 - Episode 16 of 33最后这一集讲的是利用上面提到的Dataset和DataLoader两个PyTorch类来访问数据集，以查看其中的图片和标签。这一集需要用的的包在上一集导入包的笔记部分包括了。 在上一集中已经利用 Dataset 和 DataLoader 两个类创建了 train_set 和 train_loader 两个对象。 首先可以查看数据集中有多少个图片，使用 Python 的 len() 函数： 12&gt; len(train_set)60000 Fashion-MNIST 数据集中的训练集有 60000 张图片，与查询结果是符合的。（此处疑问：测试集是需要单独从数据源提取一次吗？） 如果希望查看所有图片的标签，只需要访问 train_set.targets 属性（适用于 PyTorch 0.2.2 及以后版本）： 12&gt; train_set.targetstensor([9, 0, 0, ..., 3, 0, 5]) 数字编号和语义标签的对应关系可以参照上面的 Fashion-MNIST 说明。 如果希望查看数据集中每一个类别有多少个标签（即多少个图片，适用于图片全部有标记的情况），可以用 PyTorch 的 bincount() 函数： 12&gt; train_set.targets.bincount()tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]) 可以看到 Fashion-MNIST 数据集中每一类都有 6000 个图片和标签对，这种每一类的样本数量相等的数据集称作 balanced dataset，反之类别之间样本数量不一致的数据集称为 unbalanced dataset。 关于 unbalanced dataset，这里有一篇论文专门讨论这个问题： A systematic study of the class imbalance problem in convolutional neural networksarxiv.org/abs/1710.05381 论文的结论简要概括为：类不平衡有损于CNN的分类性能，通过对样本数量少的类进行超采样是解决此问题的最佳办法（甚至就是简单复制以扩充样本数量）。 （1）访问和查看 train_set 中的单个数据如果只希望一次只查看单张图片，首先将 train_set 这个对象传递给 Python 内建的 iter() 函数，它会返回一个可以在其上迭代的代表数据流（stream of data）的对象，使我们可以沿数据流访问数据。 接下来再使用 Python 的内建函数 next() 来获取数据流中的下一个数据元素，如此就可以获取数据集中的一个单独数据（因此下面命名变量都是单数形式）： 1234567&gt; sample = next(iter(train_set))&gt; len(sample)2&gt; type(sample)tuple 发现获取的一个单独数据长度为 2，这是因为数据集是由图片-标签对的形式组成的，每一个 data element 中都包含两个东西，一个是存储图片数据的张量，另一个是其对应的标签。 sample 的数据类型是 tuple，tuple 是Python中的一种 sequence types，是一个可以迭代的顺序不可变的数据序列？参阅：官方文档 可以用 sequence unpacking 来将其中的图像和标签分别提取出来： 1&gt; image, label = sample 和下面这种写法是等效的： 12&gt; image = sample[0]&gt; label = sample[1] 可以进一步查看数据类型和shape： 1234567891011&gt; type(image)torch.Tensor&gt; type(label)int&gt; image.shapetorch.Size([1, 28, 28]) &gt; torch.tensor(label).shapetorch.Size([]) 因为 Fashion-MNIST 数据集是单通道的灰度图，所一张图片的 tensor shape 就是 1x28x28。把没有用的颜色通道 squeeze 掉： 12&gt; image.squeeze().shapetorch.Size([28, 28]) 接下来就可以显示出图片和标签了： 123&gt; plt.imshow(image.squeeze(), cmap=&quot;gray&quot;)&gt; torch.tensor(label)tensor(9) 标签是“9”，代表靴子，与图片是相符的。 （2）利用 DataLoader 成批访问数据这里就用到上一集末尾提到的 DataLoader 中的 batch size 了，如果不指定 batch size 则默认为 1： 1234567&gt; batch = next(iter(train_loader))&gt; len(batch)2&gt; type(batch)list list 也是一种 Python sequence types，与 tuple 的不同在于 list 是可变序列的？可能是传递给 DataLoader 之后自动转换的？具体可以看上面给出链接的官方文档。 比如一次访问 10 张图片，则需要给 DataLoader 指定 batch_size： 123&gt; display_loader = torch.utils.data.DataLoader( train_set, batch_size=10) 关于 DataLoader 中的“shuffle=True”：如果“shuffle=True”，则每次调用 next() 返回的 batch 都会不同，训练集中的第一组样本将在第一次调用 next() 时返回，这个功能默认是 False。 接下来就可以像上面一样对 display_loader 使用 iter() 和 next() 来每次查看 10 张图片： 123&gt; batch = next(iter(display_loader))&gt; print('len:', len(batch))len: 2 同样进行 sequence unpacking： 123456&gt; images, labels = batch&gt; print('types:', type(images), type(labels))&gt; print('shapes:', images.shape, labels.shape)types: &lt;class 'torch.Tensor'&gt; &lt;class 'torch.Tensor'&gt;shapes: torch.Size([10, 1, 28, 28]) torch.Size([10]) 可以发现此时返回的图像张量是 [10, 1, 28, 28] 的四阶张量，标签是一个长度为 10 的一阶张量。可以单独查看其中每一个图片和标签： 12345&gt; images[0].shapetorch.Size([1, 28, 28])&gt; labels[0]tensor(9) 要一次绘制一批图像，可以使用 torchvision.utils.make_grid() 函数创建一个可以按网格绘制图片的 grid： 12345678&gt; grid = torchvision.utils.make_grid(images, nrow=10) # nrow指定每行多少列图片&gt; plt.figure(figsize=(15,15)) # figsize=(a, b) 设置图形的大小，a为图形的宽， b为图形的高，单位为英寸&gt; plt.imshow(np.transpose(grid, (1,2,0))) # 作用与下面一句代码相同&gt; plt.imshow(grid.permute(1,2,0)) # 这一步让grid符合imshow的要求，不清楚细节&gt; print('labels:', labels)labels: tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5]) 解释：在plt.imshow在显示图片的时候输入的是（imagesize,imagesize,channels）,而def imshow(img,text,should_save=False)中，参数img的格式为（channels,imagesize,imagesize）,这两者的格式不一致，我们需要调用一次np.transpose函数，即np.transpose(npimg,(1,2,0))，将npimg的数据格式由（channels,imagesize,imagesize）转化为（imagesize,imagesize,channels）,进行格式的转换后方可进行显示。或者如上图第二种实现，原理应该是相同的。 然后还有一个更好看的（更秀的）显示方法： 1234567891011121314151617181920212223how_many_to_plot = 20train_loader = torch.utils.data.DataLoader( train_set, batch_size=1, shuffle=True)mapping = { 0:'Top', 1:'Trousers', 2:'Pullover', 3:'Dress', 4:'Coat' ,5:'Sandal', 6:'Shirt', 7:'Sneaker', 8:'Bag', 9:'Ankle Boot'}plt.figure(figsize=(50,50))for i, batch in enumerate(train_loader, start=1): image, label = batch plt.subplot(10,10,i) fig = plt.imshow(image.reshape(28,28), cmap='gray') fig.axes.get_xaxis().set_visible(False) fig.axes.get_yaxis().set_visible(False) plt.title(mapping[label.item()], fontsize=28) if (i &gt;= how_many_to_plot): breakplt.show() 这一部分结束，我感觉目前看好像图片数据的预处理主要就是把图片裁剪缩放到合适的尺寸（成熟的数据集已经做好这步了），然后把图片转用张量表示就行了。 接下来就要开始构建卷积神经网络了。","link":"/2021/11/02/Pytorch_DeepLizard%E6%95%99%E7%A8%8B14-16%EF%BC%9A%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86/"},{"title":"Pytorch_DeepLizard教程17-19：构建CNN（上）神经网络的层","text":"此笔记引用自文章知乎链接并修改 学习笔记Part II. Section 2 讲的是 Neural Networks and PyTorch Design，内容比较多一共8集，所以我拆分成了上中下三篇笔记。 - Episode 17 of 33这一集讲的是在PyTorch中利用面向对象编程的方法来构建神经网络。 关于CNN的原理和组成部分，deeplizard专门做了一个教程： Machine Learning &amp; Deep Learning Fundamentalsdeeplizard.com/learn/video/gZmobeGL0Yg 想要快速了解CNN的话，可以先看这几篇： Convolutional Neural Networks (CNNs) explained Visualizing Convolutional Filters from a CNN Zero Padding in Convolutional Neural Networks explained Max Pooling in Convolutional Neural Networks explained Learnable Parameters in a Convolutional Neural Network (CNN) explained 或者看李宏毅老师课程的笔记：https://unclestrong.github.io/DeepLearning_LHY21_Notes/Notes_html/09_CNN.html 在PyTorch中采用扩展torch.nn.ModulePyTorch类的方法来构建神经网络，因此一开始先讲了一下面向对象编程和类的概念。 （1）面向对象编程（OOP）为什么要学OOP？因为PyTorch本身就是用OOP的方法搭建起来的。学过python的可以跳过不看这一part。 上一篇已经写过类（class）是什么了，这里再补充一下对象（Object）是什么以及和 class 是什么关系。 简单来说就是 class 是 object 的一个模板，原视频里用的词是 blueprint（蓝图）。class 可以是PyTorch或者其他Python包预先定义好的，也可以是自己定义的。在调用这个模板的时候会以 class 为 blueprint 来创建一个 object，也就是说 object 是 class 的实例化（an instance of the class）。这样以同一个 class 为模板可以创建不同的 object 来存储和处理不同的数据。而面向对象编程，自然就是围绕对象来进行程序设计。 在上一篇中说过 class 是代码（实现特定功能的方法或者函数）和数据（以对象的）的模块化封装，其实应该 object 才是，class 只是对一个 object 的描述。在 object 中二者分别称为： Methods 方法(code) Attributes 属性(data) We can think about attributes as describing the characteristics of an object, and methods as the behavior of the object. 以一个蜥蜴（lizard）举例，蜥蜴就是一个对象，那他的属性可能有颜色、长度；属性的值（数据）比如颜色是绿色，长度是10cm；它包含的 methods 可能包括会走会跳，或者改变颜色（改变属性值）。对于所有的蜥蜴（类）都有这些属性和方法，但是每只具体的蜥蜴（对象）可能有不同的属性值（data），并且这些属性值是只属于（封装在）这个具体的蜥蜴（对象内部）的。 下面来定义一个 class 举例： 123456class Lizard: #class declaration def __init__(self, name): #class constructor (code) self.name = name #attribute (data) 把传递进来的name参数赋值给self.name属性 def set_name(self, name): #method declaration (code) 类内函数 self.name = name #method implementation (code) # 修改self.name属性值 第一行是声明创建了一个 class，这个 class 的名字为 Lizard，将其实例化的时候就是引用这个名字。 第二行定义的 init() 是一个特殊的 method，称作 class constructo。当这个 class 被实例化的时候（就是创建了一个 object 的时候），这个函数内的内容会自动执行。 PS：“init()”读法为：“dunder in-it”，“dunder”=“double underscore” 上面例子中的 init() 函数有两个参数，分别是 self、name。 self 是一个特殊的参数，他是一个指向当前 object 的指针，使得object 能够把属性值（数据）保存/封装在其内部。但是在调用 class 创建 object 时，并不需要向 object 传递 self 这个参数，Python会自动进行。 name 就是我们根据需要为这个 class 声明的需要传递进来的参数。 下面的 setname(self, name) 是在类内创建的专有 method。这个函数只有在调用的时候才会执行，而不是像 _init() 一样自动执行，相当于为我们提供了一个可以修改 object 中数据的一个接口/工具。 class 定义好了之后，看一下怎么创建和应用 object： 1234567&gt; lizard = Lizard('deep') # 用Lizard类创建一个对象lizard，并把'deep'传递给name参数&gt; print(lizard.name) # 由于__init__()自动执行，'deep'被赋给了lizard.name属性deep&gt; lizard.set_name('lizard') # 调用lizard内部函数set_name()，并把'lizard'传递给name&gt; print(lizard.name) # 上面执行了set_name()函数后，lizard.name的值被修改lizard 因为 self 是Python自动传递的参数，所以直接忽略掉从 self 后一个参数开始传递就好。 （2）Pytorch 的 torch.nn 包 torch.nn.Module class PyTorch的 torch.nn 包是神经网络的库，里面包含构建神经网络所需的所有典型组件。导入的时候可以： 1import torch.nn as nn 神经网络是多层的，构建神经网络所需要的主要组件就是层（layer）。 layer 主要由两部分组成： A transformation (code) —— 比如卷积层的卷积/线性层的线性变换 A collection of weights (data) —— 权重 PyTorch的 torch.nn 包中含有用于构建层的类（class）——nn.Module class，它是所有神经网络模块（包括层）的基类。 也就是说PyTorch中，构建神经网络的层都是以扩展 nn.Module 类，并继承 nn.Module 类中所有PyTorch的内置功能来实现的。神经网络本身也是以 扩展 nn.Module 类来构建的，因为神经网络其实就相当于层的复合函数，可以看做一个大的层。 关于类的继承、重写和扩展： Python类的继承（简单继承、重写、拓展） - coco小锦鲤 - 博客园www.cnblogs.com/Christalccc123/p/12060594.html Python3 面向对象 | 菜鸟教程www.runoob.com/python3/python3-class.html 被继承和拓展的 nn.Module 类被称为父类，继承和扩展 nn.Module 类后创建的新类称为子类。 nn.Module 中的 forward() method 张量进入神经网络后，在神经网络中前向传递（forward pass）来输出预测结果到输出层。前向传递是通过 nn.Module 中的 forward() 函数实现的。因此在构建层和神经网络时，必须提供该 forward() 函数的实现，forward() 函数是实际的 transformation。 这里我的理解是 layer 中包含了该层神经元的权重数据（data）和变换函数（code），但是 layer 是在类的 init() 构造函数中定义的属性，不自动执行，是在类下的 forward() 函数中来定义如何使用 layer 中的 code 来对输入进行 tramsform。前向传递也是通过调用 .forward() 函数来完成。（但是不清楚线性层的激活函数在哪里定义？） PyTorch的 nn.functional 包 PyTorch的 torch.nn.functional 中包中提供了很多必要的操作，可用于构建神经网络的层。可以直接调用其中的函数来进行 nn.Module 子类中 forward() 函数的实现。 这个会在后面的教程中介绍。 （3）在PyTorch中构建神经网络简单来说，构建神经网络分为三个步骤： Extend the nn.Module base class. Define layers as class attributes. Implement the forward() method. 具体来说： 通过扩展 nn.Module 基类的方法，创建一个神经网络子类。 在类的构造函数（ init() 函数 ）中，使用 torch.nn 包中预构建的层，将需要的网络层定义为类的属性。 使用层的属性以及 nn.functional 包中的操作，来定义网络的前向传递（ forward() ）。 Step 1. 扩展PyTorch的 nn.Module 类 为了讲解方便，创建一个简单的类来表示神经网络的框架： 1234567class Network: # 声明一个名为Network的类 def __init__(self): self.layer = None # 定义了类的属性.layer，是一个空的layer def forward(self, t): # 定义类的方法forward()，前向传递函数，有一个输入参数t t = self.layer(t) # 使t经过.layer()的变换 return t # 把变换后的t返回给调用者 上面只是一个构建神经网络的框架示例，在构造函数中创建了一个单层（空的），并提供了 forward() 函数的实现（也是空的）。 为了继承和扩展 nn.Module 类，需要对上面的代码做两点改动： 12345678class Network(nn.Module): # line 1 def __init__(self): super().__init__() # line 3 self.layer = None def forward(self, t): t = self.layer(t) return t 第一行在声明 Network 类的时候指定从 nn.Module 类继承。 Network 作为子类，如果不添加属性或方法，就会完全继承父类 nn.Module 中的属性或方法。如果在 Network 定义了和父类名称一样的属性或方法（称作重写），则子类在实例化的时候会忽略父类中的内容而根据子类重写的内容实例化。如果子类中定义了父类中不包含的属性或方法，称作对父类的扩展。 因此第三行添加了“super().init()”，通过这行代码指定在实例化的时候从父类 nn.Module 的构造函数继承。否则的话，由于 Network 子类重写了构造函数，实例化的时候会直接忽略父类的构造函数。之后子类中添加的“self.layer = None”就是对父类的扩展了。 如此就通过扩展类的方法，将一个我们自己写的简单神经网络类，转换成了一个PyTorch神经网络类，并且具有 nn.Module 类的所有功能。 Step 2. 将需要的神经网络的层定义为类属性 上面的实例中，构建的 Network 类只有一个空的层作为属性，现在我们要利用PyTorch的 torch.nn 库中预先构建好的一些真实的层来替换掉它。对于卷积神经网络，我们将使用卷积层（conv）和全连接层（fc，也称作 linear layer 或者 dense layer，在 nn.Linear 内）两种类型的层： 12345678910111213class Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, t): # implement the forward pass return t 至此，我们有了一个名为 Network 的Python类，该类是由扩展PyTorch的 nn.Module 类构建的。在 Network 类内部，我们有5个定义为类的属性的层，其中包含2个卷积层 self.conv1 和 self.conv2 ，以及3个线性层 self.fc1，self.fc2 和 self.out（输出层）。 （好像 forward(self, t) 函数中内容被暂时忽略了？后面会讲？） - Episode 18 of 33这一集主要讲定义 layer 的参数（parameter）都有哪些，以及参数值（argument）是如何确定的。 （1）layer 类一开始先讲了一下，之前在定义 layer 的时候其实也是用扩展PyTorch类的办法实现的（面向对象.jpg），视频里去找了 torch.nn.Linear 类的源码： 然后讲了一下每一个 layer 的 weight 是保存在 layer 的对象内的属性，这个属性会随着网络的训练进行更新（权值更新）。并且在构建的神经网络对象中，由于实例化了 layer 的类，也会在神经网络对象中自动 track 到 layer 中的属性，并继承它的功能，能够在训练中更新。 （2）CNN Layer Parameters Parameter vs Argument 首先解释一下函数中 parameter 和 argument 两个词的含义，parameter 指的是参数的名称（place-holder），可以看作是函数内部的本地变量名；而 argument 指的是传递给函数的实际值。 比如上面的 conv1 层有一个参数是“in_channels=1”，这里面“in_channels”就是 parameter，“1”就是“argument”。 两类网络层 parameter 网络层的 parameter 可以分为两个类别： Hyperparameters（超参数） Data dependent hyperparameters（数据相关的超参数） 超参数指的是需要在构建网络之初就要手工设定的参数，而且这些参数不会像权值一样随着模型的训练进行优化，而是需要手工（用交叉验证的方法）进行优化。 以上面构建的神经网络为例： 12345678910111213class Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, t): # implement the forward pass return t 首先解释一下，对于卷积层有三个 parameter 需要定义： kernel_size = n - 深度学习中 kernel 和 filter 说的是同一件事，都是指卷积层滤波器的像素尺寸，是一个 n x n 的正方形，只要指定一个数字 n 即可； out_channels = n - 前面讲过卷积层的 output channel 也称为 feature map，所以这个参数实际上制定的是 filter 的数量（种类数），每个 filter 对应一个 output channel。对于灰度图片，输出通道数就是 1 x n；对于RGB图像，实际输出通道是 3 x n 吗？ in_channels = n - 输入卷积层的通道数。 （但是 filter 的种类在哪指定呢？） 对于全连接层有两个 parameter ： in_features = n - 全连接层的输入特征数，因为全连接层的输入和输出都是一阶张量，这里的特征数指的就是一阶张量的长度（元素数量）； out_features = n - 全连接层的输出特征数，说明同上。 Hyperparameters 上面的例子中，加粗和下划线的这些都是 hyperparameters，这些都是需要手工指定的超参数，这些参数决定了网络的结构，会直接影响网络的性能，是我们在设计神经网络时主要考虑的。 Data dependent hyperparameters 上面这些参数中，加粗和下划线的部分是 data dependent hyperparameters，很明显的第一个卷积层的 in_channels 是由图片的通道数决定的，灰度图为1， RGB图为3；最后一个线性层也就是输出层的 out_features 是由图片的类别数决定的，此例中图片分为 0-9 十个类别。其余各层的 in_channels 和 in_features 需要和其前一层的输出相匹配，所以也算是 data dependent hyperparameters。 这里要注意的是，从卷积层输入到全连接层的过程中，原本的高阶张量需要 flatten 为一阶张量，因此 fc1 层的 in_features 参数需要注意与 conv2 的输入相匹配。（后面会讲为什么是 12 4 4，因为加了池化层） 这个表里对上面涉及到的所有参数进行了一个总结： 最后要说明的一点是： 在各个层的输出 parameters，也就是 out_channels 和 out_features 参数，取值有一个这样的一般规律：在各个卷积层中，out_channels 从前至后越来越多；而在各全连接层中，out_features 从前之后越来越少。 - Episode 19 of 33这一集讲的是 CNN 中的权值（weight）。如上一集所述，权值保存在每一层的内部，并且随着网络的训练而更新。 （1）Learnable Parameters上一集说过 hyperparameters 和 data dependent hyperparameters 两类参数，现在再介绍一类参数就是 learnable parameters。 Learnable parameters 是指参数的值在训练过程中可以被学习的参数。 Learnable parameters 一般随机给定初值，并随着网络学习以迭代方式更新这些值。所谓学习就是指通过迭代的方法逐渐寻找到使得损失函数最小的参数值。 （2）创建神经网络实例化对象我们首先对上一集创建的网络进行实例化： 1network = Network() 这样我们利用扩展的 Network 类实例化了一个名为 network 的 object。 在上一集的笔记说过，我们是利用扩展 nn.Module class 的方法构建了 Network subclass，并且指定了5个 layer 作为 class attributes。实例化之后，layer 就是 network 这个 object 的属性了，而属性本身也可以是一个 object（可以套娃）。而我们创建的 layer 本身也是通过预定义的类创建的，而权值 weight 就是 layer 这些 object 中的属性。 Network String Representation 实例化之后，如果我们输入如下代码： 12345678&gt; print(network)Network( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=192, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=60, bias=True) (out): Linear(in_features=60, out_features=10, bias=True)) 会发现输出了一段字符信息，告诉我们这个网络的结构和参数是怎样的。network object 具有这个功能是因为他是扩展自 nn.Module 类创建的，继承了 nn.Module 类中功能，而并非是由我们编写的（你可以尝试一下把 Network class 括号内的“nn.Module”删掉，再把第三行 super 那一行删掉，就没有这个功能了）。准确的说，这个输出 string representation 的功能是继承自Python默认的一个 object base class，所有的Python class 都是通过扩展 object 基类创建的。 Overriding 如果想要按照自己设定的格式，需要修改这个已经设定好的功能，称作重写（override），上一集的笔记也讲过这个概念了。 于是我们在原来的 Network subclass 中将原来的 string representation 功能重写（利用 repr(self) 函数）： 12345678910111213141516class Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, t): # implement the forward pass return t def __repr__(self): # 重写 string representation return &quot;lizardnet&quot; 这时候就会有： 12&gt; print(network)lizardnet 所以实际上，PyTorch中的 nn.Module class 也是重写了Python的 object class 中的 repr(self) 函数。一言不合看源码（nn.Module 中重写的函数）： String Representation 的解释 接下来解释一下打印出的这些信息的含义： 12345678&gt; print(network)Network( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=192, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=60, bias=True) (out): Linear(in_features=60, out_features=10, bias=True)) 卷积层的 kernel_size 上一集也说过了，只输入一个5，但 filter 是正方形的，因此 kernel_size 就是 55的。*stride 这个参数上一集没有设置，没有设置的时候默认为 (1,1)，这个参数指定的是 filter 在扫描的时候每次移动的距离，即每次向右移动1个像素，扫面完一行之后再回到最左边并下移1个像素。 （3）访问网络的 layer因为 layer 是 network 这个对象的属性，像很多语言一样，Python使用“.”这个标记来访问对象的属性，比如“object.attribute”，前面也用过很多次了。 1234567891011121314&gt; network.conv1Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))&gt; network.conv2Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))&gt; network.fc1Linear(in_features=192, out_features=120, bias=True)&gt; network.fc2 Linear(in_features=120, out_features=60, bias=True)&gt; network.outLinear(in_features=60, out_features=10, bias=True) 如上面所示，其实采用“object.attribute”访问属性的结果本身也就是输出了一个这个属性的 string representation。 访问 layer 的 weight 而 weight 又是 layer 的属性，于是可以用类似“network.layer.weight”的形式来访问某一层的权值： 123456789101112131415161718192021222324252627282930313233343536373839&gt; network.conv1.weightParameter containing:tensor([[[[ 0.0692, 0.1029, -0.1793, 0.0495, 0.0619], [ 0.1860, 0.0503, -0.1270, -0.1240, -0.0872], [-0.1924, -0.0684, -0.0028, 0.1031, -0.1053], [-0.0607, 0.1332, 0.0191, 0.1069, -0.0977], [ 0.0095, -0.1570, 0.1730, 0.0674, -0.1589]]], [[[-0.1392, 0.1141, -0.0658, 0.1015, 0.0060], [-0.0519, 0.0341, 0.1161, 0.1492, -0.0370], [ 0.1077, 0.1146, 0.0707, 0.0927, 0.0192], [-0.0656, 0.0929, -0.1735, 0.1019, -0.0546], [ 0.0647, -0.0521, -0.0687, 0.1053, -0.0613]]], [[[-0.1066, -0.0885, 0.1483, -0.0563, 0.0517], [ 0.0266, 0.0752, -0.1901, -0.0931, -0.0657], [ 0.0502, -0.0652, 0.0523, -0.0789, -0.0471], [-0.0800, 0.1297, -0.0205, 0.0450, -0.1029], [-0.1542, 0.1634, -0.0448, 0.0998, -0.1385]]], [[[-0.0943, 0.0256, 0.1632, -0.0361, -0.0557], [ 0.1083, -0.1647, 0.0846, -0.0163, 0.0068], [-0.1241, 0.1761, 0.1914, 0.1492, 0.1270], [ 0.1583, 0.0905, 0.1406, 0.1439, 0.1804], [-0.1651, 0.1374, 0.0018, 0.0846, -0.1203]]], [[[ 0.1786, -0.0800, -0.0995, 0.1690, -0.0529], [ 0.0685, 0.1399, 0.0270, 0.1684, 0.1544], [ 0.1581, -0.0099, -0.0796, 0.0823, -0.1598], [ 0.1534, -0.1373, -0.0740, -0.0897, 0.1325], [ 0.1487, -0.0583, -0.0900, 0.1606, 0.0140]]], [[[ 0.0919, 0.0575, 0.0830, -0.1042, -0.1347], [-0.1615, 0.0451, 0.1563, -0.0577, -0.1096], [-0.0667, -0.1979, 0.0458, 0.1971, -0.1380], [-0.1279, 0.1753, -0.1063, 0.1230, -0.0475], [-0.0608, -0.0046, -0.0043, -0.1543, 0.1919]]]], requires_grad=True) 这里发现返回值上面多了一个“Parameter containing: ”的字样。这里解释了一下因为 weight 这个 tensor 是一个可以学习的 special tensor（会更新）。为了能够追踪这种 learnable parameter 的变化，PyTorch使用了一个扩展自 torch.Tensor class 的类叫做 Parameter class，而每一个 layer 中保存 weight 的这个 tensor 都是 Parameter 类的一个实例。“Parameter containing: ”就是在 Parameter 类中重写 repr() 函数时加上的（又去找了源码）。 Weight Tensor Shape 看一下各个 layer 中 weight tensor 的 shape，先看卷积层： 1234567891011&gt; network.conv1Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))&gt; network.conv1.weight.shapetorch.Size([6, 1, 5, 5])&gt; network.conv2Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))&gt; network.conv2.weight.shapetorch.Size([12, 6, 5, 5]) 卷积层的 weight tensor 其实就是 filter 本身。卷积层的卷积操作就是在 input channel 和 filter 两个 tensor 之间进行的运算。 我们定义的 conv1 层有6个单颜色通道的 55 filter，这6个 filter 使用了1个 tensor 而非 6个 tensor 来代表，因此这一层的 weight tensor shape 是 [6, 1, 5, 5]，对应参数的话就是 *[out_channels, in_channels, kernel_size, kernel_size]。 那么相对应的 conv2 层的 shape 就是 [12, 6, 5, 5] 了，12个 filter，6个 input channels，55 的 filter。这里要说明的是 filter 的数量仍然是12个，而不是126个，这里其实是把12个 filter 复制了6份，用来匹配上一层过来的6个 input channels，这样一次张量运算就可以得到结果了。这里的第二个参数“6”就称作 filter 的 depth。 所以卷积层的 weight tensor shape 的含义又是：[filter number, depth of the filter, filter height, filter width]。 接下来看线性层： 12345678&gt; network.fc1.shapetorch.Size([120, 192])&gt; network.fc2.shape torch.Size([60, 120])&gt; network.out.shapetorch.Size([10, 60]) 线性层 or 全连接层的输入和输出都是一阶张量，所以线性层的 weight tensor 是一个二阶张量，因此也称为 weight matrix，weight matrix shape 对应的线性层的参数是是 [out_features, in_features]。 之所以输入和输出看似是颠倒的，是因为与矩阵乘法规则有关。比如 in_features 是一个 m1 的列向量，out_features 是一个 n1 的列向量，out_features 等于一个矩阵左乘 in_features，那显然这个矩阵应该是 n*m 的。 这也说明，线性层的变换（linear function）就是由 weight matrix 来定义的，因此 weight matrix 也称作 linear map（线性映射）。 （4）批量访问网络参数前面讲的是如何访问各层的 weight，可以用下面的方法一次性获取所有层的 weight 信息（加个循环呗）。 方法一： 12345678910111213&gt; for param in network.parameters(): print(param.shape)torch.Size([6, 1, 5, 5])torch.Size([6])torch.Size([12, 6, 5, 5])torch.Size([12])torch.Size([120, 192])torch.Size([120])torch.Size([60, 120])torch.Size([60])torch.Size([10, 60])torch.Size([10]) 这个方法是继承自 nn.Module 类的，也是上面说的 Parameter 类派上用场的地方。 这种方法是最常用的，在训练过程中进行权重更新时也是使用这种方法来遍历权重。 方法二（如果还想看见层的名字）： 12345678910111213&gt; for name, param in network.named_parameters(): print(name, '\\t\\t', param.shape)conv1.weight torch.Size([6, 1, 5, 5])conv1.bias torch.Size([6])conv2.weight torch.Size([12, 6, 5, 5])conv2.bias torch.Size([12])fc1.weight torch.Size([120, 192])fc1.bias torch.Size([120])fc2.weight torch.Size([60, 120])fc2.bias torch.Size([60])out.weight torch.Size([10, 60])out.bias torch.Size([10]) 可以看见，每个 layer 的 bias 其实也是 learnable parameter。每层都有一个 weight tensor 和一个 bias tensor。关于 bias 的作用是什么可以看这个文章： Bias in an Artificial Neural Networkdeeplizard.com/learn/video/HetFihsXSys 如果以一个线性层来举例的话，比如某一个线性层共有n个神经元，每个神经元都从前一层的m个神经元获得输入，那么这一层的第i个神经元中执行的运算就是： 前面 w*x 的项就是加权求和后的输入，g()代表激活函数（比如ReLU），bias 就是在加权求和之后，激活函数之间，在和上加一个偏置，起到类似调整阈值的作用。 这一篇结束吧。这个 section 的后续内容在（中）（下）两篇里继续做笔记。","link":"/2021/11/03/Pytorch_DeepLizard%E6%95%99%E7%A8%8B17-19%EF%BC%9A%E6%9E%84%E5%BB%BACNN%EF%BC%88%E4%B8%8A%EF%BC%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%B1%82/"},{"title":"","text":"深度学习的张量操作：Concatenate（连接） VS Stack（堆叠？）在这一集中，我们将剖析将张量串联和堆叠在一起的区别。我们将看三个示例，一个使用 PyTorch，一个使用 TensorFlow，一个使用 NumPy。 作者将演示两个操作在不同框架中的实现区别。 Existing Vs New Axes堆叠张量和串联张量之间的区别可以用一句话概括： 连接沿现有轴连接一系列张量，堆叠沿新轴连接一系列张量。 这就是这一集所介绍的两个张量操作的区别，后面的内容是对这个操作的不同实现和补充。 如何在张量中添加或插入轴12import torcht1 = torch.tensor([1,1,1]) 如上所示，导入 PyTorch 并创建一个简单的张量t1，它具有一个长度为 3 的轴。现在，要在 PyTorch 中向该张量添加轴，我们使用该unsqueeze()函数。 12&gt; t1.unsqueeze(dim=0)tensor([[1, 1, 1]]) 在这里，我们在这个张量索引（dim）为 0 的位置添加了一个轴，或者是一个维度。现在这个张量的形状变成1 x 3。 我们也可以试着在张量的第二个索引处添加一个轴： 1234&gt; t1.unsqueeze(dim=1)tensor([[1], [1], [1]]) 这个操作则可以看出使这个张量的形状变成了3 x 1，像这样添加轴如我们所见会改变张量内数据的组织方式，但不会改变数据本身。基本上，我们只是在重塑（reshape）张量。我们可以通过检查其中每一个的形状来看到这一点。 123456&gt; print(t1.shape)&gt; print(t1.unsqueeze(dim=0).shape)&gt; print(t1.unsqueeze(dim=1).shape)torch.Size([3])torch.Size([1, 3])torch.Size([3, 1]) OK，现在我们回想一下最开始说的串联与堆叠张量，当我们串联时，我们正在沿着现有轴连接一系列张量。这意味着我们正在扩展现有轴的长度。 当我们堆叠时，我们正在创建一个以前不存在的新轴，这发生在我们序列中的所有张量中，然后我们沿着这个新序列连接。 PyTorch 中的 Stack Vs Cat我们用于这些操作的两个函数是stack和cat。 建立三个张量t1 t2 t3 12345import torcht1 = torch.tensor([1, 1, 1])t2 = torch.tensor([2, 2, 2])t3 = torch.tensor([3, 3, 3]) 现在进行连接操作即cat 12345&gt; torch.cat( (t1,t2,t3) ,dim=0)tensor([1, 1, 1, 2, 2, 2, 3, 3, 3]) 可以看出我们通过连接操作得到了一个轴长度为 9 的张量。 现在，让我们沿着我们将插入的新轴堆叠这些张量。我们将在第一个索引处插入一个轴。请注意，此插入将在stack函数的后台隐式发生 。 1234567&gt; torch.stack( (t1, t2, t3), dim=0)tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]]) 可以看出我们通过堆叠操作得到了一个形状为 3 x 3 的张量。 也可以通过第二种做法得到这样的效果，即先对张量进行unsqueeze操作，在进行连接cat。 1234567891011&gt; torch.cat( ( t1.unsqueeze(0) ,t2.unsqueeze(0) ,t3.unsqueeze(0) ) ,dim=0)tensor([[1, 1, 1], [2, 2, 2], [3, 3, 3]]) 在这种情况下，我们可以看到我们得到的结果与通过堆叠得到的结果相同。 但是需要注意的是，我们想沿着第二个轴连接 3 个张量的话，光改变dim是无法做到的，这是因为当前不存在第二个轴，不存在咋连接？对吧。因此在这种情况下，我们只能使用堆叠操作完成。 1234567&gt; torch.stack( (t1,t2,t3) ,dim=1)tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]]) 如果对原有的三个张量t1 t2 t3 ，先进行添加轴再连接，才能得到上述堆叠操作的相同结果。 1234567891011&gt; torch.cat( ( t1.unsqueeze(1) ,t2.unsqueeze(1) ,t3.unsqueeze(1) ) ,dim=1)tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3]]) TensorFlow里的 Stack Vs Concat如果不感兴趣直接跳过~ 同样的，先创建三个张量t1 t2 t3 12345import tensorflow as tft1 = tf.constant([1,1,1])t2 = tf.constant([2,2,2])t3 = tf.constant([3,3,3]) 现在，让我们将这些张量相互连接起来。为了在 TensorFlow 中做到这一点，我们使用的是tf.concat()函数，在PyTorch中指定的是dim，在TensorFlow 中，我们指定的是axis（轴），意思其实是相同的。 12345&gt; tf.concat( (t1,t2,t3) ,axis= 0 ) tf.Tensor: id = 4 , shape=( 9 ,), dtype=int32, numpy=array([ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ]) 现在，堆叠操作如下所示： 12345678&gt; tf.stack( (t1,t2,t3) ,axis= 0 ) tf.Tensor: id = 6 , shape=( 3 , 3 ), dtype=int32, numpy= array([[ 1 , 1 , 1 ], [ 2 , 2 , 2 ], [ 3 , 3 , 3 ]]) 同PyTorch一样，我们也试着在对张量手动插入新维度（添加新的轴）后连接这些张量。 123456789101112&gt; tf.concat( ( tf.expand_dims(t1, 0 ) ,tf.expand_dims(t2, 0 ) ,tf.expand_dims(t3, 0 ) ) ,axis= 0 ) tf.Tensor: id = 15 , shape=( 3 , 3 ), dtype=int32, numpy= array([[ 1 , 1 , 1 ], [ 2 , 2 , 2 ], [ 3 , 3 , 3 ]]) 可以看出，TensorFlow 使用的相同功能函数名为expand_dims()，这两者也是相同的。（后面不再赘述） Unsqueezing and expanding dims mean the same thing. 接下来，对张量相对于第二个轴进行堆叠操作： 12345678&gt; tf.stack( (t1,t2,t3) ,axis=1)tf.Tensor: id=17, shape=(3, 3), dtype=int32, numpy=array([[1, 2, 3], [1, 2, 3], [1, 2, 3]]) 起同样效果的： 123456789101112&gt; tf.concat( ( tf.expand_dims(t1, 1), tf.expand_dims(t2, 1), tf.expand_dims(t3, 1) ), axis = 1)tf.Tensor: id = 26 , shape=( 3 , 3 ), dtype=int32, numpy= array([[ 1 , 2 , 3 ], [ 1 , 2 , 3 ], [ 1 , 2 , 3 ]]) 可以观察到，这些操作所得的结果都是与 PyTorch 一致的。 NumPy 里的 Stack Vs Concat同样的，先创建三个张量t1 t2 t3 ，在 numpy 里，张量其实就是nd-array： 12345import numpy as npt1 = np.array([1,1,1])t2 = np.array([2,2,2])t3 = np.array([3,3,3]) 连接操作，使用np.concatenate()函数： 12345&gt; np.concatenate( (t1,t2,t3) ,axis=0)array([1, 1, 1, 2, 2, 2, 3, 3, 3]) 与 TensorFlow 一样，NumPy 也使用axis作为轴参数名，但在这里，我们还看到了另一种命名变化：NumPy 使用完整的单词 concatenate作为函数名。 下面是不同的库所对应的函数名： Library Function Name PyTorch cat() TensorFlow concat() NumPy concatenate() 堆叠操作： 1234567&gt; np.stack( (t1,t2,t3) ,axis=0)array([[1, 1, 1], [2, 2, 2], [3, 3, 3]]) 手动添加轴，再连接： 1234567891011&gt; np.concatenate( ( np.expand_dims(t1, 0 ) ,np.expand_dims(t2, 0 ) ,np.expand_dims(t3, 0 ) ) ,axis= 0 ) array([[ 1 , 1 , 1 ], [ 2 , 2 , 2 ], [ 3 , 3 , 3 ]]) 使用第二个轴来完成堆叠操作： 1234567&gt; np.stack( (t1,t2,t3) ,axis= 1 ) array([[ 1 , 2 , 3 ], [ 1 , 2 , 3 ], [ 1 , 2 , 3 ]]) 通过手动插入维度连接： 1234567891011&gt; np.concatenate( ( np.expand_dims(t1, 1 ) ,np.expand_dims(t2, 1 ) ,np.expand_dims(t3, 1 ) ) ,axis= 1 ) array([[ 1 , 2 , 3 ], [ 1 , 2 , 3 ], [ 1 , 2 , 3 ]]) 现实中使用连接和堆叠的例子将图像合并为一个批次假设我们有三个单独的图像作为张量。每个图像张量都有三个维度，一个通道轴，一个高度轴，一个宽度轴。请注意，这些张量中的每一个都是相互独立的。现在，假设我们的任务是将这些张量连接在一起以形成三个图像的单个批量张量。 我们是连接还是堆叠？ 答案是进行堆叠，由于在此示例中，仅存在三个维度，而对于一个批次来说，我们需要四个维度。这意味着答案是沿新轴堆叠张量，这个轴我们将其称为批次轴即batch axis。这将为我们处理得到一个具有四个维度的张量。 就像这样： 1234567891011121314import torch# 创建三个张量，代表着3通道，大小为 28 * 28 pixel的图像t1 = torch.zeros(3,28,28)t2 = torch.zeros(3,28,28)t3 = torch.zeros(3,28,28)# 三张图像堆叠，即是得到一个具有三张图片的批次batchtorch.stack( (t1,t2,t3) ,dim=0).shape## output ##torch.Size([3, 3, 28, 28]) 将批次合并为一个批次假设我们拥有与之前相同的三张图像，但这次图像已经具有批次的维度。这实际上意味着我们有三个批次，batch_size 为 1 。 我们是连接还是堆叠？ 答案是连接。 上述意味着我们可以沿着批次维度连接这些批次。在这种情况下，不需要堆叠。 就像这样： 123456789import torch t1 = torch.zeros( 1 , 3 , 28 , 28 ) t2 = torch.zeros( 1 , 3 , 28 , 28 ) t3 = torch.zeros( 1 , 3 , 28 , 28 ) torch.cat( (t1 ,t2, t3) ,dim= 0 ).shape ## output ## torch.Size([ 3 , 3 , 28 , 28 ]) 最终得到一个张量，这个张量代表着三个批次的图像，每批次图像只有一张。 将图像与现有批次合并假设我们有相同的三个独立的图像张量。只是，这一次，我们已经有了一个批次的张量。假设我们的任务是将这三个单独的图像与这个批次连接起来。我们是连接还是堆叠？ 这实际上是一项非常常见的任务，我们如果对这两个操作足够熟悉应该是能够知道答案是先堆叠然后连接。 就像这样： 12345678910111213141516171819import torchbatch = torch.zeros(3,3,28,28)t1 = torch.zeros(3,28,28)t2 = torch.zeros(3,28,28)t3 = torch.zeros(3,28,28)torch.cat( ( batch ,torch.stack( (t1,t2,t3) ,dim=0 ) ) ,dim=0).shape## output ##torch.Size([6, 3, 28, 28]) 老样子，堆叠换成手动添加一个轴再连接： 123456789101112131415161718import torchbatch = torch.zeros(3,3,28,28)t1 = torch.zeros(3,28,28)t2 = torch.zeros(3,28,28)t3 = torch.zeros(3,28,28)torch.cat( ( batch ,t1.unsqueeze(0) ,t2.unsqueeze(0) ,t3.unsqueeze(0) ) ,dim=0).shape## output ##torch.Size([6, 3, 28, 28]) 🆗，这一集我们知道了如何在不同的框架中进行张量连接和堆叠操作，那么下一集即是有关混淆矩阵的内容。 使用 PyTorch 创建混淆矩阵在这一集中，我们将构建一些函数，使我们能够获得训练集中每个样本的预测张量。 然后，我们将看到如何利用这个预测张量以及每个样本的标签来创建混淆矩阵。这个混淆矩阵将使我们能够看到我们的网络相互混淆了哪些类别。事不宜迟，让我们开始吧。 我们现在在课程中的位置。 准备数据 构建模型 训练模型 分析模型的结果 构建、绘制和解释混淆矩阵 混淆矩阵要求要为我们的整个数据集创建一个混淆矩阵，我们需要有一个与我们的训练集长度相同的单一维度的预测张量。 12&gt; len (train_set) 60000 这个预测张量将包含我们训练集中每个样本的十个预测。获得这个张量后，我们可以使用标签张量来生成混淆矩阵。 12&gt; len (train_set.targets) 60000 混淆矩阵将向我们展示模型在哪里变得混淆。更具体地说，混淆矩阵将向我们显示模型正确预测的类别以及模型错误预测的类别。对于不正确的预测，我们将能够看到模型预测的是哪个类别，这将向我们显示哪些类别 混淆了模型。 获取整个训练集的预测为了获得所有训练集样本的预测，我们需要将所有样本向前传递通过网络。为此，可以创建batch_size为1的DataLoader，这样将一次将单个批次传递给网络，并将为我们提供所有训练集样本所需的预测张量。 然而，如果我们在不同的数据集上训练，根据计算资源和训练集的大小，我们需要一种方法来预测较小的批次并收集结果。为了收集结果，我们将使用该torch.cat()函数将输出张量连接在一起以获得我们的单个预测张量。让我们构建一个函数来做到这一点。 构建一个函数来获得所有样本的预测我们将创建一个名为get_all_preds()的函数，我们将传递一个模型和一个data loader。该模型将用于获得预测值，data loader将用于提供训练集中的批次。 123456789101112@torch.no_grad()def get_all_preds(model, loader): all_preds = torch.tensor([]) for batch in loader: images, labels = batch preds = model(images) all_preds = torch.cat( (all_preds, preds) ,dim=0 ) return all_preds 此函数的植入会创建一个空张量all_preds以保存输出预测。然后，它迭代来自data loader的批次，并将输出预测与 all_preds张量连接起来。最后，所有的预测all_preds，返回给调用者。 请注意，在顶部，我们使用@torch.no_grad()PyTorch 装饰器对函数进行了注释。这是因为我们希望此函数执行省略**梯度跟踪**。 这是因为梯度跟踪使用内存，并且在推理期间（在不训练的同时获得预测）不需要跟踪计算图。装饰器是在执行特定功能时局部关闭梯度跟踪功能的一种方式。 另外一种局部关闭梯度计算功能的方式 123with torch.no_grad(): prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000) train_preds = get_all_preds(network, prediction_loader) 使用预测张量现在，我们有了预测张量，我们可以将它get_num_correct()与训练集标签一起传递给我们在前一集中创建的函数，以获得正确预测的总数。 123456&gt; preds_correct = get_num_correct(train_preds, train_set.targets)&gt; print('total correct:', preds_correct)&gt; print('accuracy:', preds_correct / len(train_set))total correct: 53578accuracy: 0.8929666666666667 我们可以看到正确预测的总数，并通过除以训练集中的样本数来打印准确率。 构建混淆矩阵我们构建混淆矩阵的任务是计算相对于真实值（targets）的预测值数量。 这将创建一个矩阵作为热图，告诉我们预测值相对于真实值的下降位置。 为此，我们需要有targets张量和来自train_preds张量的预测标签。 12345&gt; train_set.targetstensor([9, 0, 0, ..., 3, 0, 5])&gt; train_preds.argmax(dim=1)tensor([9, 0, 0, ..., 3, 0, 5]) 现在，如果我们按元素比较两个张量，我们可以查看预测值是否与目标匹配。此外，如果我们计算预测值与目标标签的数量，两个张量内的值将作为我们矩阵的坐标。让我们沿着第二维堆叠这两个张量，这样我们就可以得到60,000个有序对。 12345678910111213141516171819202122232425# 预测值和标签两个张量进行堆叠&gt; stacked = torch.stack( ( train_set.targets ,train_preds.argmax(dim=1) ) ,dim=1)&gt; stacked.shapetorch.Size([60000, 2])&gt; stackedtensor([ [9, 9], [0, 0], [0, 0], ..., [3, 3], [0, 0], [5, 5]])&gt; stacked[0].tolist()[9, 9] 现在，我们可以迭代这些对并计算矩阵中每个位置的出现次数。让我们创建矩阵。由于我们有十个预测类别，我们将有一个十乘十的矩阵。 123456789101112131415# cmt: confusion matrix&gt; cmt = torch.zeros(10,10, dtype=torch.int64)&gt; cmttensor([ [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]) 现在，我们将迭代预测目标对，并在每次出现特定位置时向矩阵内的值加 1。 123for p in stacked: tl, pl = p.tolist() cmt[tl, pl] = cmt[tl, pl] + 1 得到的混淆矩阵结果如下： 12345678910111213&gt; cmttensor([ [5637, 3, 96, 75, 20, 10, 86, 0, 73, 0], [ 40, 5843, 3, 75, 16, 8, 5, 0, 10, 0], [ 87, 4, 4500, 70, 1069, 8, 156, 0, 106, 0], [ 339, 61, 19, 5269, 203, 10, 72, 2, 25, 0], [ 23, 9, 263, 209, 5217, 2, 238, 0, 39, 0], [ 0, 0, 0, 1, 0, 5604, 0, 333, 13, 49], [1827, 7, 716, 104, 792, 3, 2370, 0, 181, 0], [ 0, 0, 0, 0, 0, 22, 0, 5867, 4, 107], [ 32, 1, 13, 15, 19, 5, 17, 11, 5887, 0], [ 0, 0, 0, 0, 0, 28, 0, 234, 6, 5732]]) 为了以numpy.ndarray的形式操作生生成的混淆矩阵（目的是为了更好地使用python库进行可视化操作？），我们使用sklearn.metrics 库中的confusion_matrix()函数。 导入相关库： 1234import matplotlib.pyplot as pltfrom sklearn.metrics import confusion_matrixfrom resources.plotcm import plot_confusion_matrix 在这里需要注意的是，作者这里导入的plotcm是本地文件，即是位于当前目录中名为resource的文件夹中的plotcm.py文件。我们将在本地中实现它。它的作用是借助plot库绘制confusion matrix。 首先我们先生成一个混淆矩阵（由于是在不同的时间生成的示例，所以在这里作者得到的矩阵不相同）： 1234567891011121314151617&gt; cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1))&gt; print(type(cm))&gt; cm&lt;class 'numpy.ndarray'&gt; # 得到numpy.ndarray类型的混淆矩阵Out[74]:array([[5431, 14, 88, 145, 26, 7, 241, 0, 48, 0], [ 4, 5896, 6, 75, 8, 0, 8, 0, 3, 0], [ 92, 6, 5002, 76, 565, 1, 232, 1, 25, 0], [ 191, 49, 23, 5504, 162, 1, 61, 0, 7, 2], [ 15, 12, 267, 213, 5305, 1, 168, 0, 19, 0], [ 0, 0, 0, 0, 0, 5847, 0, 112, 3, 38], [1159, 16, 523, 189, 676, 0, 3396, 0, 41, 0], [ 0, 0, 0, 0, 0, 99, 0, 5540, 0, 361], [ 28, 6, 29, 15, 32, 23, 26, 14, 5827, 0], [ 0, 0, 0, 0, 1, 61, 0, 107, 1, 5830]], dtype=int64) plotcm.py的实现: 123456789101112131415161718192021222324252627import itertoolsimport numpy as npimport matplotlib.pyplot as pltdef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues): if normalize: cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print('Confusion matrix, without normalization') print(cm) plt.imshow(cm, interpolation='nearest', cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = '.2f' if normalize else 'd' thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') 在本地实现该文件（函数）后，由于我们知道PyTorch 张量是 类似数组的 Python 对象，所以我们可以将它们直接传递给confusion_matrix()函数。 在做好这些之后，我们已经准备好绘制混淆矩阵，但首先我们需要创建一个预测类list以传递给plot_confusion_matrix()函数。我们的预测类及其对应的索引如下表所示： Index Label 0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot OK，我们接着就可以调用下面函数绘制矩阵： 1234567891011121314&gt; plt.figure(figsize=(10,10))&gt; plot_confusion_matrix(cm, train_set.classes)Confusion matrix, without normalization[[5431 14 88 145 26 7 241 0 48 0][ 4 5896 6 75 8 0 8 0 3 0][ 92 6 5002 76 565 1 232 1 25 0][ 191 49 23 5504 162 1 61 0 7 2][ 15 12 267 213 5305 1 168 0 19 0][ 0 0 0 0 0 5847 0 112 3 38][1159 16 523 189 676 0 3396 0 41 0][ 0 0 0 0 0 99 0 5540 0 361][ 28 6 29 15 32 23 26 14 5827 0][ 0 0 0 0 1 61 0 107 1 5830]] 绘制结果如下： 混淆矩阵的解读： 混淆矩阵具有三个轴： 预测标签（类） 真标签 热图值（颜色） 矩阵对角线表示矩阵中预测和真实值相同的位置，因此这是我们希望热图更暗（颜色更深）的位置。而任何不在对角线上的值都是不正确的预测，因为预测和真实标签不匹配。 我们可以观察到：网络将 T 恤/上衣与衬衫混淆，但不会将 T 恤/上衣与鞋类义务混淆，这是很有道理的。而随着我们的模型进一步训练学习，我们将看到位于对角线之外的数字变得越来越小。 这一集就结束了，到这里我们已经完成了在 PyTorch 中构建和训练 CNN 的大量工作。恭喜你（我们）走到这一步！","link":"/2023/02/19/Pytorch_DeepLizard%E6%95%99%E7%A8%8B27%EF%BC%9A%E4%BD%BF%E7%94%A8Pytorch%E5%88%9B%E5%BB%BA%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5/"},{"title":"Pytorch_DeepLizard教程20-22：构建CNN（中）前向传播和预测","text":"此笔记引用自文章知乎链接并修改 学习笔记这个 section 太长，笔记分成了上中下三篇，这里是中篇的三集，分别讲层的 forward pass、整个网络的 forward propagation，和输入单张图片进行预测。 - Episode 20 of 33这一集主要讲了一个概念叫做 callable neural networks，通过这个概念解释了神经网络中的单独一个线性层是如何进行前向传递的。 （这一集中的代码和上一篇中构建CNN的代码没有继承关系，仅仅是为了说明问题用的） 太长不看版本：对于我们采用 nn.Linear 类创建的线性层 object fc： 12in_data = torch.tensor([1,2,3,4], dtype=torch.float32)fc = nn.Linear(in_features=4, out_features=3, bias=False) 只需要像下面这样 call fc 然后传入 iuput data 就可以得到这一层的输出了（得到前向传递的结果）： 12&gt; fc(in_data)tensor([-0.8877, 1.4250, 0.8370], grad_fn=&lt;SqueezeBackward3&gt;) 接下来是具体的解释： （1）线性层的 transformation 矩阵乘法 matmul() 函数 比方说我们现在需要通过一个线性层完成如下这样一个变换，从一个4个元素的一阶张量的输入，经过 weight matrix 的变换后变为3个元素的一阶张量输出： 形如 （这里没有考虑 bias 和 激活函数）： 可以简单地使用一个 matmul() 函数（matrix multiplication）来完成矩阵乘法： 12345678910&gt; in_features = torch.tensor([1,2,3,4], dtype=torch.float32)&gt; weight_matrix = torch.tensor([ [1,2,3,4], [2,3,4,5], [3,4,5,6]], dtype=torch.float32)&gt; weight_matrix.matmul(in_features)tensor([30., 40., 50.]) 可以看到打印的结果就是我们需要的经过前向传递后的输出。 通过 call layer 实现线性层的 transformation 对于一个用 nn.Linear 类创建的线性层 object fc，也可以可以直接用 fc(in_features) 的形式得到输出，这个方法称作 call fc layer： 1234&gt; fc = nn.Linear(in_features=4, out_features=3)&gt; fc(in_features)tensor([-0.8877, 1.4250, 0.8370], grad_fn=&lt;SqueezeBackward3&gt;) 上一集看过 nn.Linear 的源码，这个类在实例化的时候自动生成了一个 weight matrix 并指定为实例化的 fc object 的属性。但因为 weight matrix 的数值是随机的，因此和上面的结果不一样。 那么我们也可以手动指定 weight matrix 的初始值（in_features tensor 和 weight_matrix tensor 都在刚刚指定过了）： 12345&gt; fc = nn.Linear(in_features=4, out_features=3)&gt; fc.weight = nn.Parameter(weight_matrix)&gt; fc(in_features)tensor([30.0261, 40.1404, 49.7643], grad_fn=&lt;AddBackward0&gt;) 这时候发现与预期结果还是不一样，这是因为 bias 是默认启用的，如果指定在这一层中关闭 bias，就可以看到结果和使用 matmul() 函数完全一致了： 12345&gt; fc = nn.Linear(in_features=4, out_features=3, bias=False)&gt; fc.weight = nn.Parameter(weight_matrix)&gt; fc(in_features)tensor([30., 40., 50.], grad_fn=&lt;SqueezeBackward3&gt;) 这里也就是想说：前向进行单独一层的前向传递，只需要直接 call 需要进行前向传递的 layer，并且给它传入 input tensor 就行了。 （2）Callable Neural Networks因为使用 call 这种方法可以直接便捷的获得前向传递结果，因此用PyTorch构建的神经网络是 callable 的， 即 callable neural networks。 接下来解释一下为什么可以用 call layer 这样的方法来实现前向传递（解释一下这个轮子是怎么造的）。开始疯狂看源码，来感受OOP的美妙（误，节省时间可以不看）。 因为我们用来创建线性层实例 fc 的 nn.Linear 这个类也是通过扩展 nn.Module 类来构建的，于是就继承了 nn.Module 的 methods： 所以我们先来看 nn.Module 的源码。nn.Module 类中定义了一个 special method 就是 call() 函数： nn.Module 的 call() 函数 这个特殊函数就是我们在对 fc 进行 call 的时候执行的动作。因为 nn.Linear 中没有重写这个函数，而是直接继承了 nn.Module 的这个函数，所以我们在 call fc 的时候就是执行了 nn.Module 中的这个函数的功能。 接下来直接看到红框中框出的部分，call() 函数中进行前向传递就是调用了 forward() 函数来实现。而因为 nn.Linear 中重写了 forward() 函数，因此通过 call() 函数调用的就是 nn.Linear 中重写过的 forward()，而非原本 nn.Module 中的 forward() 函数。——简单来说就是：call 的功能是由基类提供的，子类并不修改，但是通过 call 执行的内容在每个不同的子类中重写以满足不同类型的层的需要。 于是我们接着来看 nn.Linear 中的 forward() 函数： 如你所见，只有一行。在 nn.Linear 中又是通过调用了 F.linear() 这个函数实现的，而 F 就是 nn.functional 这个包： 于是我们再去找 nn.functional.linear 的源码： Ohhhhh，破案了，原来你个浓眉大眼的 callable linear layer 用的也是矩阵乘法！ 所以进行前向传递的时候，不要调用 layer 中的 forward() 函数，直接 call 需要前向传递的 layer 就会自动调用 forward() 函数了。 另外，所有通过扩展 nn.Module 基类构建的子类，都可以被 call。 - Episode 21 of 33在上一级的基础之上，我们知道了怎么对神经网络中的一个 layer 进行前向传递。这一集中讲的是如何为之前构建的整个CNN编写前向传递的方法。（激活函数和池化都在这） 回顾一下我们之前构建的CNN，只是构建了五个 layer，并没有编写 forward(self, t) 函数的内容（记得把训练数据那篇笔记中导入的包都加上）： 123456789101112131415161718192021import torchimport torch.nn as nnimport torch.optim as optimimport torch.nn.functional as Fimport torchvisionimport torchvision.transforms as transformsclass Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, t): # implement the forward pass return t 这里要说的是，神经网络是有一个输入层的，所以算生输入层的话，我们构建的这个神经网络其实是6层的。但是输入层其实这是把数据放在那，并没有任何变换，或者说就是一个 identity transformation ，所以一般在构建神经网络的时候都是直接省略掉的。但是我们为了代码的完整性，还是把这一层先补上： 123456789101112131415class Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, t): # (1) input layer 补在这里了 t = t return t 接下来继续构建两个卷积层的前向传递，如上一集所说，直接用 call conv layer 的方法实现。同时这里在每一层的卷积计算之后，都加上了激活函数（用了ReLU）和池化（max pooling）：（破案了，原来在这） （为了少占点篇幅，就只贴出来 forward(self, t) 函数内部的内容了，前面的不变）： 123456789101112131415def forward(self, t): # (1) input layer t = t # (2) hidden conv layer t = self.conv1(t) # call the conv1 layer t = F.relu(t) # activation function (or activation layer) t = F.max_pool2d(t, kernel_size=2, stride=2) # max pooling (or pooling layer) # (3) hidden conv layer t = self.conv2(t) # call the conv2 layer t = F.relu(t) # activation function (or activation layer) t = F.max_pool2d(t, kernel_size=2, stride=2) # max pooling (or pooling layer) return t 在这要说的是，之前我一直误解了，以为要在 network 的 attribute 里面把所有的层都定义好（包括激活函数什么的）。但现在发现其实不是，其实只要把含有数据的部分定义为属性就可以了，其他不需要数据的操作直接在 forward() 函数里定义。 这里所说的数据指的是神经网络自身的参数，比如上面的2个卷积层中保存的数据是 filter 的类型；3个线性层中保存的数据是前一层输入到这一层的权值。而像激活函数、池化这些操作，本身不包含参数的数据，只是纯操作，就不需要在属性中定义了，直接调用函数操作即可。 可能有些说法中把激活函数也称作激活层、池化操作称作池化层，但因为这些都只是纯操作而没有 weight data，因此这里还是不将其称为层。将其视为卷积层中的三个操作之二（卷积操作、激活操作、池化操作）。 训练用的数据本身是通过临时变量 t 在网络中传递和变换的。 接下来再把两个全连接层的前向传递加上，然后再解释上一篇中说的“4*4”是池化操作的结果是怎么回事。这里记得要把卷积层输出的高阶张量进行 flatten ： 123456789101112131415161718192021222324def forward(self, t): # (1) input layer t = t # (2) hidden conv layer t = self.conv1(t) # call the conv1 layer t = F.relu(t) # activation function t = F.max_pool2d(t, kernel_size=2, stride=2) # max pooling # (3) hidden conv layer t = self.conv2(t) # call the conv2 layer t = F.relu(t) # activation function t = F.max_pool2d(t, kernel_size=2, stride=2) # max pooling # (4) hidden linear layer t = t.reshape(-1, 12 * 4 * 4) # flatten （为啥不用torch.flatten()？） t = self.fc1(t) # linear mapping t = F.relu(t) # activation function # (5) hidden linear layer t = self.fc2(t) # linear mapping t = F.relu(t) # activation function return t 4*4 是从 conv2 层经过池化操作之后输出的 feature maps 的高和宽的像素数，而我们输入的是 2828 的灰度图片。**这个从 28\\28 缩减到 4*4 的变化是由卷积操作和池化操作引起的**，具体在下一集里面讲。 在这可以简单说一下：因为卷积操作没有补零（zero padding），5*5 的卷积核（filter）按默认 stride=1 扫描，会变成 24\\24；再经过一个 2*2 且 stride=2 的 max pooling 变成 12*12。再进下一层卷积层，经卷积操作变成 8*8，再经池化就变成了 4*4。 最后补上输出层，输出层的 output 就是预测结果，像我们前面说过的，结果是一个10个元素的一阶张量，内容是对应0-9每个类别的 prediction value（相当于置信概率），10个元素之和应该为1。输出层的 linear mapping 和前面的线性层是一样的，但是在最后的激活函数操作有改动： 12345678910111213141516171819202122232425262728def forward(self, t): # (1) input layer t = t # (2) hidden conv layer t = self.conv1(t) # call the conv1 layer t = F.relu(t) # activation function t = F.max_pool2d(t, kernel_size=2, stride=2) # max pooling # (3) hidden conv layer t = self.conv2(t) # call the conv2 layer t = F.relu(t) # activation function t = F.max_pool2d(t, kernel_size=2, stride=2) # max pooling # (4) hidden linear layer t = t.reshape(-1, 12 * 4 * 4) # flatten （为啥不用torch.flatten()？） t = self.fc1(t) # linear mapping t = F.relu(t) # activation function # (5) hidden linear layer t = self.fc2(t) # linear mapping t = F.relu(t) # activation function # (6) output layer t = self.out(t) # linear mapping # t = F.softmax(t, dim=1) # not use here return t 一般来说在神经网络的输出层，会把隐藏层用的激活函数 ReLU 换成 softmax 函数。上面说的输出对应各个类别的 prediction value 就是由 softmax 函数实现的，softmax 函数适用于这种只需要单类别预测情况，它返回对应每个类别的正预测值，并且使得所类别预测值的和为1（归一化？）。 但是我们在这里不用 softmax，因为两个原因：（1）如果不要求输出归一化的预测值，只需要找到最大的预测值对应的类别就可以完成预测了；（2）我们采用的损失函数是 nn.functional 包中的F.cross_entropy()，这个函数自动对输入值先进行一步 softmax 操作。 也就是说，我们在训练网络的时候采用 softmax 操作（方便与标签进行比较？）；而在预测时并不需要多进行一步 softmax 操作，直接输出 output layer 经过一步 linear mapping 后的结果即可， - Episode 22 of 33这一集是前面部分的一个小结，是要用我们之前准备好的 Fashion-MNIST 数据集和上面构建好的 CNN 来完成一次预测。 （1）Forward Propagation首先简单讲一下 forward propagation 这个术语。 前面用的术语一直都是 forward pass，我翻译成了前向传递（不知对不对），指的是数据从前向后经过一层网络，比如数据经过一层卷积层，或者经过一层线性层。 但 forward propagation（中文翻译为前向传播）指的是数据从前向后经过整个网络，将处理过的图像数据输入 input layer，然后在 output layer 获得预测结果的整个过程。 与 forward propagation 相对，还有一个 back propagation 的术语，指的是网络训练过程中的（误差）反向传播，它将发生在正向传播之后，在之后CNN训练的部分会讲。 像本篇笔记中第一集（总第20集）中说的，所有通过扩展 nn.Module 基类构建的子类都可以被 call，而我们构建的神经网络类 Network，也是通过扩展 nn.Module 基类实现的，因此对于 Network 类实例化的对象 network，我们是通过 call 的方式调用他其中的 forward() 函数来实现整个网络的前向传播。 （2）预测前的准备工作 先把要用到的代码都贴过来： 先导入需要的包，设置打印格式（行宽）： 12345678import torchimport torch.nn as nnimport torch.nn.functional as Fimport torchvisionimport torchvision.transforms as transformstorch.set_printoptions(linewidth=120) Fashion-MNIST 数据集的 E&amp;T（暂时不用 Load）： 12345678train_set = torchvision.datasets.FashionMNIST( root='./data' ,train=True ,download=True ,transform=transforms.Compose([ transforms.ToTensor() ])) 构建 CNN： 12345678910111213141516171819202122class Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12 * 4 * 4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, t): # input layer 省掉了 t = F.relu(self.conv1(t)) t = F.max_pool2d(t, kernel_size=2, stride=2) t = F.relu(self.conv2(t)) t = F.max_pool2d(t, kernel_size=2, stride=2) t = F.relu(self.fc1(t.reshape(-1, 12*4*4))) t = F.relu(self.fc2(t)) t = self.out(t) return t OK上面的都准备好了，但是再开始之前还要关掉 PyTorch 的 gradient calculation feature。这个功能是默认开启的，它会在我们进行正向传播计算的时候自动生成一个 computation graph，这个 computation graph 是用来在训练过程中计算 loss function 关于 weights 的梯度的。 上述这个生成计算图的过程是实时的，这就是一开始在PyTorch介绍中说的，PyTorch是同的是 dynamic computational graph。 因为我们目前并不训练，只是尝试走一次单张图片的预测，不需要计算梯度，所以可以把这个功能关掉来节省内存（不关也行，土豪请随意）： 12&gt; torch.set_grad_enabled(False) &lt;torch.autograd.grad_mode.set_grad_enabled at 0x17c4867dcc0&gt; （3）向神经网络输入单张图片进行预测先将我们构建的 Network 类实例化： 1&gt; network = Network() 然后从数据集中提取出单独一张图片，查看 shape（因为只是单张图片可以不进行 Load，Load 是为了 batch，参见前面数据集的笔记）： 1234&gt; sample = next(iter(train_set)) &gt; image, label = sample &gt; image.shape torch.Size([1, 28, 28]) 可以看到我们得到了一个 1 x 28 x 28 的三阶张量，代表具有一个灰度通道的 28 x 28 像素图片。 但是之前讲过，CNN 的 input tensor 是一个四阶张量，第一个维度是 batch size，显然我们这里只有一张图片的话 batch size = 1。我们把提出来的 image 这个 tensor unsqueeze 为四阶张量（见前面 tensor 操作的笔记）： 123# Inserts an additional dimension that represents a batch of size 1&gt; image.unsqueeze(0).shapetorch.Size([1, 1, 28, 28]) 于是现在我们得到了一个 shape 符合 [batch_size × in_channels × H × W] 的 input tensor，可以通过 call network 进行前向传播，并查看结果： 1234567891011121314151617181920212223# 前向传播进行预测# image shape needs to be (batch_size × in_channels × H × W)&gt; pred = network(image.unsqueeze(0))# 下面都是在查看结果&gt; pred # 查看 prediction valuetensor([[0.0991, 0.0916, 0.0907, 0.0949, 0.1013, 0.0922, 0.0990, 0.1130, 0.1107, 0.1074]])&gt; pred.shape # output tensor shapetorch.Size([1, 10])&gt; label # 图片的正确标签9&gt; pred.argmax(dim=1) # 查看预测标签tensor([7])&gt; F.softmax(pred, dim=1) # softmax 后的 prediction value（置信概率）tensor([[0.1096, 0.1018, 0.0867, 0.0936, 0.1102, 0.0929, 0.1083, 0.0998, 0.0943, 0.1030]])&gt; F.softmax(pred, dim=1).sum() # 验证和为1tensor(1.) 可以看到 output tensor 是一个 1 x 10 的二阶张量，其中 1 对应 batch size，10 是十个类别对应的 prediction value，因为没有进行 softmax 所以有正有负而且和不一定为1。 这里要说明的是，因为没有进行训练，使用的 weights 都是一开始随机生成的，所以不同的 network 实例的预测结果会不同，甚至可能同一个 network 先后两次实例化的结果都不同。但是可以看 softmax 之后的 prediction value，会发现每个类别的预测值都接近 10%（因为随机 weights 就是随机猜嘛，10个类别都是 10%）。 最后补充一个很有意思的小插曲，这一集在编程的时候有个地方写错了，运行的时候出了 bug，然后主讲人在把 bug 改好之后说了一句： Bugs are no match for the lizard.Especially, deeplizard. 哈哈哈原来 deeplizard 这个名字是这么来的，整挺好！ 这一篇笔记也到此结束，构建神经网络的这个 Part 还剩下最后两集，接下来会讲如何一次输入一批图像进行 forward propagation。","link":"/2021/11/03/Pytorch_DeepLizard%E6%95%99%E7%A8%8B20-22%EF%BC%9A%E6%9E%84%E5%BB%BACNN%EF%BC%88%E4%B8%AD%EF%BC%89%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E9%A2%84%E6%B5%8B/"},{"title":"OpenCV基础用法（一）","text":"本文涉及到的知识点 图像介绍 读取图像、视频和摄像头 基础的图像处理函数 改变大小和裁剪 添加形状和文字 图像透视变换 图像堆叠（结合） [TOC] 图像介绍图像看作由小方格组成，这些小方块都有一个明确的位置和被分配的色彩数值，小方格颜色和位置就决定该图像所呈现出来的样子。 可以将像素视为整个图像中不可分割的单位或者是元素。不可分割的意思是它不能够再切割成更小单位抑或是元素，它是以一个单一颜色的小格存在 。每一个点阵图像包含了一定量的像素，这些像素决定图像在屏幕上所呈现的大小。 常见图片格式的大小VGA = 640 x 480(height) HD = 1280 x 720 FHD = 1920 x 1080 4K = 3840 x 2160 二值图像、灰度图像、彩色图像二值图像二值图像(Binary Image)，按名字来理解只有两个值，0和1，0代表黑，1代表白，或者说0表示背景，而1表示前景。其保存也相对简单，每个像素只需要1Bit就可以完整存储信息。如果把每个像素看成随机变量，一共有N个像素，那么二值图有2的N次方种变化，而8位灰度图有255的N次方种变化，8位三通道RGB图像则有255*255*255的N次方种变化。也就是说同样尺寸的图像，二值图保存的信息更少。二值图像（binary image），即图像上的每一个像素只有两种可能的取值或灰度等级状态，人们经常用黑白、B&amp;W、单色图像表示二值图像。 灰度图像灰度图像是二值图像的进化版本，是彩色图像的退化版，也就是灰度图保存的信息没有彩色图像多，但比二值图像多，灰度图只包含一个通道的信息，而彩色图通常包含三个通道的信息，单一通道的理解可以理解为单一波长的电磁波，所以，红外遥感，X断层成像等单一通道电磁波产生的图像都为灰度图，而且在实际中灰度图易于采集和传输等性质的存在导致基于灰度图像开发的算法非常丰富。 1灰度图像（gray image）是每个像素只有一个采样颜色的图像，这类图像通常显示为从最暗黑色到最亮的白色的灰度，尽管理论上这个采样可以任何颜色的不同深浅，甚至可以是不同亮度上的不同颜色。灰度图像与黑白图像不同，在计算机图像领域中黑白图像只有黑色与白色两种颜色；但是，灰度图像在黑色与白色之间还有许多级的颜色深度。灰度图像经常是在单个电磁波频谱如可见光内测量每个像素的亮度得到的，用于显示的灰度图像通常用每个采样像素8位的非线性尺度来保存，这样可以有256级灰度（如果用16位，则有65536级）。 彩色图像彩色图像，每个像素通常是由红（R）、绿（G）、蓝（B）三个分量来表示的，分量介于（0，255）。 Read images, videos and webcam 读取图像、视频和摄像头cv2.imread：读取图像，输入图片地址cv2.imshow(窗口名，图像对象)：输出图像信息cv2.waitkey(delay)：delay参数表示图像显示的时间，0为一直显示不关闭窗口，1000为显示1000ms（1s）后关闭窗口 读取视频流123456789cap = cv2.VideoCapture(&quot;Resources/test_video.mp4&quot;)while True: success, img = cap.read() cv2.imshow(&quot;Video&quot;, img) if cv2.waitKey(1) &amp; 0xFF == ord('q'): # &amp; 0xFF == ord('q')即是表示当读取到键盘输入英文字母q之后终止循环。 break cv2.VideoCapture()：上面代码参数即是输入视频文件地址 原理：实际上读取视频后利用一个while true loop逐帧显示图片 获取本机摄像头视频，只需要将上述代码稍作改动，见注释1234567891011cap = cv2.VideoCapture(0) # 0即指摄像头ID，只有单个摄像头输入0即可，多个摄像头的话就递增。即：VideoCapture()中参数是0，表示打开笔记本的内置摄像头，参数是视频文件路径则打开视频cap.set(3, 640) #设置视频流的帧的宽度 cap.set(4, 480) #设置视频流的帧的高度 cap.set(10, 100) #设置图像的亮度(仅适用于照相机)#具体参数表示意义见下表while True: success, img = cap.read() cv2.imshow(&quot;Video&quot;, img) if cv2.waitKey(100) &amp; 0xFF == ord('q'): break param define cv2.VideoCapture.get(0) CV_CAP_PROP_POS_MSEC 视频文件的当前位置（播放）以毫秒为单位 cv2.VideoCapture.get(1) CV_CAP_PROP_POS_FRAMES 基于以0开始的被捕获或解码的帧索引 cv2.VideoCapture.get(2) CV_CAP_PROP_POS_AVI_RATIO 视频文件的相对位置（播放）：0=电影开始，1=影片的结尾。 cv2.VideoCapture.get(3) CV_CAP_PROP_FRAME_WIDTH 在视频流的帧的宽度 cv2.VideoCapture.get(4) CV_CAP_PROP_FRAME_HEIGHT 在视频流的帧的高度 cv2.VideoCapture.get(5) CV_CAP_PROP_FPS 帧速率 cv2.VideoCapture.get(6) CV_CAP_PROP_FOURCC 编解码的4字-字符代码 cv2.VideoCapture.get(7) CV_CAP_PROP_FRAME_COUNT 视频文件中的帧数 cv2.VideoCapture.get(8) CV_CAP_PROP_FORMAT 返回对象的格式 cv2.VideoCapture.get(9) CV_CAP_PROP_MODE 返回后端特定的值，该值指示当前捕获模式 cv2.VideoCapture.get(10) CV_CAP_PROP_BRIGHTNESS 图像的亮度(仅适用于照相机) cv2.VideoCapture.get(11) CV_CAP_PROP_CONTRAST 图像的对比度(仅适用于照相机) cv2.VideoCapture.get(12) CV_CAP_PROP_SATURATION 图像的饱和度(仅适用于照相机) cv2.VideoCapture.get(13) CV_CAP_PROP_HUE 色调图像(仅适用于照相机) cv2.VideoCapture.get(14) CV_CAP_PROP_GAIN 图像增益(仅适用于照相机)（Gain在摄影中表示白平衡提升） cv2.VideoCapture.get(15) CV_CAP_PROP_EXPOSURE 曝光(仅适用于照相机) cv2.VideoCapture.get(16) CV_CAP_PROP_CONVERT_RGB 指示是否应将图像转换为RGB布尔标志 cv2.VideoCapture.get(17) CV_CAP_PROP_WHITE_BALANCE × 暂时不支持 cv2.VideoCapture.get(18) CV_CAP_PROP_RECTIFICATION 立体摄像机的矫正标注（目前只有DC1394 v.2.x后端支持这个功能） capture.set 作用 capture.set(CV_CAP_PROP_FRAME_WIDTH, 1080); 宽度 capture.set(CV_CAP_PROP_FRAME_HEIGHT, 960); 高度 capture.set(CV_CAP_PROP_FPS, 30); 帧率 帧/秒 capture.set(CV_CAP_PROP_BRIGHTNESS, 1); 亮度 capture.set(CV_CAP_PROP_CONTRAST,40) 对比度 40 capture.set(CV_CAP_PROP_SATURATION, 50); 饱和度 50 capture.set(CV_CAP_PROP_HUE, 50); 色调 50 capture.set(CV_CAP_PROP_EXPOSURE, 50); 曝光 50 获取摄像头参数 Basic processing of image 基础的图像处理函数 函数 功能 cv2.cvtColor 颜色空间转换函数 cv2.GaussianBlur 高斯模糊 cv2.Canny 边缘检测 cv2.dilate 图像膨胀，一般用于图像边缘 cv2.erode 图像腐蚀，一般用于图像边缘 示例代码 12345678910111213141516171819import cv2import numpy as npprint(cv2.__version__)kernel = np.ones((5,5), np.uint8) # 利用numpy库定义一个进行图像操作的内核img = cv2.imread(&quot;Resources/lena.png&quot;)imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # 将彩色图片转化为灰度图像imgBlur = cv2.GaussianBlur(imgGray, (7, 7), 0) # 高斯（滤波）模糊，一种线性平滑滤波。（7，7）是内核，ksize.width和ksize.height可以不同，但它们都必须为正数和奇数，也可以为零imgCanny = cv2.Canny(imgGray, 150, 200) # 边缘检测imgDialation = cv2.dilate(imgCanny, kernel, iterations=1) # 图像膨胀imgEroded = cv2.erode(imgDialation, kernel, iterations=1) # 图像腐蚀cv2.imshow(&quot;Gray image&quot;, imgGray)cv2.imshow(&quot;Blur image&quot;, imgBlur)cv2.imshow(&quot;Canny image&quot;, imgCanny)cv2.imshow(&quot;Dialation image&quot;, imgDialation)cv2.imshow(&quot;Eroded image&quot;, imgEroded)cv2.waitKey(0) 代码运行结果 灰度图像 高斯模糊图像 边缘检测图像 图像膨胀 图像腐蚀（对膨胀后的图像进行操作） Resizing and cropping 改变大小和裁剪首先，第一个要注意的点是：在opencv中，我们要知道的是，x轴由西到东延伸（符合习惯），而y轴由北到南延伸（与习惯相反），如下图 第二个要注意的点，见下面代码中print(img.shape)的注释： 1234567import cv2img = cv2.imread(&quot;Resources/lambo.PNG&quot;)print(img.shape) # 打印图像形状，注意的是格式为（y轴，x轴，图层channel），即第一个数值为高度，第二个为宽度cv2.imshow(&quot;image&quot;, img)cv2.waitKey(0) 改变大小使用resize函数，参数输入图片对象和改变的宽度以及高度 1234567891011import cv2img = cv2.imread(&quot;Resources/lambo.PNG&quot;)print(img.shape)imgResize = cv2.resize(img, (800, 500)) # 图片改变大小至宽800高500print(imgResize.shape)cv2.imshow(&quot;image&quot;, img)cv2.imshow(&quot;Resized_IMG&quot;, imgResize)cv2.waitKey(0) 裁剪见代码注释（裁剪不必使用函数） 12345678910import cv2img = cv2.imread(&quot;Resources/lambo.PNG&quot;)print(img.shape)imgCropped = img[0:200, 200:500] # 即高度从y轴坐标0到200px，宽度截取x轴坐标200到500pxcv2.imshow(&quot;image&quot;, img)cv2.imshow(&quot;Cropped_IMG&quot;, imgCropped)# cv2.imshow(&quot;Resized_IMG&quot;, imgResize)cv2.waitKey(0) Adding shapes and text to image 添加形状和文字定义一个高400，宽500的蓝色图像 12345678910import cv2import numpy as npimg = np.zeros((400, 500, 3), np.uint8) # 高400，宽500img[:] = 0, 255, 0 # 与裁剪同理，img[:]指的是对整幅图片进行颜色更改，[0, 255, 0]表示的是B(0)G(255)R(0)绿色print(img)cv2.imshow(&quot;img&quot;, img)cv2.waitKey(0) 而如果颜色赋值代码如下： 1img[100:200, 200:300] = 0, 255, 0 则是在y轴100-200，x轴200-300的矩形颜色置为绿色。like this： 绘制基础形状：线段、矩形、圆 12345678910import cv2import numpy as npimg = np.zeros((400, 500, 3), np.uint8)cv2.line(img, (0, 0), (500, 400), (0, 255, 0), 2)cv2.rectangle(img, (0, 0), (200, 250), (255, 0, 0), 2)cv2.circle(img, (200, 200), 80, (0, 0, 255), 2)cv2.imshow(&quot;img&quot;, img)cv2.waitKey(0) 绘制基础形状cv2.line(img, (0, 0), (500, 400), (0, 255, 0), 2)：绘制一条(0, 0)到(500, 400)坐标的绿色线段，厚度为2； cv2.rectangle(img, (0, 0), (200, 250), (255, 0, 0), 2)：绘制一个左上顶点在(0, 0)，右下顶点在(200, 250)的蓝色矩形，线条厚度为2； cv2.circle(img, (200, 200), 80, (0, 0, 255), 2)：绘制一个圆心在(200, 200)，半径为80的红色圆形，线条厚度为2； 图片上绘制文字：cv2.putText()1用法： cv2.putText(image, text, org, font, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]]) 参数：image:它是要在其上绘制文本的图像。text:要绘制的文本字符串。org:它是图像中文本字符串左下角的坐标。坐标表示为两个值的元组，即(X坐标值，Y坐标值)。font:它表示字体类型。一些字体类型是FONT_HERSHEY_SIMPLEX，FONT_HERSHEY_PLAIN，等fontScale:字体比例因子乘以font-specific基本大小。color:它是要绘制的文本字符串的颜色。对于BGR，我们通过一个元组。例如：(255，0，0)为蓝色。thickness:它是线的粗细像素。lineType:这是一个可选参数，它给出了要使用的行的类型。bottomLeftOrigin:这是一个可选参数。如果为true，则图像数据原点位于左下角。否则，它位于左上角。 code: 123456789img = np.zeros((400, 500, 3), np.uint8)cv2.line(img, (0, 0), (500, 400), (0, 255, 0), 2)cv2.rectangle(img, (0, 0), (200, 250), (255, 0, 0), 2)cv2.circle(img, (200, 200), 80, (0, 0, 255), 2)cv2.putText(img, &quot;OpenCV Text&quot;, (100, 200), cv2.FONT_HERSHEY_SIMPLEX, 2, (122, 122, 122), 1)cv2.imshow(&quot;img&quot;, img)cv2.waitKey(0) warp perspective 透视变换cv2.warpPerspective() 叫做透视变换。 cv2.warpPerspective()用于解决cv2.warpAffine()不能处理视场和图像不平行的问题。 应用cv2.warpPerspective()前需先使用cv2.getPerspectiveTransform()得到转换矩阵。转换矩阵为3x3阶。 1234567891011121314import cv2import numpy as npimg = cv2.imread(&quot;Resources/cards.jpg&quot;)width, height = 250, 350point1 = np.float32([[111, 219], [287, 188], [154, 482], [352, 440]]) # 定位所需要裁切的图片左上、右上、左下、右下坐标点。point2 = np.float32([[0,0], [width, 0], [0, height], [width, height]]) # 定义展示的图像窗口坐标点matrix = cv2.getPerspectiveTransform(point1, point2) # 得到转换矩阵。imgOutput = cv2.warpPerspective(img, matrix, (width, height)) # 实现透视变换转换cv2.imshow(&quot;Image&quot;, img)cv2.imshow(&quot;Image Output&quot;, imgOutput)cv2.waitKey(0) join images 图像结合numpy.hstack()：在水平方向上平铺 numpy.vstack()：在竖直方向上堆叠 12345678910111213import cv2import numpy as npimg = cv2.imread('Resources/lena.png')imgHor = np.hstack((img,img)) # 水平堆叠图片imgVer = np.vstack((img,img)) # 垂直堆叠图片cv2.imshow(&quot;Horizontal&quot;,imgHor)cv2.imshow(&quot;Vertical&quot;,imgVer)cv2.waitKey(0) 提供一个原教学者自写的基于numpy和opencv原有函数的堆叠图片函数，用于堆叠不同通道数的图片，比如灰度图片和彩色图片堆叠。 12345678910111213141516171819202122232425262728293031323334353637383940def stackImages(scale, imgArray): rows = len(imgArray) cols = len(imgArray[0]) rowsAvailable = isinstance(imgArray[0], list) width = imgArray[0][0].shape[1] height = imgArray[0][0].shape[0] if rowsAvailable: for x in range(0, rows): for y in range(0, cols): if imgArray[x][y].shape[:2] == imgArray[0][0].shape[:2]: imgArray[x][y] = cv2.resize(imgArray[x][y], (0, 0), None, scale, scale) else: imgArray[x][y] = cv2.resize(imgArray[x][y], (imgArray[0][0].shape[1], imgArray[0][0].shape[0]), None, scale, scale) if len(imgArray[x][y].shape) == 2: imgArray[x][y] = cv2.cvtColor(imgArray[x][y], cv2.COLOR_GRAY2BGR) imageBlank = np.zeros((height, width, 3), np.uint8) hor = [imageBlank] * rows hor_con = [imageBlank] * rows for x in range(0, rows): hor[x] = np.hstack(imgArray[x]) ver = np.vstack(hor) else: for x in range(0, rows): if imgArray[x].shape[:2] == imgArray[0].shape[:2]: imgArray[x] = cv2.resize(imgArray[x], (0, 0), None, scale, scale) else: imgArray[x] = cv2.resize(imgArray[x], (imgArray[0].shape[1], imgArray[0].shape[0]), None, scale, scale) if len(imgArray[x].shape) == 2: imgArray[x] = cv2.cvtColor(imgArray[x], cv2.COLOR_GRAY2BGR) hor = np.hstack(imgArray) ver = hor return verimg = cv2.imread('Resources/lena.png')imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)imgStack = stackImages(0.5, ([img, imgGray, img], [img, img, img])) # 使用函数堆叠图片，第一行第二列为灰度图片。把原来的图片规模参数定为0.5imgHor = np.hstack((img,img))imgVer = np.vstack((img,img)) result:","link":"/2021/10/14/opencv%E5%9F%BA%E7%A1%80%E7%94%A8%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"怎样更好地理解并记忆泰勒展开式？（知乎陈二喜）","text":"注：此文为知乎作者陈二喜所写，由于该作者账号注销，便保存以之后拜读，侵权则删 今天，我要讲讲我和苍井空的故事。 FBI Warning:未成年人请在家长陪同下观看。 德艺双馨的苍老师是我的启蒙老师。初入大学，暂时摆脱高考的巨大压力后，终于可以放飞自我。在那个草长马发情的年代，无数个月光如水的燥热夜晚，苍老师的课件一次次给我以直逼心灵的抚慰。 嗯，这就是苍老师本尊了。为了表达我对苍老师的敬意，送她一副对联，上联是：肤如凝脂唇红齿白花容月貌倾国倾城千娇百媚，下联是：爱岗敬业任劳任怨废寝忘食一丝不苟精益求精，横批：德艺双馨。 作为她的铁粉，我想把这张照片画出来，或者雕刻出来，使她出现在我手中，免受隔着屏幕的煎熬。 想复制苍老师的美，首先要在整体尺寸上保持相同。如下： 紧接着，要在第一步的基础上进一步细化、精确化。所以第二步就要保证和苍老师本尊的局部形状相似。改进后就变成了如下： 嗯，尽管这时候很粗糙，但至少已经有了婀娜多姿的影子了。下一步帮苍老师画上bra和胖次，再加上发型，并且把大腿、小腿、脚的分界线画上。下图： 此时，苍老师的特征已经非常明显了，仿佛就要呼之欲出了，尤其那道事业线，使我仿佛看到一对大白在调皮地跳跃。我要继续努力，进一步细化，进一步使我手中的苍老师变得真实。 此时手中的苍老师外部线条更加细腻了，整体丰满了，仅有的服饰上增加了一些细节。如果不断地细化，画上五官，增加质感，添加纹理，那么进行无穷次细化之后，我笔下的苍老师一定会无穷接近真实。最终会变成这个样子： 当然，我没能有足够的时间继续细化下去，我那年的青春已经随着她的退役而完结，只是，我仍会在某个无眠的夜里回忆起苍老师认真工作的身影，回忆起我那年的青涩和成长，回忆起那年的憧憬和迷茫，回忆起我那年的生命曾经因为苍老师的出现而灼灼其华。 谨以此文献给新婚的苍老师。 好了，大家都精神了吧。现在开始进入正题。 本段的核心思想是仿造。 当我们想要仿造一个东西的时候，无形之中都会按照上文提到的思路，即先保证大体上相似，再保证局部相似，再保证细节相似，再保证更细微的地方相似……不断地细化下去，无穷次细化以后，仿造的东西将无限接近真品。真假难辨。 这是每个人都明白的生活经验。 =============== 一位物理学家，把这则生活经验应用到他自己的研究中，则会出现下列场景： 一辆随意行驶的小车，走出了一个很诡异的轨迹曲线： 物理学家觉得这段轨迹很有意思，也想开车走一段一摸一样的轨迹。 既然是复制，他把刚才关于“仿造”生活经验应用到这里，提出了一个解决办法： 既然想模仿刚才那辆车， 那首先应该保证初始位置一样， 继续模仿，让车在初始位置的速度也一样， 不满足，继续细化，这次保持位置、在初始位置处的速度一样的同时，保证在初始位置处车的加速度也一样， 不满足，继续细化，这次保证初始位置、初始位置处的速度、初始位置处的加速度都一样，也保证初始位置处的加速度的变化率也一样， 不满足，精益求精，可以一直模仿下去。 物理学家得出结论：把生活中关于“仿造”的经验运用到运动学问题中，如果想仿造一段曲线，那么首先应该保证曲线的起始点一样，其次保证起始点处位移随时间的变化率一样（速度相同），再次应该保证前两者相等的同时关于时间的二阶变化率一样（加速度相同）……如果随时间每一阶变化率（每一阶导数）都一样，那这俩曲线肯定是完全等价的。 ================= 一位数学家，泰勒，某天看到一个函数 ,不由地眉头一皱，心里面不断地犯嘀咕：有些函数啊，他就是很恶心，比如这种，还有三角函数，这样的函数本来具有很优秀的品质（可以无限次求导，而且求导还很容易），但是呢，如果是代入数值计算的话，就很难了。比如，看到 后，我无法很方便地计算 时候的值。 为了避免这种如鲠在喉的感觉，必须得想一个办法让自己避免接触这类函数，即把这类函数替换掉。 可以根据这类函数的图像，仿造一个图像，与原来的图像相类似，这种行为在数学上叫近似。不扯这个名词。讲讲如何仿造图像。 他联想到生活中的仿造经验，联想到物理学家考虑运动学问题时的经验，泰勒首先定性地、大概地思考了一下整体思路。（下面这段只需要理解这个大概意思就可以，不用深究。） 面对 的图像，泰勒的目的是：仿造一段一模一样的曲线 ，从而避免余弦计算。 想要复制这段曲线，首先得找一个切入点，可以是这条曲线最左端的点，也可以是最右端的点，anyway，可以是这条线上任何一点。他选了最左边的点。 由于这段曲线过 这个点，仿造的第一步，就是让仿造的曲线也过这个点， 完成了仿造的第一步，很粗糙，甚至完全看不出来这俩有什么相似的地方，那就继续细节化。开始考虑曲线的变化趋势，即导数，保证在此处的导数相等。 经历了第二步，现在起始点相同了，整体变化趋势相近了，可能看起来有那么点意思了。想进一步精确化，应该考虑凹凸性。高中学过：表征图像的凹凸性的参数为“导数的导数”。所以，下一步就让二者的导数的导数相等。 起始点相同，增减性相同，凹凸性相同后，仿造的函数更像了。如果再继续细化下去，应该会无限接近。所以泰勒认为“仿造一段曲线，要先保证起点相同，再保证在此处导数相同，继续保证在此处的导数的导数相同……” 有了整体思路，泰勒准备动手算一算。 下面就是严谨的计算了。 先插一句，泰勒知道想仿造一段曲线，应该首先在原来曲线上随便选一个点开始，但是为了方便计算，泰勒选择从 这个点入手。 把刚才的思路翻译成数学语言，就变成了： 首先得让其初始值相等，即： 其次，得让这俩函数在x=0处的导数相等，即： 再次，得让这俩函数在x=0处的导数的导数相等，即： …… 最终，得让这俩图像在x=0的导数的导数的导数的……的导数也相同。 这时候，泰勒思考了两个问题： 第一个问题，余弦函数能够无限次求导，为了让这两条曲线无限相似，我仿造出来的 必须也能够无限次求导，那 得是什么样类型的函数呢？ 第二个问题，实际操作过程中，肯定不能无限次求导，只需要求几次，就可以达到我想要的精度。那么，实际过程中应该求几次比较合适呢？ 综合考虑这两个问题以后，泰勒给出了一个比较折中的方法：令 为多项式，多项式能求几次导数呢？视情况而定，比如五次多项式 ,能求5次导，继续求就都是0了，几次多项式就能求几次导数。 泰勒比我们厉害的地方仅仅在于他想到了把这种生活经验、翻译成数学语言、并运用到仿造函数图像之中。假如告诉你这种思路，静下心来你都能自己推出来。 泰勒开始计算，一开始也不清楚到底要求几阶导数。为了发现规律，肯定是从最低次开始。 先算个一阶的。 可以看出，除了在 这个点，其他的都不重合，不满意。 再来个二阶的。 可以看出，在 这个点附近的一个小范围内，二者都比较相近。 再来个四阶的。 可以看出，仍然是在 这个点附近的一个范围内二者很相近。只是，此时二者重合的部分扩大了。 到这里，不光是泰勒，我们普通人也能大概想象得到，如果继续继续提高阶数，相似范围继续扩大，无穷高阶后，整个曲线都无限相似。插个图，利用计算机可以快速实现。 然而泰勒当时没有计算机，他只能手算，他跟我们一样，算到四阶就算不动了，他就开始发呆：刚才为什么这么做来着？哦，对了，是为了计算 的时候避免出现余弦。所以他从最左端 处开始计算，算着算着，他没耐心了，可是离着计算 还有一段距离，必须得继续算才能把这俩曲线重合的范围辐射到 处。 此时，他一拍脑门，恍然大悟，既然我选的点离着我想要的点还远，我为啥不直接选个近点的点呢，反正能从这条曲线上任何一个点作为切入，开始仿造。近了能省很多计算量啊。想计算 ，可以从 处开始仿造啊。 所以啊，泰勒展开式就是把一个三角函数或者指数函数或者其他比较难缠的函数用多项式替换掉。 也就是说，有一个原函数 ，我再造一个图像与原函数图像相似的多项式函数 ,为了保证相似，我只需要保证这俩函数在某一点的初始值相等，1阶导数相等，2阶导数相等，……n阶导数相等。 写到这里，你已经理解了泰勒展开式。 如果能理解，即使你记不住泰勒展开式，你都能自己推导。所以，我建议你，考试之前临时死记硬背一下，即使考试因为紧张忘了，也可以现场推。如果不是为了考试，那记不住也没关系，反正记住了一段时间不用，也会忘。用的时候翻书，找不到书就自己推导。 继续说泰勒。 泰勒算到四阶以后就不想算了，所以他想把这种计算过程推广到n阶，算出一个代数式，这样直接代数就可以了。泰勒就开始了下面的推导过程。 首先要在曲线 上任选一个点，为了方便，就选 ,设仿造的曲线的解析式为 ，前面说了，仿造的曲线是一个多项式，假设算到n阶。 能求n次导数的多项式，其最高次数肯定也为n。所以，仿造的曲线的解析式肯定是这种形式： 前面说过，必须保证初始点相同，即 ,求出了 接下来，必须保证n阶导数依然相等，即 因为对 求n阶导数时，只有最后一项为非零值，为 ， 由此求出 求出了 ，剩下的只需要按照这个规律换数字即可。 综上： 知道了原理，然后把原理用数学语言描述，只需要两步即可求出以上结果。背不过推一下就行。 泰勒推到这里，又想起了自己刚才那个问题：不一定非要从x=0的地方开始，也可以从 开始。此时，只需要将0换成 ，然后再按照上面一模一样的过程重新来一遍，最后就能得到如下结果： 泰勒写到这里，长舒一口气，他写下结论： 有一条解析式很恶心的曲线 ，我可以用多项式仿造一条曲线 ,那么 泰勒指出：在实际操作过程中，可根据精度要求选择n值，只要n不是正无穷，那么，一定要保留上式中的约等号。 若想去掉约等号，可写成下面形式： 好了，泰勒的故事讲完了。其实真正的数学推导只需要两步，困难的是不理解思想。如果背不过，就临时推导，只需要十几二十秒。 =============== 泰勒的故事讲完了，但是事情没完，因为泰勒没有告诉你，到底该求导几次。于是，剩下一帮人帮他擦屁股。 第一个帮他擦屁股的叫佩亚诺。他把上面式子中的省略号中的东西给整出来了。然而最终搁浅了，不太好用。 后面拉格朗日又跳出来帮佩亚诺擦屁股。至此故事大结局。 首先讲讲佩亚诺的故事。 简单回顾一下，上文提到，泰勒想通过一个多项式函数 的曲线，把那些看起来很恶心的函数 的曲线给仿造出来。提出了泰勒展开式，也就是下面的第一个式子： 佩亚诺开始思考误差的事。先不说佩亚诺，假如让你思考这个问题，你会有一个怎样的思路？既然是误差，肯定越小越小对吧。所以当我们思考误差的时候，很自然的逻辑就是让这个误差趋近于0。 佩亚诺也是这么想的，他的大方向就是令后面这半部分近似等于0，一旦后半部分很接近0了，那么就可以省去了，只展开到n阶就可以了，泰勒展开就可以用了。但是他不知道如何做到。 后来，他又开始琢磨泰勒的整个思路：先保证初始点位置相同，再保证一阶导数相同，有点相似了，再保证二阶导数相同，更细化了，再保证三阶导数相同……突然灵光闪现：泰勒展开是逐步细化的过程，也就是说，每一项都比前面一项更加精细化（更小）。举个例子，你想把90斤粮食添到100斤，第一次，添了一大把，变成99斤了，第二次，添了一小把，变成99.9斤了，第三次，添了一小撮，变成99.99斤了……每一次抓的粮食，都比前一次抓的少。泰勒展开式里面也是这样的： 由此可见，最后一项（n阶）是最小的。皮亚诺心想：只要让总误差（后面的所有项的总和）比这一项还要小，不就可以把误差忽略了吗？ 现在的任务就是比较大小，比较泰勒展开式中的最后一项、与误差项的大小，即： 如何比较大小？高中生都知道，比较大小无非就是作差或者坐商。不能确定的话，一个个试一下。最终，皮亚诺用的坐商。他用误差项除以泰勒展开中的最小的项，整理后得到： 红框内的部分是可以求出具体数字的。佩亚诺写到这里，偷了个懒，直接令 趋近于 ，这样，误差项除以泰勒展开中的最小项不就趋近于0了吗？误差项不就趋近于0了吗？ 我不知道你们看到这里是什么感觉，可能你觉得佩亚诺好棒，也可能觉得，这不糊弄人嘛。 反正，为了纪念佩亚诺的贡献，大家把上面的误差项成为佩亚诺余项。 总结一下佩亚诺的思路：首先，他把泰勒展开式中没有写出来的那些项补全，然后，他把这些项之和称为误差项，之后，他想把误差项变为0，考虑到泰勒展开式中的项越来越小，他就让误差项除以最后一项，试图得到0的结果，最后发现，只有当趋近于时，这个商才趋近于0，索性就这样了。 其实整体思路很简单，当初学不会，无非是因为数学语言描述这么个思路会让人很蒙逼。 佩亚诺的故事讲完了，他本想完善泰勒展开，然而，他的成果只能算 趋近于 时的情况。这时候，拉格朗日出场了。 拉格朗日的故事说来话长，从头说起吧。话说有一天，拉格朗日显得无聊，思考了一个特别简单的问题：一辆车，从 处走到 处，中间用了时间 ，那么这辆车的平均速度就是 ，假如有那么一个时刻，这辆车的瞬时速度是小于平均速度 的，那么，肯定有一个时刻，这辆车的速度是大于平均速度 的，由于车的速度不能突变，从小于 逐渐变到大于 ，肯定有一个瞬间是等于 的。 就这个问题，我相信在做的大多数，即使小时候没有听说过拉格朗日，也一定能想明白这个问题。 拉格朗日的牛逼之处在于，能把生活中的这种小事翻译成数学语言。他把 图像画出来了，高中生都知道，在这个图像中，斜率表征速度： 把上面的这个简单的问题用数学语言描述出来，就是那个被拉格朗日了的定理，简称拉格朗日中值定理：有个函数 ，如果在一个范围内连续，可求导，则 后来啊，拉格朗日的中值定理被柯西看到了，柯西牛逼啊，天生对于算式敏感。柯西认为，纵坐标是横坐标的函数，那我也可以把横坐标写成一个函数啊，于是他提出了柯西中值定理： 拉格朗日听说了这事，心里愤愤不平，又觉得很可惜，明明是自己的思路，就差这么一步，就让柯西捡便宜了，不过柯西确实说的有道理。这件事给拉格朗日留下了很深的心理阴影。 接下来，拉格朗日开始思考泰勒级数的误差问题，他同佩亚诺一样，只考虑误差部分（见前文）。 插一句，各位老铁，接下来拉格朗日的操作绝壁开挂了，我实在是编不出来他的脑回路。 首先，跟佩亚诺一样，先把误差项写出来，并设误差项为 ： 误差项 中每一项都是俩数的乘积，假如是你，你肯定是想两边同时除掉一个 ，对吧，为了简单，把 设为 : 所以除过之后，就成了： 等等，这一串东西看着怎么眼熟？咦？这不是柯西老哥推广的我的中值定理么？剩下的不就是……： 红框中，脑路之清奇、操作之风骚、画风之诡异、场面之震撼，让我们不禁感慨，拉格朗到底日了什么，脑海里才会想到柯西。 拉格朗日写到这里卡住了，不知道你们有没有这种经验，反正我思考一道数学题的时候，会尝试着把思路进行到底，直到完全进了死胡同才会否定这种思路。有了前面的脑洞，拉格朗日继续复制这种思路，想看看能不能继续往下写： 先看分子 再看分母 好巧合，又可以用一次柯西的中值定理了。 总之，按照这种方法，可以一直求解下去，最终的结果就是： 至此，拉格朗日把后面无数多的误差项给整合成了一项，而且比配诺亚更加先进的地方在于，不一定非要让 趋近于 ，可以在二者之间的任何一个位置 处展开，及其好用。 本文涵盖泰勒展开式、佩亚诺余项、拉格朗日中值定理、柯西中值定理、拉格朗日余项。全文完毕。 多谢大家的赞同以及批评和指正，回头看了一下全文，发现一个最大的问题：前半部分太“湿”，后半部分太干。以及，最后讲解拉格朗日余项时，堆砌的公式太多，讲的直观道理太少，影响阅读体验以及理解。我将会在我的下一篇关于傅里叶变换的回答中加以改正。 历时四天，终于把本文更新完毕。全文八千字左右。其实如果是用语言讲解，这一块的内容最多用十分钟即可讲完。为了解放双手，我在考虑年后要不要开一场live，把微积分和数学物理方法中的所有数学思想利用这种直观的生活经验讲解出来，全程重在理解，不会出现数学语言。名字我都想好了，就叫《燕园吴彦祖带你三小时深刻理解微积分的所有思想》。届时我会保证全程开车的同时、干货不断。 什么？你觉得我做不到全程开车？你可以质疑我的才华、可以质疑我的颜值，但是你不能质疑我的技术，因为。 我骚啊。 开个玩笑啦，我本人理工科博士在读，每天同一帮老男人一起讲段子，目前积累的段子有6亿多段，而且，在新东方和学而思当老师，不会开车根本没办法制伏倒霉孩子。 谢谢。新年快乐。 ========== 说最重要的一点，对于非数学系的理工科学生来说，永远都要记住，数学家都是凡人，你所接触到的所有数学知识，都来源于某一种数学思想，所有的数学思想都来源于生活经验。而这种生活经验，我们每个人都有，即使没有，也会很容易就能想通。 所以，你内心要有一种信仰，所有的数学思想都来源于生活经验，你肯定可以搞明白。学习数学，最忌讳的就是把它当作一种抽象的数字游戏，非数学系的理工科接触到的数学，必然有一条条形象的、直观的生活经验与之对应。 之所以觉得微积分困难，可能怪老师，可能怪课本，一开始就堆砌一堆晦涩难懂拗口的数学语言，对于初学者来说，直接就望而却步了。如果老师讲泰勒展开之前，先把这种思想讲明白，那接下来再去抠数学语言就轻松很多。","link":"/2021/10/24/%E6%80%8E%E6%A0%B7%E6%9B%B4%E5%A5%BD%E5%9C%B0%E7%90%86%E8%A7%A3%E5%B9%B6%E8%AE%B0%E5%BF%86%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E5%BC%8F%EF%BC%9F%EF%BC%88%E7%9F%A5%E4%B9%8E%E9%99%88%E4%BA%8C%E5%96%9C%EF%BC%89/"},{"title":"如何理解傅里叶变换公式（知乎陈二喜）","text":"注：此文为知乎作者陈二喜所写，由于该作者账号注销，便保存以之后拜读，侵权则删 温馨提示：为防止大家被我带跑偏，希望大家注意看每一节的标题。我已经加粗了字体。 在接下来的文字中，我会尝试把一个数学题当作小说来写，全文更多的侧重于一个人的碎碎念，质量没办法保障。 此帖不开车，敬回忆。 回忆是个好东西，它在岁月消失后永久封存在内心深处，虽然永远无法再度还原过去的生活，但它永远提醒我们：在过往的岁月，我们曾经拥有过一段怎样的经历。我试图用一种心平气和的方式将我和阿花之间为数不多的回忆以一种虔诚的姿态记录下来，我始终相信，文字是有力量的，它能够将抽象的遥远的回忆用一种更为直观的方式呈现，并且可以根据自己的喜好对过去的情节加以演绎甚至篡改，用一种偷梁换柱掩耳盗铃的方式来满足自我内心的空虚与遗憾，哪有什么真相，骗得过自己，就是真相。 本帖预计一万字，更新比较佛系。 ==================== PART1:生活中的傅立叶变换 十年前，我读高中。未满十八岁，认识了一帮不正经的同学。 在那间容纳着五十多个躁动灵魂的教室，我坐倒数第二排靠近后门的位置。上过中学的都知道，班上最闹腾的几个学生基本都分布在这一带。中学生的闹腾包含但不限于起哄和讲黄段子。我后座叫五角星，是班上看书最多的男生，整日满嘴的四书五经，满嘴的杜拉斯、卡夫卡，当然，也满嘴的荤段子。 课间，楼道里一副热闹景象，几个男生拿着着篮球练习垫步，模仿麦蒂的投篮技术，偶尔有熟悉的女孩子经过，还会佯装扔球砸一下，然后等待女孩子充满仇恨的白眼。我和两三个同学一起懒洋洋趴在栏杆上晒太阳，一边看着楼下嬉笑打闹的学生，一边享受着被阳光包裹的安全感。那是初春时节，北方的天空湛蓝而澄澈，阳光灿烂而温暖。这时候五角星跳起来趴我们身上，被我们嫌弃地挤开，他开口道：我给你们讲个段子吧。 故事是这样的，话说有一大户人家的小姐，非常漂亮，有多漂亮呢？用古话说是“手如柔荑，肤如凝脂，颈如蝤蛴，齿如瓠犀，螓首娥眉，巧笑倩兮，美目盼兮。”当然，用现在的大白话说就是“肤白貌美大长腿，D胸翘臀水蛇腰，人见人爱、花见花开鬼见鬼痴呆。”此等美女子到了适婚年龄后，她父亲在门前张贴了一告示，大概意思是说吾家有女，尚未婚配，欲寻得如意郎君花前月下，携手相伴此生。告示一贴出，门口前来求见的适龄青年络绎不绝，皆聚于庭院之中，府中丫头数了数，足足有一百位公子。小姐静坐闺房，心中不免犯愁，这么多男的，逐个相处选择貌似不可行，但又不想放弃任何一个潜在的优质男性。她思索良久，心生一计。 姑娘让丫头召集了这一百个男青年，然后吩咐丫鬟传话下去：“我家小姐说了，不求男子形比沈约，貌胜潘安，但求誓无二志、情比金坚。”言外之意就是，帅不帅不重要，对感情认真才是最重要的。语毕，有36个人热烈鼓掌，拍手大好。然后，丫鬟就让人把这36个男的赶出去了。随后，丫鬟进屋禀报小姐：小姐，已经按照您说的，把这36个“丑逼”赶出去了。（信这种鬼话的都是丑逼，所以才拍手叫好，而真正的帅哥却因为自己的外貌优势没有发挥出来，所以没鼓掌） 然后姑娘又吩咐丫鬟说下一句：“不喜男子良田万亩、豪宅数间。但爱勤奋努力、鸿业远图。”剩下这64个男的中，有25个鼓掌，鼓完掌还竖起一只大拇指似乎想点个赞。丫鬟把这25个也赶出去了，禀报小姐说：已经按照您的吩咐把这25个“穷逼”赶出去了。 小姐又吩咐丫鬟说下一句：“不喜男子屌长似驴，但求一倒一颠眠不得，鸡声唱破五更秋”。（大不大不重要，技术才重要）剩下这39个人中，有29个鼓掌说，我也这么认为。于是，这29个“短小”的男的也被清理出去。 事毕，丫鬟进屋去问小姐，剩下的10个怎么办，小姐媚眼流转，面若桃花，一脸娇羞，轻启朱唇，道： ‘’快把那10个“财大器粗的帅哥”给我请进来。” 经过这番操作，报名的那100个男的，变成了：36个丑逼+25个穷逼+29个短小+10个器大多金的帅哥。一目了然。 看完这个故事，我们几个的脸上都泛起了淫荡不堪的笑，在阳光下，贱兮兮的。我们纷纷佩服五角星有才，也不由得为小姐的机智所折服。 我后来就在想，这个段子真的特别妙，可以说是蕴含着深刻的生活智慧以及人生哲理，就这套逻辑，当年大贪官和珅也用过，话说当年饥荒，朝廷命和珅发放赈灾粮款。只要一发吃的，方圆数十里的灾民蜂拥而至，仅有的赈灾粮禁不住哄抢，很快就完，但沿路依旧饿殍满地。这部分抢粥喝的人当中，有些是真的要饿死了，而有些仅仅是想占便宜。于是和珅想了一个办法，把米换成平时喂猪用的糠，而且还在里面撒沙子，这样一来，哪些捡便宜的村民嘴叼，就不去抢这类吃的。而那些真正要饿死的人，才不管有没有沙子，总比观音土和树皮好吃。依旧上去哄抢。 经过这番操作，抢吃的村民变成了：一部分真正需要粮食的+单纯想捡便宜的。 高中时代听过的这段子似乎使我窥探到了生活真理。我时常在想，我又帅又有才华，知乎上私信排队跟我表白的人多的可以排满中关村大街，可能还要在三环上拐个弯。这帮私信搭讪的人涵盖了各个年龄阶层，有离异少妇、有都市白领、还有纯情学生。这些人怀揣着各种各样的目的，有想让我教她数学的，有想给我生猴子的，还有插科打诨的，比如： 假如，我要在我主页签名上加一句“非单身”，表白的人会变得很少。如果仍有表白的，那就只能说明对方爱我爱的死去活来、礼崩乐坏，宁可拆墙角也要表白，挣脱束缚，跨越世俗，这是一番怎样的爱啊。 经过这番操作，我能把这帮人变成：非真爱的+真爱的。 多年之后，上了大学，我才知道，当年那个百无聊赖的课间所探讨的人生哲理，用数学语言来描述就是“傅立叶变换+滤波”。 至此，可以放一个初步的结论： 傅立叶变换就是面对一团杂乱的信息，为了挑选方便，将其分类排列好。就像下图这样。 傅立叶变换 滤波就是把你不想要的分组剔除。 滤波 以上，从我们的生活经验出发，总结出来一个道理： 用接地气的生活语言形容这个道理就是：面对一团乱哄哄的东西的时候，可以通过某种操作，来将这团乱哄哄的东西分类，分类完毕后，再选择出自己需要的。 第一部分完结，从生活经验出发，十分不严格地讲述了傅立叶变换的思想。 当然，傅立叶分析、傅立叶变换、傅立叶级数是三个不同的概念，这是后话。此外，上文的几个例子也不严谨，接下来说一个严格的傅里叶变换的例子。 ================================ PART 2:物理中的傅立叶分析。 十年前，我读高中。未满十八岁，暗恋一个姑娘。 高中时代的大课间都会跑操，操场在东，主席台在西，操场周围有一圈高大的白杨树。常年生病旷课的体育老师站在主席台上以吓唬畜牲的口吻命令我们迅速排好队伍。每个班级队伍前都有一个举班旗的旗手，表情坚毅庄重，一般由班上的品学兼优的学生干部从事这项光荣伟大的工作。她就是隔壁班的棋手。留着齐肩的头发，一半头发扎起来在头顶挽成了一个小丸子，与朱茵有几分神似，很瘦很干净，穿一件一尘不染的纯白衣服，站在阳光下发出刺眼的光。 那天，对她的偷瞄一直持续到跑操结束，我把自己幻想成一个诗人，能够吟唱出两句摆得上台面的句子来满足自我内心的虚荣，未果。“娴静犹如花照水，行动好比风拂柳。眼前分明外来客，心底恰似旧时友”，嗯，没有那句比这句更加符合我此刻的心境。 从那天起，我开始正式暗恋她。 朴记告诉我说我暗恋的那姑娘叫阿花，是他的小学同学。 朴记是我们班上一朝鲜族同学，姓朴，每天跟我们分享各种一手校园八卦以及NBA赛事资讯，我们亲切的称他为朴记者，简称朴记。朴记告诉我这条重要线索的时候，两手扶着栏杆，所有所思地望着远方，嘴微微张着，两唇之间残留的口水拉出一条透明的丝，在阳光下，由于薄膜干涉发出彩色的光。 高中的时光，忙碌而轻松。午后的阳光总是那么刺眼，讲台上的语文老师讲课总是很催眠，她说有一种夸张的修辞手法，是通过超前想象来实现的，比如这句“农民伯伯看到绿油油的麦苗，仿佛闻到了白馒头的香味”，我觉得好无聊，就这种修辞句式，我能不带重样的写一本，比如，“我看到漂亮的阿花，有一种当爸爸的喜悦”。那时候的我，思维敏捷、下流至极。高中前两年，时间粘稠的像一杯浆糊，我们每个人内心充满着躁动与不安，在浆糊里窒息似的挣扎。 她每次从她们教室前门出来，经过我们教室后门，我能通过那阵香风的味道判断出是不是阿花。到现在我都记得那个味道，那是一种混合了洗面奶、保湿水、洗发水、洗衣液以及她身体的独特味道。 马上高三了，我趴在桌子上又一次睡了一个彻午，午休结束的铃声响起，我艰难的从课桌上爬起来，坐在座位上回魂，心中一阵罪恶感油然而生。后座的五角星跟别人说“少不看水浒老不看三国男不看西游女不看红楼，终身不看金瓶梅”，那是我第一次听说金瓶梅的存在。 我感觉眼前的世界一点点趋于真实，慢慢从混沌中一点点清醒，上课铃响后，我转过去问他，为什么终生不看金瓶梅，他告诉我，金瓶梅是一本黄书。 之后，物理老师提着拖鞋睡眼惺忪的进屋了。物理老师是个不拘小节的人，这一点从他的发型和穿搭便可以知道。如果观察其他地方，总会有不同的收获，比如今天，他牙齿上有一个韭菜叶。 那节课的讲的是光的色散，光通过三菱镜之后发生色散。我从来没有预习的习惯。第一次看到如此直观的感受到：光因为频率不同，而在三菱镜中走向了不同的路径。 学过高中物理的人对下面这幅图应该很熟悉。学名叫光的色散。这也是解释彩虹形成的原理。 物理老师告诉我们：我们平时看到的自然光是没有任何颜色的，俗称白光，里面包含了各种颜色的光——红橙黄绿蓝靛紫，以及人类无法识别出颜色的光。光能呈现不同颜色，本质原因在于其频率不同。我们平时看到的自然光是包含了各种频率的光线的混合体。 如果让这束白光通过一个三棱镜，由于不同频率的光具有不同的折射率，导致它们穿过这个三棱镜之后，走向了不同的方向。利用这个原理，就可以把白光中不同频率的光分离出来，并且按照频率的大小关系依次排开。 杂乱的白光=红光+橙光+黄光+绿光+蓝光+靛光+紫光+其他看不见的光。 三棱镜对于白光来说，相当于做了一个傅里叶变换。 比较上图与上一节第一张图会发现，这些图都在表达相同的道理： 即，一团杂乱的东西=具有a特征的A+具有b特征的B+…… 当然，那个时候的还并不明白什么是傅里叶变换，我只是隐约觉得这个世界的奇妙深不可测。白光融合了所有频率的光，经过三棱镜最终走向了不同的道路。那么人是否也会拥有各自的频率，或者说拥有一种类似于频率的属性呢？就像此刻我们虽然坐在同一间教室上课，但是以后会遇到一些事情，导致我们就会走向不同的人生道路。我为自己不小心开的脑洞而感到罪恶。快高考了，还有心思瞎想，还想不想跟阿花一起了。我心底那个积极的小人又出来鞭策我了。而就在此时，我发现这件事情细思恐极。如果我高考没考好，岂不是就不能和阿花在同一个城市了？这算不算“走向了不同的路径”？高考是个分水岭，那他是不是就相当于一个三棱镜？虽然我们此刻纠缠在同一个学校，将来会出现很多个三棱镜最终会把我们引到不同的人生道路上。那么，人是不是有频率的？ 伴随着我一不小心打开的脑洞，下课铃响了，我发现物理老师大门牙上沾的那片韭菜叶子不见了，第一排同学脑门上多了一片韭菜叶子，同物理老师牙齿上沾的那片大小一致，形状相同，在阳光下亮晶晶绿油油。（本段致敬冯唐） 总结：傅里叶变换其实就是把一团杂乱的东西，按照频率的不同依次分组。 第二部分完结，通过物理的例子直观理解傅立叶变换。 后记：高中毕业五周年，我们两个兄弟班一起组织聚餐。三杯两盏淡酒下肚，我微醺，且恍惚，我笑着跟阿花说，“妈的，我天生基因不好，脸上皮松，生下来就有褶子，显的又老又丑”。阿花说，怎么就丑了，男人老一点才有魅力，显得成熟。听到这句，甚感欣慰，我问阿花，你跟你厦门那个前任好了四年，为什么最后分了呀。 阿花说 “因为他长的又老又丑” 随后，我用阿花的手机请别人帮我们拍了唯一一张合影。 ＝＝＝＝＝＝＝ PART3：为什么要进行傅立叶变换 十年的漫漫长夜和那些阳光灿烂的白昼过去之后，岁月只留下记忆供我缅怀过去，我感到自己的记忆只能点点滴滴的出现，而且转瞬即逝。当我意识到自己与青春的交集只剩下几幅过往画面的时候，我开始思考一个问题：为什么大多数人都喜欢回忆青春？我觉得，一切冠以青春的命题都脆弱不堪，我们回忆青春，除了它是一个鲜活的载体的可能，更多在于它的短暂，而非美好。我高中时代的青春，懵懂、慌乱以及迷茫参杂在一起，形成一种透骨的无力感，美好个jb。 6月3号，班主任老师宣布第二天放假，要为高考腾地方，并说了一句，再过几天，高三的学生高考完了，你们就是高三的了，回家抓紧时间学习。我的高中生涯似乎按了快进键，这让我措手不及。 我觉得我必须要做点什么，为了我的暗恋。我很郑重的给她写了一封并没有任何表白信息的信，结尾附上我的qq号，随身带着，距离放假还有一节课的时候，阿花经过我的身边，我把信交给她。她先是愣了一下，然后看了看我，又低头把信收起来，匆忙跑开。 回家后焦急等待，她当晚加了我扣扣。 我们聊了两天，包括个人爱好、日常吐槽以及喜欢的大学，她说她想考人大，末了她约我开学后第一个周末去旱冰场。 旱冰场喧嚣不堪，放的是跳楼大甩卖专属背景音乐《Golden Sky》。阿花滑旱冰的技术特别好，能正着滑、倒着滑，转圈滑。她说，我教你滑旱冰吧你个笨蛋，我说我可能太笨了，她说聪明的我不教啊你个笨蛋。 随后，她扶着我的胳膊，手把手教我。我看到她的发丝细而软，脸蛋白里透红，脖子上隐约可见青色的毛细血管…… 稍微熟练后，她拉着我的手滑。我感觉她的手像脱骨鸡爪，软而绵，而我整个手臂，就像过电一样。 那天我终究是没学会，我靠在围栏上远远望着她一起一伏的身影，像一朵花，还有，胸前春桃初胀。 我感到一种无力感，我喜欢美丽的姑娘，我喜欢牛逼哄哄，我喜欢飞黄腾达，但我一无所有，只剩一种无法驾驭美好的无力感。那年的那个下午的我，永远也不会知道，在接下来的十年里，我拼命努力试图去消除掉的无力感，只会越来越强烈。 阿花在远处冲我喊了一句话，那句话淹没在喧嚣之中，我没听清。 那天，阿花到底对我说了什么。似乎永远无法知晓。 回学校后，我疯狂做题， 当我做到光的色散的物理题的时候，我又想到阿花说的那句淹没在嘈杂声中的话。我在想，世间是否有一种设备能够将杂糅在一起的声波按照频率将其剥离开，就像利用一个三棱镜能够把光分离一样。声音也是波，光也是波，二者之间是否会有一个相同的机制。 下课后，我去了物理老师办公室，他喝茶。我问他，老师，是不是只有我们学到的光能够发生色散，其他频率的波是否也会存在色散？物理老师喝完一口茶，用手把不小心喝进嘴里的茶叶拿出来重新扔回杯子里，用一口浓重的保定方言说道“zhei个问题，问滴挺好，上喽大学就明白lia，你说滴这个叫傅里叶变换”。之后他拿出纸笔，开始给我演示。 他告诉我说，色散这种效应跟波的频率没关系，波都存在这种色散。以前学习电磁场的时候选讲了调频广播的内容。各个电台发射各个频率的电波，比如fm103.1啊fm90.5啊，这个数字就是频率，这些频率在空中混在一起，彼此纠缠。手中的收音机其实是能听到各个频率的声音，但是每个台都听不清楚。你调整你收音机的这个动作叫调频，就是调整接收频率的意思。把其他频率的声音过滤掉。只听你想要的频率。然后他给我画了一张图。大概就是这个意思。 四个电台同时发送不同频率的波，彼此相互纠缠，变成“一团乱麻”。这团乱麻似的东西相互叠加，传到收音机之后，变成了四段波的矢量和。如下图。 如果将上面的信息直接播放，将会什么都听不见，为了听清楚，收音机要做的事情就是从上面叠加之后的声波中挑选出特定频率的声音。为了达到这个目的，需要做一个傅立叶变换。 上文提到，傅立叶变换就是把一团乱糟糟的东西给按照频率排开。显然，上面那团叠加之后的声波，按照频率排开应该是下图： 把上面的图画在一起，应该是下图： 想收听特定的电台，只需要调旋钮，选接收特定频率即可。这样，就能从那团乱麻之中挑选出某一段特定频率的声音了。 我接着问他，你刚才说的是超声波，那有没有一种东西，能够直接把普通的声波进行加工和挑选。比如，在闹市中，只听某个人的声音，他说， 理论上应该有，和收音机原理一样，但是好不好用就不知道了，可以试试录音笔。 之后我又问他，那您刚说的傅立叶变换是什么。他点燃一支烟，说，你在这个自然界中所感受到的任何信息，都是在时间坐标下的感受。比如，我们两个之间的对话，说白了就是声波。声波是什么样子的呢？横坐标是时间。这就叫时域，意思是这个区域是按照时间的不同来排列的。而每个人声音的不同是频率不同，横坐标是频率。这叫做频域，意思是这个区域里的东西，是按照频率的不同来排列的。 把同一件事从时域换到频域，就是傅立叶变换。把频域换到时域，就是傅立叶逆变换。 讲完这些，物理老师问我听完这些有没有什么想说的。 我有些蒙，嘴角抽动了几下，说“老师，您今天穿的袜子不是一对”。 那天，从物理老师办公室走出来，屋外的阳光依旧明媚刺眼，路旁的白杨树被风吹动发出呲啦啦的声音。我似乎并没有一种解决问题之后的满足感，取而代之的是一种再也无法搞清楚阿花那天到底说了什么的空虚与遗憾。 距离那天已经过去十多年，回首往事有时就像是翻阅陈旧的日历，昔日曾经出现过的欢乐和痛苦的时光成为了同样的颜色，在泛黄的纸上字迹都是一样的暗淡，使人难以区分。这似乎就是人生之路，经历总是比回忆鲜明有力。而回忆却比经历更加持久。 说回到傅立叶变换。 由于我们时时刻刻都在接收着世间万物所传递出来的信息，这些信息都是放在一个横坐标为世间的坐标系当中，很多信息柔和在一起，相互叠加，形成一团乱麻，乱糟糟的。你无法分辨。为了选取自己想要的信息，需要把他们按照频率排列开。你选择自己想要的即可。 现在再进一步严格化傅里叶变换的概念：傅里叶变换，就是把时域里的一团乱糟糟的信息，按照频率的不同依次排列好。傅立叶变换前后，信息本质不会改变。 小结：阐述了为什么要进行傅里叶变换。即：从一团乱麻中挑选出自己最想要的东西。 后记：旱冰场之约过后，我开始疯狂学习。五角星觉得我是被应试教育迫害了，一直试图把我从苦海中剥离出来。他以笔杆子为武器，以我为原型，写了一篇名叫《纯灰年代》的小说，以幽默诙谐的风格描写了一个高中生在一个高考大省、面对高考压力、出现一种疯狂刷题的病态。后来，他去上海参加新概念作文大赛，拿了个奖，一战成名。后来变成杂志《萌芽》的常客。把《纯灰年代》发表了。该文与本文故事有出入的情节，请以本文为准，特此声明。 ======================== PART 4:如何进行傅立叶变换（1）？ 十年前，我读高中，差两个星期满十八岁。 当时班里没有人拥有手机这么高级的设备，土豪东哥除外。东哥的手机块头很大，来电话时周围还有一圈跑马灯闪烁，炫酷的很。手机里存了黄片，想借来看的话，午休时间5块，熄灯后10块。东哥的生意逐渐拓展到整栋宿舍楼，来找他租手机的络绎不绝，形形色色，有的光着膀子直接吼“东哥，快，我要租手机，我要看片！别拦着”，我看到他胸前的两坨肥肉也随着上下抖动，想笑不敢笑，怕挨揍。有的顾客很斯文，客气得小声询问“请问你是东哥吗？那个……我……”有趣的很。 后来东哥跟我说过，他真的是发自内心的为同学服务，青春期的少年看黄片，就像春天里一颗小草的生长一样自然，没有什么目的。风起时一匹公马发情一样天经地义，也没有什么目的。草长马发情，绝非表演给什么人看的，这就是事实本身。（本段致敬王小波） 关于5块、10块的价格，也是经过深思熟虑的，且不说物价水平、风险系数，万一把价格定低了，戕害了我们祖国的栋梁罪过可就大了。 后来，东哥去天津上大学，把这套服务广大学子的赤诚之心也带了过去，创办了一个叫涉川教育的公司，被河工大学子戏称为射穿教育。 我们没有手机，但几乎人人都有一个本子，写一些公开的随笔，写完赶紧拿给别人看，别人就会在文章的各个角落里添加评论，笑点十足。那时候的我们，已经有了自溢的表达欲，用老师教给我们的知识骂老师、骂社会。这应该是纸质版的博客吧。 在距离我十八岁生日还有两个星期的时候，五角星开始不断游说我要过一次别开生面的生日。理由是十八岁是一个标杆，一定要有仪式感。给自己一个交代。只不过，当时我满脑子都是高考失败配不上阿花的想法，就以距离高考还有三个月为由拒绝了。为这件事他很鄙视我。他认为我成功的被培养成了高考机器。 我生日那天，他随手从花坛里拔了一棵草，说，这草你收着，生日礼物。也算是很有仪式感了。你想啊，一百年以后，你都118岁了，我不希望你那时候对于18岁生日这天的记忆一无所有。 确实，我十八岁生日那天做了哪些题，说了哪些话我全都忘了，记住的只有他送我一棵草。 五角星曾经扬言，一个月后，他的十八岁生日一定要轰轰烈烈。结果那天三模考试，他的计划泡汤。 晚自习的时候，他戳戳我，说，我写了一首18行的减字诗，纪念自己的十八岁，给你看看。我说，我一听减字诗我就不想看，牛逼的诗词一定犹如《将进酒》犹如《蜀道难》，充满自由，保持文字节奏的同时还能放荡不羁、信笔开河。而非像《雨霖铃》《江城子》那样直接在模版里填词。伟大的感情岂能在特定的字数规定下酣畅淋漓地表达？这跟八股文有什么区别？然而，看完后，我还是很震撼。那首诗是这样的： 习惯那些花儿开了谢了堆砌成冗长的笑意 细数着日出日落却再找不回丢失的结局 我种下十八个四季收获十八年的沉泣 把时间一段段摔碎在光与影的罅隙 谁让谁的归途成为不能说的秘密 爱与恨写进一本会遗忘的日记 笔尖氤氲着年华淌过的水迹 记忆烘不干没有你的天气 曾经的感动铺满了心底 疯长了十八年的情绪 倾泻成华丽的葬礼 来吧来吧来下雨 湿透所有别离 前行路刻下 你的笑语 会有爱 还有 你 以现在的眼光来看，充满了浓浓的非主流葬爱风格。当时我一口气读了好几遍，然后问他，这诗有名字了吗？ “还没有，你有什么愚见？” 我觉得小学生的作文里对于感情的抒发永远都类似于天总是蓝的，水总是清的，红领巾是国旗的一角、永远都是鲜艳的，所以小学生永远都像小鸟一样快乐。而我们处于青春期，青春文学作品必须得疼痛，必须得呻吟，越深沉越显得牛逼。 想到这些，我说“不如叫《骊歌轻唱，忍把十八载年华埋葬》” 行文至此，驻笔片刻，感慨良多。在那苦闷的高三生活中，我们班里的人仍然能够在百忙中抽出时间来写写文艺（作品再烂也是文艺），看看文艺，并以之为乐，实属难得。正如阿城在《棋王》里想要表达的那样，艺术永远都可以供我们在任何环境任何年代里尽情陶醉、尽情挥霍。（本段致敬阿城） 过了没多久，高考来了。阿花考的怎么样，我无从得知。 我在家收麦子。坐在田间地头上的树荫下望着远处的机器在金黄的麦田里轰隆隆收割。鼻腔里充斥着麦子被太阳炙烤后发出的干烫味。我想，语文老师骗人，别说绿油油的麦苗了，我就算看到金灿灿的麦田，我都没有闻到白馒头的香味。 这时候，我收到阿花发来的一串长长的短信，她在短信里说她考砸了，准备复读，还有，谢谢我这段时间以来的陪伴，但是希望我忘了她，她有男朋友，只不过初中毕业就去当兵了，她还在等她，并对此事表示抱歉。 就这样，我十八岁的初恋，还没有表白就已经结束了。我为了给自己找台阶下，回复她说：“我毕竟没有找你表白，不用抱歉，我祝福你们”。 之后我去地里扛麻袋背麦子，夕阳西下，放眼望去天地间是一望无垠的金黄，红色的落日把我的身体印的通红，我心里想的是“阿花啊阿花，早知道你是这种选手，我做题就不那么努力了”。 上了大学，自然会学到傅立叶变换，我还记得我与傅里叶变换结缘是因为阿花，傅立叶变换与阿花之间有脱不开的干系。大学一年级的我，还与阿花保持着一周两次通话的习惯，当然，这是后话。在大一下学期，我以一种虔诚的心态学习傅立叶变换。 傅立叶发明傅立叶变换的事情是这样的： 傅立叶公爵当年是搞传热学的，有一天他需要解一个非稳态偏微分方程，然而卡住了。 这里多嘴解释一下。 普通方程就是求解未知数。 微分方程就是通过某个函数的微分来求解原函数。 偏微分方程就是这个函数不只一个自变量。 非稳态偏微分方程就是说这个函数的其中一个自变量是时间t。 由此可知，傅立叶老爷子当年要求解的函数这个非常复杂，比下图要复杂的多。 导致他利用现有的数学知识无法解决。 但是，傅立叶老爷子具有超凡的物理直觉。他从物理角度上分析这个问题：他认为，此函数之所以如此复杂，是因为在实际的传热过程中，有很多个传热效应掺杂在一起。这多种传热效应组合、形成一团乱麻，使他无法求解。既然如此，那如果把这些传热效应依次分离开，处理起来是否会简单呢？ 就好比当年，我听不清阿花的声音，是因为旱冰场有各种各样嘈杂的声音掺杂在一起，形成一团乱麻，传入我耳中的声波变成了一个极其复杂的存在，我无法分辨。如果把这些声各自分离开，我就更容易分辨清楚每种声音。 傅立叶开始逐一思考各个传热效应。最后他发现，对于每一种传热效应，竟然都是一个很简单的正弦波，仅仅四个简单的正弦波相互叠加，就能够生成一个看起来极其复杂的函数。 比如，上图中的函数，看起来完全没有任何规律可言，可以说相当复杂了，但实际上，上图中的函数，其实仅仅是四个正弦波叠加而来的，如下图。 上面四个正弦波具有不同的频率，为了处理方便，我们可以把上面四个正弦波按照频率依次排开。就变成下图这样： 还是前面那句话，好多东西组合在一起，会形成一团乱麻，为了方便处理，可以将每个东西按照频率大小依次排列开。 好了，以上脑回路是我编的。接下来就是傅立叶老爷子的真事了。 补充一个常识，任何几个周期函数相互叠加，组成的新函数肯定也是一个周期函数。（没记错的话应该是高中知识？） 傅立叶认为，上面我举的例子比较简单，而且比较巧合。假如有一个复杂的函数，而且这个函数并不是像传热学问题或者声学问题那样，由几个效应共同组成，而是由于函数本身真的特别复杂，这种时候还能不能继续利用上述思路简化。 他极其大胆的认为：可以。 他的理由是这样的：假如有一个函数，并不是由于几个正弦波简单叠加，那么，完全可以仿造一个新函数，新函数的曲线与想求的函数曲线一模一样，而这个仿造出来的新函数是由几个正弦波叠加过来的。这样，我就可以把新函数中、各个正弦波按照频率依次排开，利用上述思路处理后即可求得新函数，新函数与原函数一样。 傅里叶这个想法有两个明显的问题： 一个问题是：你怎么可以如此大胆地认为，任何一个函数都可以用正弦波叠加就能仿造呢？所以，拉格朗日认为傅立叶是瞎扯，他说：你把下面这个矩形波函数用正弦波叠加仿造出来，能仿造出来算我输。 在那个没有电脑的年代，拉格朗日说的不无道理。因为从定性上讲，正弦波属于光滑的、没有棱角的函数。而上述矩形波函数有棱角。肯定无法匹配。 另一个问题就是：我们可以预想到，假如傅里叶真的拿一系列正弦波叠加去仿造与原函数一模一样的新函数的话，为了保证精度，他肯定需要无限个正弦波，这就意味着，这些正弦波拥有无限个频率，有频率为1的，有频率为1.1的，有频率为1.11的……无穷匮也。换一种说法就是，把无数个正弦波按照频率依次排开（频域图像），应该是一个连续的函数，不再像上面画的频域图像那样，是一个个离散的频率。 所以，我们可以进一步推测，傅立叶变换公式将不再是之前我们认为的那样： 而应该变成： 第一个式子叫傅里叶级数，第二个式子叫傅里叶变换，傅里叶级数和傅里叶变换统称为傅里叶分析。 基本思路有了，现在的问题是：如何写出无限个正弦波叠加呢？下节欧拉公式。 小结：阐述了傅立叶发明傅立叶变换的思路，他认为，任何一个函数，我都可以用很多个正弦波叠加的方式仿造一个一模一样的函数。仿造出来之后，再把这么多正弦波按照频率依次排开。 后记：阿花让我放弃她、并坦白她有男朋友的当天晚上，跟我诉说了她男朋友的故事：她男朋友叫舒农，家住香椿树街，生活在一个没有关爱的家庭，品行不端的父亲对他疏于管教，使他成为一个标准的带有严重自卑感的“问题少年”，为了扬眉吐气、受孩子们尊敬，终日打架斗狠，似乎这才是这个年龄的孩子获得认同感的唯一方式。故事如果按照这个方向继续发展下去，他要么被人打残，要么进少管所，所幸，阿花的出现改变了他整个的人生轨迹。后来，在阿花的劝说下，舒农去当了兵，成了一个干净阳光的男子汉。（本段致敬苏童） ================= PART 5:欧拉公式 十年前，我高中毕业，已满十八岁。 那年暑假去学校领通知书，彼时学校的复读班已经开学，我知道阿花就坐在某间教室里上课。上课时间，学校里出奇的安静，原来母校这么漂亮啊，甬道两旁铺天盖地的垂柳安静的悬挂，恰到好处的映衬着整个校园的肃穆。我沿着学校走了一圈又一圈，看了一场篮球，然后走出校门，来到那家熟悉的店面吃了一碗牛肉拉面。以前的周末下午，总会有两节课的闲暇时光，我和朴记、五角星他们也会打一场篮球，筋疲力尽之后会出门去吃一碗奢侈的牛肉拉面。今天，我一个人又把这些事按照顺序做了一遍，算是完成了我对高中母校的道别。 学校南门外是开元寺塔，我坐在广场长椅上，盯着学校门口发呆，嗯，此时我还没有向阿花正式道别。后来，我找了一个“阿花太忙，不方便见我”的借口，决定不道别了。我点开她的qq空间，给她留言： 人大不远，明年不远 开往北京的火车只为有梦想的人晚点 幸福不远，爱情不远 明年六月，把酒狂欢 ……我掏钱 留言完毕，返回主页的时候，我看到她的qq签名是“世界上最短的咒语是一个人的名字” 八月中旬，阿花联系我，她说她实在受不了复读班的变态生活，也实在没有勇气赌明年就能考上人大，所以，她放弃了，要去厦门读大学。也就在那段时间，她与舒农断了，qq签名也改成了“要到一个有海的城市去哭” 我与阿花又恢复了联系，我依旧没有表白，尽管她知道我喜欢她，尽管我也知道，她对舒农有太多不舍和无奈。 读了大学，认识了一帮损友。 读了大学，才知道原来城里的孩子都请家教补课。 读了大学，才知道长途电话费这么贵。 我买了一张IC电话卡，在那种电话亭30块钱能打500分钟，我与阿花每周通两次电话。 我找了一个家教工作，准备攒够钱买一张飞机票去找阿花表白。每周六早晨五点半起床，坐一个多小时的公交车去小孩家里上课，晚上在他家吃完饭回学校。到学校已经九点了。一天能挣960。我第一次感觉，赚钱太容易了。如今，我天天为找工作焦虑，我才明白，结合物价水平，十年那个我，是时薪的最高峰。 我比较爱热闹，带着暴发户的心态，每次拿了钱，都要请大家去外面吃一顿。所以，去找阿花的机票钱还没有攒够。 有一天，龙哥给我发飞信：晚上去吃鱼？ 我回：下次吧，我这两天没钱了。 龙哥回：钱呢？ 我：草泥马 在我还没有攒够钱的时候，阿花在电话里跟我说，她要谈恋爱了。对方是班里同学，对她挺好的，人也长的挺好。再一次，那种深深的无力感吞噬了我。憋的我喘不过来气。最后，我没等阿花安慰完，我对她说 “再见！” 我打电话喊龙哥出去吃鱼、喝酒。龙哥陪我喝的断片，我背他回来。回来路上，我一边忍受深秋的寒冷，一边泣不成声。晚上，泪水打湿枕头，无力的悸痛感不断折磨我的心脏，借着酒意，我在纸上写下一封不会寄出去的信： 我想把此生爱恨 用黑色流淌在这个秋天最后一片落叶的背面 寄给昨天 让冗长的冬季来安葬对你的思念 一句再见 在你我之间插入了一千年的时间 让我们再也看不到当初的容颜 只是 在纷扬落叶的那棵树下 一个人，睁着眼想念 我怕时间把冬日老树的枯枝折断 从此再回到没有你的春天 暗自想你却被眼泪看见 从此便在孤单的夜里决堤漫延 却再不能沾湿 你曾用手写在我脸上的爱怜 可是你曾像花儿一样笑了啊,只被我看见 我说爱自说出口起就不会变 我说错在当初我们曾经遇见 说好的幸福是命运的签 说好的爱你却再回不到从前 从不敢奢求你在爱与友之间划清界限 就像我们竟感受不到那是夏天 世界结了霜来温暖谁那 绝望的、用霜结成的眼 一句再见的温度 生生地让牵着的手 断！断！断！ 生生地 扭转了冬天与夏天 可我还爱你呢 却再不能让你听见 就像在旱冰场的我不懂傅里叶变换 感谢命运让我们拥有那段牵手滑过冰场的时间 在回忆中 一直，一直向前 无数光年 我尽量用写字的频率去揣测你心中回忆的线 它将牵向哪里呀？ 它也在想我吗？ 我想用它们的契合来告诉你 传说中永恒的牵挂 一直,一直都在我的心间 无论 再见不再见 第二天，我完全恢复了理智，就告诉她，没什么，我看开了。这样，我们依旧是朋友，只是，我们联系的频率，平均两个月一次，每次只聊几句毫无营养的客套话。 再后来，阿花问我数学问题。她问我什么是欧拉公式。 我说，欧拉公式就是： 她说她知道欧拉公式怎么写，关键问题在于，怎么理解。 我说，记住就是了。 她火了，说，“你是不是找抽，老娘现在快被这个折磨死了，当年我就只知道记住虚数的平方是负数，现在，开始又出现了这么个东西，我无法理解也就罢了，关键我怎么都想不通这等式怎么可能成立。” 我清了清嗓子，说， 你还记得泰勒公式吗？话说在一百多年以前，还没有计算机、计算器，人们根本没办法快速计算三角函数、对数函数以及开方等。为了解决这个问题，泰勒提出了泰勒展开。后来佩亚诺、拉格朗日、柯西等人充分完善了这个工作，以前你学到的洛必达法则，罗尔定理、拉格朗日中值定理，以及现在卡住的欧拉公式，都是可以用泰勒展开的。 ps:泰勒展开的回答：陈二喜：怎样更好地理解并记忆泰勒展开式？ 正是有了泰勒展开的理论，所以，你如今才可以方便的用计算器计算cos2。 利用泰勒公式可知： 把x替换成it，就可以得出 综上，欧拉公式得证，欧拉公式就是这么来的，我告诉阿花，假如当年你学了泰勒公式，你也能发明欧拉公式。 后记：阿花最后跟厦门的这个男生分手了，她原话是这么说的：“我没有多少青春陪着一个人一穷二白”，本科毕业后迅速找了个有钱的男朋友，闪婚。高中毕业五周年聚餐上，我与阿花拍了唯一一张合影，当时，她已经戴上了结婚戒指。而那时候的我，衣品很差，全靠颜值死撑。 PART 6:如何进行傅立叶变换（2）？ 未完待续……","link":"/2021/10/24/%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%85%AC%E5%BC%8F%EF%BC%88%E7%9F%A5%E4%B9%8E%E9%99%88%E4%BA%8C%E5%96%9C%EF%BC%89/"}],"tags":[{"name":"Latex","slug":"Latex","link":"/tags/Latex/"},{"name":"机器学习 深度学习 Pytorch","slug":"机器学习-深度学习-Pytorch","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Pytorch/"},{"name":"统计学习方法","slug":"统计学习方法","link":"/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"刷题 leetcode 数据结构","slug":"刷题-leetcode-数据结构","link":"/tags/%E5%88%B7%E9%A2%98-leetcode-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"线性代数 数学理论","slug":"线性代数-数学理论","link":"/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0-%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"},{"name":"数据挖掘 数学理论","slug":"数据挖掘-数学理论","link":"/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98-%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"},{"name":"泰勒展开式 数学理论","slug":"泰勒展开式-数学理论","link":"/tags/%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E5%BC%8F-%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"},{"name":"傅里叶变换 数学理论","slug":"傅里叶变换-数学理论","link":"/tags/%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2-%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA/"}],"categories":[{"name":"机器学习 深度学习","slug":"机器学习-深度学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"创建博客_Github_hexo","slug":"创建博客-Github-hexo","link":"/categories/%E5%88%9B%E5%BB%BA%E5%8D%9A%E5%AE%A2-Github-hexo/"},{"name":"Latex","slug":"Latex","link":"/categories/Latex/"},{"name":"机器学习 深度学习 Pytorch","slug":"机器学习-深度学习-Pytorch","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-Pytorch/"},{"name":"统计学习方法","slug":"统计学习方法","link":"/categories/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"OpenCV","slug":"OpenCV","link":"/categories/OpenCV/"},{"name":"刷题","slug":"刷题","link":"/categories/%E5%88%B7%E9%A2%98/"},{"name":"数学理论学习","slug":"数学理论学习","link":"/categories/%E6%95%B0%E5%AD%A6%E7%90%86%E8%AE%BA%E5%AD%A6%E4%B9%A0/"},{"name":"数据挖掘","slug":"数据挖掘","link":"/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"}],"pages":[{"title":"","text":"Resume About me Basic Information Name: PaddeyZhang Email: paddeyzhang@gmail.com Education Degree Guangdong University of Technology, Information Security, 2022 Degree Guangdong University of Technology, Computer Science and Technology, 2025 Research Interest Federated Learing Machine Learing","link":"/about/index.html"}]}